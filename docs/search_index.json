[
["usgs-national-water-information-system-nwis.html", "Chapter 7 USGS National Water Information System (NWIS) 7.1 USGS Mission: 7.2 USGS Water Resources Mission: 7.3 Types of USGS NWIS Data 7.4 USGS R Packages: Collaborative and reproducible data analysis using R 7.5 Introduction to USGS R Packages 7.6 Data available 7.7 Common NWIS function arguments 7.8 Discovering NWIS data 7.9 Common WQP function arguments 7.10 Discovering WQP data 7.11 readNWIS functions 7.12 Additional Features 7.13 readWQP functions 7.14 Attributes and metadata 7.15 USGS Coding Lab Exercises 7.16 geoKnife - Introduction 7.17 Lesson Summary 7.18 Lesson Objectives 7.19 Lesson Resources 7.20 Remote processing 7.21 geoknife components: fabric, stencil, knife 7.22 Available webdata 7.23 Available webgeoms 7.24 Available webprocesses 7.25 Setting up a geojob 7.26 Checking the geojob status 7.27 Getting geojob data 7.28 wait and email 7.29 USGS NWIS Culmination Write Up", " Chapter 7 USGS National Water Information System (NWIS) United States Geological Survey’s National Water Information System (NWIS) 7.1 USGS Mission: Changes to the natural world combined with growing human demands put our health and safety, national security, and economy at risk. We are focused on some of the most significant issues society faces, and our science is making a substantial contribution to the well-being of the Nation and the world. You can explore USGS Missions and programs here. 7.2 USGS Water Resources Mission: Water information is fundamental to national and local economic well-being, protection of life and property, and effective management of the Nation’s water resources. The USGS works with partners to monitor, assess, conduct targeted research, and deliver information on a wide range of water resources and conditions including streamflow, groundwater, water quality, and water use and availability. The United States Geological Survey (USGS) has collected water-resources data at approximately 1.5 million sites in all 50 States, the District of Columbia, Puerto Rico, the Virgin Islands, Guam, American Samoa and the Commonwealth of the Northern Mariana Islands. A map of collection sites can be found here 7.3 Types of USGS NWIS Data The types of data collected are varied, but generally fit into the broad categories of surface water and groundwater. Surface-water data, such as gage height (stage) and streamflow (discharge), are collected at major rivers, lakes, and reservoirs. Groundwater data, such as water level, are collected at wells and springs. Water-quality data are available for both surface water and groundwater. Examples of water-quality data collected are temperature, specific conductance, pH, nutrients, pesticides, and volatile organic compounds. The NWIS web site serves current and historical data. Data are retrieved by category of data, such as surface water, groundwater, or water quality, and by geographic area. 7.4 USGS R Packages: Collaborative and reproducible data analysis using R Contributors: Jordan Read, Lindsay Carr Adapted from USGS ‘Getting Started with USGS R Packages’ course materials Recently, the USGS has built a suite of software packages and tutorials for R users to to interact with their data and streamline workflows. Here we have adapted course materials from thier USGS R packages course materials written and developed by Lindsay R. Carr. The common workflow for completing the data processing pipeline is subject to human error at every step: accessing data, analyzing data, and producing final figures. Multi-site analyses are especially error-prone because the same workflow needs to be repeated many times. This course teaches a modular approach to the common data analysis workflow by building on basic R data analysis skills and leveraging existing USGS R packages that can create advanced, reproducible workflows, such as for accessing gridded climate data, analyzing high frequency water observations, and for taking full advantage of the USGS ScienceBase repository. The USGS packages covered in this course span a variety of applications: accessing web data, accessing personally stored data, and releasing data for publication. The modular workflows taught in this section will prepare researchers to create automated, robust data processing workflows through more efficient code development. Following the course, students will be capable of integrating these packages into their own scientific workflows. 7.4.1 Suggested prerequisite knowledge This course assumes a moderate to advanced knowledge of the statistical programming language R. If you’re interested in using USGS packages for data analysis but have no R experience, please visit the Introduction to R curriculum available at this site. Experience using R to import, view, and summarize data Recommended: experience creating simple plots in R Recommended: familiarity with RStudio 7.4.2 Course outline ## Warning: package &#39;htmlTable&#39; was built under R version 3.6.2 Summary of Modules Module Description Duration dataRetrieval Accessing time series data. 2 hours geoknife Accessing gridded data. 1 hour Application Use the packages introduced in previous modules to create and use a robust modular workflow. 1.5 hour 7.4.3 Software requirements See the R installation instructions page for how to install/upgrade R and RStudio, add GRAN to your settings, and install some basic packages. Then, execute these lines so that you have the most up-to-date version of the packages used in this course. install.packages(c(&#39;dataRetrieval&#39;, &#39;geoknife&#39;)) 7.4.4 Lesson Summary This lesson will focus on finding and retrieving hydrologic time series data using the USGS R package, dataRetrieval. The package was created to make querying and downloading hydrologic data from the web easier, less error-prone, and reproducible. The package allows users to easily access data stored in the USGS National Water Information System (NWIS) and the multi-agency database, Water Quality Portal (WQP). NWIS only contains data collected by or for the USGS. Conversely, WQP is a database that aggregates water quality data from multiple agencies, including USGS, Environmental Protection Agency (EPA), US Department of Agriculture (USDA), and many state, tribal, and local agencies. dataRetrieval functions take user-defined arguments and construct web service calls. The web service returns the data as XML (a standard data structure), and dataRetrieval takes care of parsing that into a useable R data.frame, complete with metadata. When web services change, dataRetrieval users aren’t affected because the package maintainers will update the functions to handle these modifications. This is what makes dataRetrieval so user-friendly. Neither NWIS nor WQP are static databases. Users should be aware that data is constantly being added, so a query one week might return differing amounts of data from the next. For more information about NWIS, please visit waterdata.usgs.gov/nwis. For more information about WQP, visit their site www.waterqualitydata.us or read about WQP for aquatic research applications in the publication, Water quality data for national-scale aquatic research: The Water Quality Portal. 7.4.5 Lesson Objectives Learn about data available in the National Water Information System (NWIS) and Water Quality Portal (WQP). Discover how to construct your retrieval calls to get exactly what you are looking for, and access information stored as metadata in the R object. By the end of this lesson, the learner will be able to: Investigate what data are available in National Water Information System (NWIS) and Water Quality Portal (WQP) through package functions. Construct function calls to pull a variety of NWIS and WQP data. Access metadata information from retrieval call output. 7.4.6 Lesson Resources USGS publication: The dataRetrieval R package Source code: dataRetrieval on GitHub Report a bug or suggest a feature: dataRetrieval issues on GitHub USGS Presentation: dataRetrieval Tutorial 7.4.7 Lesson Slide Deck Browse the slide deck here 7.5 Introduction to USGS R Packages Before continuing with this lesson, you should make sure that the dataRetrieval package is installed and loaded. If you haven’t recently updated, you could reinstall the package by running install.packages('dataRetrieval') or go to the “Update” button in the “Packages” tab in RStudio. # load the dataRetrival package library(dataRetrieval) There is an overwhelming amount of data and information stored in the National Water Information System (NWIS). This lesson will attempt to give an overview of what you can access. If you need more detail on a subject or have a question that is not answered here, please visit the NWIS help system. 7.6 Data available Data types: NWIS and WQP store a lot of water information. NWIS contains streamflow, peak flow, rating curves, groundwater, and water quality data. As can be assumed from the name, WQP only contains water quality data. Time series types: the databases store water data at various reporting frequencies, and have different language to describe these. There are 3 main types: unit value, daily value, and discrete. WQP only contains discrete data. instantaneous value (sometimes called unit value) data is reported at the frequency in which it was collected, and includes real-time data. It is generally available from 2007-present. daily value data aggregated to a daily statistic (e.g. mean daily, minimum daily, or maximum daily). This is available for streamflow, groundwater levels, and water quality sensors. discrete data collected at a specific point in time, and is not a continuous time series. This includes most water quality data, groundwater levels, rating curves, surface water measurements, and peak flow. Metadata types: both NWIS and WQP contain metadata describing the site at which the data was collected (e.g. latitude, longitude, elevation, etc), and include information about the type of data being used (e.g. units, dissolved or not, collection method, etc). 7.7 Common NWIS function arguments siteNumber All NWIS data are stored based on the geographic location of the equipment used to collected the data. These are known as streamgages and they take continuous timeseries measurements for a number of water quality and quantity parameters. Streamgages are identified based on an 8-digit (surface water) or 15-digit (groundwater) code. In dataRetrieval, we refer to this as the siteNumber. Any time you use a siteNumber in dataRetrieval, make sure it is a string and not numeric. Oftentimes, NWIS sites have leading zeroes which are dropped when treated as a numeric value. parameterCd NWIS uses 5-digit codes to refer to specific data of interest called parameter codes, parameterCd in dataRetrieval. For example, you use the code ‘00060’ to specify discharge data. If you’d like to explore the full list, see the Parameter Groups Table. The package also has a built in parameter code table that you can use by executing parameterCdFile in your console. service Identifier referring to the time series frequencies explained above, or the type of data that should be returned. For more information, visit the Water Services website. instaneous = “iv” daily values = “dv” groundwater levels = “gwlevels” water quality = “qw” statistics = “stat” site = “site” startDate and endDate Strings in the format “YYYY-MM-DDTHH:SS:MM”, either as a date or character class. The start and end date-times are inclusive. stateCd Two character abbreviation for a US state or territory. Execute state.abb in the console to get a vector of US state abbreviations. Territories include: AS (American Samoa) GU (Guam) MP (Northern Mariana Islands) PR (Puerto Rico) VI (U.S. Virgin Islands) For more query parameters, visit NWIS service documentation. 7.8 Discovering NWIS data In some cases, users might have specific sites and data that they are pulling with dataRetrieval but what if you wanted to know what data exists in the database before trying to download it? You can use the function whatNWISdata, described below. Another option is to download the data using readNWISdata, and see the first and last available dates of that data with the arguments seriesCatalogOutput=TRUE and service=&quot;site&quot;. Downloading data will be covered in the next section, readNWIS. 7.8.1 whatNWISdata whatNWISdata will return a data.frame specifying the types of data available for a specified major filter that fits your querying criteria. You can add queries by the data service, USGS parameter code, or statistics code. You need at least one “major filter” in order for the query to work. “Major filters” include siteNumber, stateCd, huc, bBox, or countyCd. In this example, let’s find South Carolina stream temperature data. We specify the state, South Carolina, using the stateCd argument and South Carolina’s two letter abbreviation, SC. data_sc &lt;- whatNWISdata(stateCd=&quot;SC&quot;) nrow(data_sc) ## [1] 186041 Let’s look at the dataframe returned from whatNWISdata: head(data_sc) ## agency_cd site_no station_nm site_tp_cd dec_lat_va ## 1 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 2 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 3 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 4 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 5 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## 6 USGS 02110400 BUCK CREEK NEAR LONGS, SC ST 33.9535 ## dec_long_va coord_acy_cd dec_coord_datum_cd alt_va alt_acy_va alt_datum_cd ## 1 -78.71974 S NAD83 5.30 .01 NAVD88 ## 2 -78.71974 S NAD83 5.30 .01 NAVD88 ## 3 -78.71974 S NAD83 5.30 .01 NAVD88 ## 4 -78.71974 S NAD83 5.30 .01 NAVD88 ## 5 -78.71974 S NAD83 5.30 .01 NAVD88 ## 6 -78.71974 S NAD83 5.30 .01 NAVD88 ## huc_cd data_type_cd parm_cd stat_cd ts_id loc_web_ds medium_grp_cd ## 1 03040206 ad &lt;NA&gt; &lt;NA&gt; 0 &lt;NA&gt; wat ## 2 03040206 dv 00010 00001 124327 &lt;NA&gt; wat ## 3 03040206 dv 00010 00002 124328 &lt;NA&gt; wat ## 4 03040206 dv 00010 00003 124329 &lt;NA&gt; wat ## 5 03040206 dv 00045 00006 124351 &lt;NA&gt; wat ## 6 03040206 dv 00060 00001 124348 &lt;NA&gt; wat ## parm_grp_cd srs_id access_cd begin_date end_date count_nu ## 1 &lt;NA&gt; 0 0 2006-01-01 2019-01-01 14 ## 2 &lt;NA&gt; 1645597 0 2005-10-01 2020-10-12 5395 ## 3 &lt;NA&gt; 1645597 0 2005-10-01 2020-10-12 5395 ## 4 &lt;NA&gt; 1645597 0 2005-10-01 2020-10-12 5395 ## 5 &lt;NA&gt; 1644459 0 2006-01-06 2020-10-12 5203 ## 6 &lt;NA&gt; 1645423 0 2005-11-17 2019-08-17 4394 The data returned from this query can give you information about the data available for each site including, date of first and last record (begin_date, end_date), number of records (count_nu), site altitude (alt_va), corresponding hydrologic unit code (huc_cd), and parameter units (parameter_units). These columns allow even more specification of data requirements before actually downloading the data. This function returns one row per unique combination of site number, dates, parameter, etc. In order to just get the sites, use unique: sites_sc &lt;- unique(data_sc$site_no) length(sites_sc) ## [1] 10127 To be more specific, let’s say we only want stream sites. This requires the siteType argument and the abbreviation “ST” for stream. See other siteTypes here. We also only want to use sites that have temperature data (USGS parameter code is 00010). Use the argument parameterCd and enter the code as a character string, otherwise leading zeroes will be dropped. Recall that you can see a table of all parameter codes by executing parameterCdFile in your console. data_sc_stream_temp &lt;- whatNWISdata(stateCd=&quot;SC&quot;, siteType=&quot;ST&quot;, parameterCd=&quot;00010&quot;) nrow(data_sc_stream_temp) ## [1] 652 We are now down to just 652 rows of data, much less than our original 186,041 rows. Downloading NWIS data will be covered in the next section, readNWIS. The whatNWISdata function can also be very useful for making quick maps with site locations, see the columns dec_lat_va and dec_long_va (decimal latitude and longitude value). For instance, # SC stream temperature sites library(maps) map(&#39;state&#39;, regions=&#39;south carolina&#39;) title(main=&quot;South Carolina Stream Temp Sites&quot;) points(x=data_sc_stream_temp$dec_long_va, y=data_sc_stream_temp$dec_lat_va) (#fig:sc_streamtemp_data_map)Geographic locations of NWIS South Carolina stream sites with temperature data Continuing with the South Carolina temperature data example, let’s look for the mean daily stream temperature. # Average daily SC stream temperature data data_sc_stream_temp_avg &lt;- whatNWISdata( stateCd=&quot;SC&quot;, siteType=&quot;ST&quot;, parameterCd=&quot;00010&quot;, service=&quot;dv&quot;, statCd=&quot;00003&quot;) nrow(data_sc_stream_temp_avg) ## [1] 95 Let’s apply an additional filter to these data using the filter function from dplyr. Imagine that the trend analysis you are conducting requires a minimum of 300 records and the most recent data needs to be no earlier than 1975. # Useable average daily SC stream temperature data library(dplyr) data_sc_stream_temp_avg_applicable &lt;- data_sc_stream_temp_avg %&gt;% filter(count_nu &gt;= 300, end_date &gt;= &quot;1975-01-01&quot;) nrow(data_sc_stream_temp_avg_applicable) ## [1] 91 This means you would have 91 sites to work with for your study. 7.9 Common WQP function arguments countrycode, statecode, and countycode These geopolitical filters can be specified by a two letter abbreviation, state name, or Federal Information Processing Standard (FIPS) code. If you are using the FIPS code for a state or county, it must be preceded by the FIPS code of the larger geopolitical filter. For example, the FIPS code for the United States is US, and the FIPS code for South Carolina is 45. When querying with the statecode, you can enter statecode=&quot;US:45&quot;. The same rule extends to county FIPS; for example, you can use countycode=&quot;45:001&quot; to query Abbeville County, South Carolina. You can find all state and county codes and abbreviations by executing stateCd or countyCd in your console. siteType Specify the hydrologic location the sample was taken, e.g. streams, lakes, groundwater sources. These should be listed as a string. Available types can be found here. organization The ID of the reporting organization. All USGS science centers are written “USGS-” and then the two-letter state abbrevation. For example, the Wisconsin Water Science Center would be written “USGS-WI”. For all available organization IDs, please see this list of org ids. The id is listed in the “value” field, but they are accompanied by the organization name in the “desc” (description) field. siteid This is the unique identification number associated with a data collection station. Site IDs for the same location may differ depending on the reporting organization. The site ID string is written as the agency code then the site number separated by a hyphen. For example, the USGS site 01594440 would be written as “USGS-01594440”. characteristicName and characteristicType Unlike NWIS, WQP does not have codes for each parameter. Instead, you need to search based on the name of the water quality constituent (referred to as characteristicName in dataRetrieval) or a group of parameters (characteristicType in dataRetrieval). For example, “Nitrate” is a characteristicName and “Nutrient” is the characteristicType that it fits into. For a complete list of water quality types and names, see characteristicType list and characteristicName list. startDate and endDate Arguments specifying the beginning and ending of the period of record you are interested in. For the dataRetrieval functions, these can be a date or character class in the form YYYY-MM-DD. For example, startDate = as.Date(&quot;2010-01-01&quot;) or startDate = &quot;2010-01-01&quot; could both be your input arguments. 7.10 Discovering WQP data WQP has millions of records, and if you aren’t careful, your query could take hours because of the amount of data that met your criteria. To avoid this, you can query just for the number of records and number of sites that meet your criteria using the argument querySummary=TRUE in the function, readWQPdata. See the lesson on downloading WQP data to learn more about getting data. You can also use whatWQPsites to get the site information that matches your criteria. Let’s follow a similar pattern to NWIS data discovery sections and explore available stream temperature data in South Carolina. 7.10.1 readWQPdata + querySummary readWQPdata is the function used to actually download WQP data. In this application, we are just querying for a count of sites and results that match our criteria. Since WQP expect state and county codes as their FIPS code, you will need to use the string “US:45” for South Carolina. wqpcounts_sc &lt;- readWQPdata(statecode=&quot;US:45&quot;, querySummary = TRUE) names(wqpcounts_sc) ## [1] &quot;content-type&quot; &quot;content-length&quot; ## [3] &quot;server&quot; &quot;date&quot; ## [5] &quot;content-disposition&quot; &quot;total-site-count&quot; ## [7] &quot;nwis-site-count&quot; &quot;storet-site-count&quot; ## [9] &quot;total-activity-count&quot; &quot;nwis-activity-count&quot; ## [11] &quot;storet-activity-count&quot; &quot;total-result-count&quot; ## [13] &quot;nwis-result-count&quot; &quot;storet-result-count&quot; ## [15] &quot;x-frame-options&quot; &quot;x-content-type-options&quot; ## [17] &quot;x-xss-protection&quot; &quot;strict-transport-security&quot; ## [19] &quot;x-cache&quot; &quot;via&quot; ## [21] &quot;x-amz-cf-pop&quot; &quot;x-amz-cf-id&quot; ## [23] &quot;age&quot; This returns a list with 23 different items, including total number of sites, breakdown of the number of sites by source (BioData, NWIS, STORET), total number of records, and breakdown of records count by source. Let’s just look at total number of sites and total number of records. wqpcounts_sc[[&#39;total-site-count&#39;]] ## [1] 7032 wqpcounts_sc[[&#39;total-result-count&#39;]] ## [1] 3613945 This doesn’t provide any information about the sites, just the total number. I know that with 3,613,945 results, I will want to add more criteria before trying to download. Let’s continue to add query parameters before moving to whatWQPsites. # specify that you only want data from streams wqpcounts_sc_stream &lt;- readWQPdata(statecode=&quot;US:45&quot;, siteType=&quot;Stream&quot;, querySummary = TRUE) wqpcounts_sc_stream[[&#39;total-site-count&#39;]] ## [1] 1994 wqpcounts_sc_stream[[&#39;total-result-count&#39;]] ## [1] 1851691 1,851,691 results are still a lot to download. Let’s add more levels of criteria: # specify that you want water temperature data and it should be from 1975 or later wqpcounts_sc_stream_temp &lt;- readWQPdata(statecode=&quot;US:45&quot;, siteType=&quot;Stream&quot;, characteristicName=&quot;Temperature, water&quot;, startDate=&quot;1975-01-01&quot;, querySummary = TRUE) wqpcounts_sc_stream_temp[[&#39;total-site-count&#39;]] ## [1] 1462 wqpcounts_sc_stream_temp[[&#39;total-result-count&#39;]] ## [1] 140424 140,424 is little more manageble. We can also easily compare avilable stream temperature and lake temperature data. wqpcounts_sc_lake_temp &lt;- readWQPdata(statecode=&quot;US:45&quot;, siteType=&quot;Lake, Reservoir, Impoundment&quot;, characteristicName=&quot;Temperature, water&quot;, startDate=&quot;1975-01-01&quot;, querySummary = TRUE) # comparing site counts wqpcounts_sc_stream_temp[[&#39;total-site-count&#39;]] ## [1] 1462 wqpcounts_sc_lake_temp[[&#39;total-site-count&#39;]] ## [1] 626 # comparing result counts wqpcounts_sc_stream_temp[[&#39;total-result-count&#39;]] ## [1] 140424 wqpcounts_sc_lake_temp[[&#39;total-result-count&#39;]] ## [1] 51878 From these query results, it looks like South Carolina has much more stream data than it does lake data. Now, let’s try our South Carolina stream temperature query with whatWQPsites and see if we can narrow the results at all. 7.10.2 whatWQPsites whatWQPsites gives back site information that matches your search criteria. You can use any of the regular WQP web service arguments here. We are going to use whatWQPsites with the final criteria of the last query summary call - state, site type, parameter, and the earliest start date. This should return the same amount of sites as the last readWQPdata query did, 1,462. # Getting the number of sites and results for stream # temperature measurements in South Carolina after 1975. wqpsites_sc_stream_temp &lt;- whatWQPsites(statecode=&quot;US:45&quot;, siteType=&quot;Stream&quot;, characteristicName=&quot;Temperature, water&quot;, startDate=&quot;1975-01-01&quot;) # number of sites nrow(wqpsites_sc_stream_temp) ## [1] 1462 # names of available columns names(wqpsites_sc_stream_temp) ## [1] &quot;OrganizationIdentifier&quot; ## [2] &quot;OrganizationFormalName&quot; ## [3] &quot;MonitoringLocationIdentifier&quot; ## [4] &quot;MonitoringLocationName&quot; ## [5] &quot;MonitoringLocationTypeName&quot; ## [6] &quot;MonitoringLocationDescriptionText&quot; ## [7] &quot;HUCEightDigitCode&quot; ## [8] &quot;DrainageAreaMeasure.MeasureValue&quot; ## [9] &quot;DrainageAreaMeasure.MeasureUnitCode&quot; ## [10] &quot;ContributingDrainageAreaMeasure.MeasureValue&quot; ## [11] &quot;ContributingDrainageAreaMeasure.MeasureUnitCode&quot; ## [12] &quot;LatitudeMeasure&quot; ## [13] &quot;LongitudeMeasure&quot; ## [14] &quot;SourceMapScaleNumeric&quot; ## [15] &quot;HorizontalAccuracyMeasure.MeasureValue&quot; ## [16] &quot;HorizontalAccuracyMeasure.MeasureUnitCode&quot; ## [17] &quot;HorizontalCollectionMethodName&quot; ## [18] &quot;HorizontalCoordinateReferenceSystemDatumName&quot; ## [19] &quot;VerticalMeasure.MeasureValue&quot; ## [20] &quot;VerticalMeasure.MeasureUnitCode&quot; ## [21] &quot;VerticalAccuracyMeasure.MeasureValue&quot; ## [22] &quot;VerticalAccuracyMeasure.MeasureUnitCode&quot; ## [23] &quot;VerticalCollectionMethodName&quot; ## [24] &quot;VerticalCoordinateReferenceSystemDatumName&quot; ## [25] &quot;CountryCode&quot; ## [26] &quot;StateCode&quot; ## [27] &quot;CountyCode&quot; ## [28] &quot;AquiferName&quot; ## [29] &quot;FormationTypeText&quot; ## [30] &quot;AquiferTypeName&quot; ## [31] &quot;ConstructionDateText&quot; ## [32] &quot;WellDepthMeasure.MeasureValue&quot; ## [33] &quot;WellDepthMeasure.MeasureUnitCode&quot; ## [34] &quot;WellHoleDepthMeasure.MeasureValue&quot; ## [35] &quot;WellHoleDepthMeasure.MeasureUnitCode&quot; ## [36] &quot;ProviderName&quot; Similar to what we did with the NWIS functions, we can filter the sites further using the available metadata in wqpsites_sc_stream_temp. We are going to imagine that for our study the sites must have an associated drainage area and cannot be below sea level. Using dplyr::filter: # Filtering the number of sites and results for stream temperature # measurements in South Carolina after 1975 to also have an # associated drainage area and collected above sea level. wqpsites_sc_stream_temp_applicable &lt;- wqpsites_sc_stream_temp %&gt;% filter(!is.na(DrainageAreaMeasure.MeasureValue), VerticalMeasure.MeasureValue &gt; 0) nrow(wqpsites_sc_stream_temp_applicable) ## [1] 74 This brings the count down to a much more manageable 74 sites. Now we are ready to download this data. 7.11 readNWIS functions We have learned how to discover data available in NWIS, but now we will look at how to retrieve data. There are many functions to do this, see the table below for a description of each. Each variation of readNWIS is accessing a different web service. For a definition and more information on each of these services, please see https://waterservices.usgs.gov/rest/. Also, refer to the previous lesson for a description of the major arguments to readNWIS functions. Table 1. readNWIS function definitions Function Description Arguments readNWISdata Most general NWIS data import function. User must explicitly define the service parameter. More flexible than the other functions. …, asDateTime, convertType, tz readNWISdv Returns time-series data summarized to a day. Default is mean daily. siteNumbers, parameterCd, startDate, endDate, statCd readNWISgwl Groundwater levels. siteNumbers, startDate, endDate, convertType, tz readNWISmeas Surface water measurements. siteNumbers, startDate, endDate, tz, expanded, convertType readNWISpCode Metadata information for one or many parameter codes. parameterCd readNWISpeak Annual maximum instantaneous streamflows and gage heights. siteNumbers, startDate, endDate, asDateTime, convertType readNWISqw Discrete water quality data. siteNumbers, parameterCd, startDate, endDate, expanded, reshape, tz readNWISrating Rating table information for active stream gages siteNumber, type, convertType readNWISsite Site metadata information siteNumbers readNWISstat Daily, monthly, or annual statistics for time-series data. Default is mean daily. siteNumbers, parameterCd, startDate, endDate, convertType, statReportType, statType readNWISuse Data from the USGS National Water Use Program. stateCd, countyCd, years, categories, convertType, transform readNWISuv Returns time-series data reported from the USGS Instantaneous Values Web Service. siteNumbers, parameterCd, startDate, endDate, tz Each service-specific function is a wrapper for the more flexible readNWISdata. They set a default for the service argument and have limited user defined arguments. All readNWIS functions require a “major filter” as an argument, but readNWISdata can accept any major filter while others are limited to site numbers or state/county codes (see Table 1 for more info). Other major filters that can be used in readNWISdata include hydrologic unit codes (huc) and bounding boxes (bBox). More information about major filters can be found in the NWIS web services documentation. The following are examples of how to use each of the readNWIS family of functions. Don’t forget to load the dataRetrieval library if you are in a new session. readNWISdata, county major filter readNWISdata, huc major filter readNWISdata, bbox major filter readNWISdv readNWISgwl readNWISmeas readNWISpCode readNWISpeak readNWISqw, multiple sites readNWISqw, multiple parameters readNWISrating, using base table readNWISrating, corrected table readNWISrating, shift table readNWISsite readNWISstat readNWISuse readNWISuv 7.11.1 readNWISdata This function is the generic, catch-all for pulling down NWIS data. It can accept a number of arguments, but the argument name must be included. To use this function, you need to specify at list one major filter (state, county, site number, huc, or bounding box) and the NWIS service (daily value, instantaneous, groundwater, etc). The rest are optional query parameters. Follow along with the three examples below or see ?readNWISdata for more information. Historic mean daily streamflow for sites in Maui County, Hawaii. # Major filter: Maui County ## need to also include the state when using counties as the major filter # Service: daily value, dv # Parameter code: streamflow in cfs, 00060 MauiCo_avgdailyQ &lt;- readNWISdata(stateCd=&quot;Hawaii&quot;, countyCd=&quot;Maui&quot;, service=&quot;dv&quot;, parameterCd=&quot;00060&quot;) head(MauiCo_avgdailyQ) ## agency_cd site_no dateTime X_00060_00003 X_00060_00003_cd tz_cd ## 1 USGS 16400000 2020-11-03 10.4 P UTC ## 2 USGS 16401000 1929-08-31 18.0 A UTC ## 3 USGS 16402000 1957-07-31 51.0 A UTC ## 4 USGS 16403000 1957-06-30 5.5 A UTC ## 5 USGS 16403600 1970-09-29 2.4 A UTC ## 6 USGS 16403900 1996-09-30 1.3 A UTC # How many sites are returned? length(unique(MauiCo_avgdailyQ$site_no)) ## [1] 132 Historic minimum water temperatures for the HUC8 corresponding to the island of Maui, Hawaii. To see all HUCs available, visit https://water.usgs.gov/GIS/huc_name.html. The default statistic for daily values in readNWISdata is to return the max (00001), min (00002), and mean (00003). We will specify the minimum only for this example. You will need to use the statistic code, not the name. For all the available statistic codes, see the statType web service documentation and NWIS table mapping statistic names to codes. Caution! In readNWISdata and readNWISdv the argument is called statCd, but in readNWISstat the argument is statType. # Major filter: HUC 8 for Maui, 20020000 # Service: daily value, dv # Statistic: minimum, 00002 # Parameter code: water temperature in deg C, 00010 MauiHUC8_mindailyT &lt;- readNWISdata(huc=&quot;20020000&quot;, service=&quot;dv&quot;, statCd=&quot;00002&quot;, parameterCd=&quot;00010&quot;) head(MauiHUC8_mindailyT) ## agency_cd site_no dateTime X_00010_00002 X_00010_00002_cd tz_cd ## 1 USGS 16508000 2003-11-24 17.4 A UTC ## 2 USGS 16516000 2003-11-24 16.3 A UTC ## 3 USGS 16520000 2004-04-14 17.5 A UTC ## 4 USGS 16527000 2004-01-13 15.4 A UTC ## 5 USGS 16555000 2004-01-13 16.4 A UTC ## 6 USGS 16618000 2020-11-03 20.8 P UTC # How many sites are returned? length(unique(MauiHUC8_mindailyT$site_no)) ## [1] 47 Total nitrogen in mg/L for last 30 days around Great Salt Lake in Utah. This example uses Sys.Date to get the most recent date, so your dates will differ. To get any data around Great Salt Lake, we will use a bounding box as the major filter. The bounding box must be a vector of decimal numbers indicating the western longitude, southern latitude, eastern longitude, and northern latitude. The vector must be in that order. # Major filter: bounding box around Great Salt Lake # Service: water quality, qw # Parameter code: total nitrogen in mg/L, 00600 # Beginning: this past 30 days, use Sys.Date() prev30days &lt;- Sys.Date() - 30 SaltLake_totalN &lt;- readNWISdata(bBox=c(-113.0428, 40.6474, -112.0265, 41.7018), service=&quot;qw&quot;, parameterCd=&quot;00600&quot;, startDate=prev30days) # This service returns a lot of columns: names(SaltLake_totalN) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; ## [3] &quot;sample_dt&quot; &quot;sample_tm&quot; ## [5] &quot;sample_end_dt&quot; &quot;sample_end_tm&quot; ## [7] &quot;sample_start_time_datum_cd&quot; &quot;tm_datum_rlbty_cd&quot; ## [9] &quot;coll_ent_cd&quot; &quot;medium_cd&quot; ## [11] &quot;tu_id&quot; &quot;body_part_id&quot; ## [13] &quot;p00003&quot; &quot;p00004&quot; ## [15] &quot;p00009&quot; &quot;p00010&quot; ## [17] &quot;p00020&quot; &quot;p00025&quot; ## [19] &quot;p00061&quot; &quot;p00065&quot; ## [21] &quot;p00095&quot; &quot;p00098&quot; ## [23] &quot;p00191&quot; &quot;p00300&quot; ## [25] &quot;p00301&quot; &quot;p00400&quot; ## [27] &quot;p30207&quot; &quot;p30209&quot; ## [29] &quot;p50280&quot; &quot;p70305&quot; ## [31] &quot;p71999&quot; &quot;p72263&quot; ## [33] &quot;p99156&quot; &quot;p99206&quot; ## [35] &quot;startDateTime&quot; # How many sites are returned? length(unique(SaltLake_totalN$site_no)) ## [1] 5 7.11.2 readNWISdv This function is the daily value service function. It has a limited number of arguments and requires a site number and parameter code. Follow along with the example below or see ?readNWISdv for more information. Minimum and maximum pH daily data for a site on the Missouri River near Townsend, MT. # Remember, you can always use whatNWISdata to see what is available at the site before querying mt_available &lt;- whatNWISdata(siteNumber=&quot;462107111312301&quot;, service=&quot;dv&quot;, parameterCd=&quot;00400&quot;) head(mt_available) ## agency_cd site_no station_nm ## 4 USGS 462107111312301 Missouri River ab Canyon Ferry nr Townsend ## 5 USGS 462107111312301 Missouri River ab Canyon Ferry nr Townsend ## 6 USGS 462107111312301 Missouri River ab Canyon Ferry nr Townsend ## site_tp_cd dec_lat_va dec_long_va coord_acy_cd dec_coord_datum_cd alt_va ## 4 ST 46.35188 -111.5239 S NAD83 3790 ## 5 ST 46.35188 -111.5239 S NAD83 3790 ## 6 ST 46.35188 -111.5239 S NAD83 3790 ## alt_acy_va alt_datum_cd huc_cd data_type_cd parm_cd stat_cd ts_id ## 4 20 NGVD29 10030101 dv 00400 00001 82218 ## 5 20 NGVD29 10030101 dv 00400 00002 82219 ## 6 20 NGVD29 10030101 dv 00400 00008 82220 ## loc_web_ds medium_grp_cd parm_grp_cd srs_id access_cd begin_date end_date ## 4 NA wat &lt;NA&gt; 17028275 0 2010-08-18 2011-09-21 ## 5 NA wat &lt;NA&gt; 17028275 0 2010-08-18 2011-09-21 ## 6 NA wat &lt;NA&gt; 17028275 0 2010-08-18 2011-09-21 ## count_nu ## 4 72 ## 5 72 ## 6 72 # Major filter: site number, 462107111312301 # Statistic: minimum and maximum, 00001 and 00002 # Parameter: pH, 00400 mt_site_pH &lt;- readNWISdv(siteNumber=&quot;462107111312301&quot;, parameterCd=&quot;00400&quot;, statCd=c(&quot;00001&quot;, &quot;00002&quot;)) head(mt_site_pH) ## agency_cd site_no Date X_00400_00001 X_00400_00001_cd ## 1 USGS 462107111312301 2010-08-18 8.9 A ## 2 USGS 462107111312301 2010-08-19 8.9 A ## 3 USGS 462107111312301 2010-08-20 8.9 A ## 4 USGS 462107111312301 2010-08-21 8.9 A ## 5 USGS 462107111312301 2010-08-22 8.8 A ## 6 USGS 462107111312301 2010-08-23 8.9 A ## X_00400_00002 X_00400_00002_cd ## 1 8.3 A ## 2 8.3 A ## 3 8.4 A ## 4 8.4 A ## 5 8.4 A ## 6 8.4 A 7.11.3 readNWISgwl This function is the groundwater level service function. It has a limited number of arguments and requires a site number. Follow along with the example below or see ?readNWISgwl for more information. Historic groundwater levels for a site near Portland, Oregon. # Major filter: site number, 452840122302202 or_site_gwl &lt;- readNWISgwl(siteNumbers=&quot;452840122302202&quot;) head(or_site_gwl) ## agency_cd site_no site_tp_cd lev_dt lev_tm lev_tz_cd_reported ## 1 USGS 452840122302202 GW 1988-03-14 &lt;NA&gt; UTC ## 2 USGS 452840122302202 GW 1988-04-05 10:50 PDT ## 3 USGS 452840122302202 GW 1988-06-16 15:00 PDT ## 4 USGS 452840122302202 GW 1988-07-19 15:33 PDT ## 5 USGS 452840122302202 GW 1988-08-30 15:20 PDT ## 6 USGS 452840122302202 GW 1988-10-03 14:39 PDT ## lev_va sl_lev_va sl_datum_cd lev_status_cd lev_agency_cd lev_dt_acy_cd ## 1 9.78 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; D ## 2 8.77 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; m ## 3 10.59 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; m ## 4 11.62 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; m ## 5 12.13 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; m ## 6 12.25 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; m ## lev_acy_cd lev_src_cd lev_meth_cd lev_age_cd lev_dateTime lev_tz_cd ## 1 2 U U A &lt;NA&gt; UTC ## 2 2 U S A 1988-04-05 17:50:00 UTC ## 3 2 U S A 1988-06-16 22:00:00 UTC ## 4 2 U S A 1988-07-19 22:33:00 UTC ## 5 2 U S A 1988-08-30 22:20:00 UTC ## 6 2 U S A 1988-10-03 21:39:00 UTC 7.11.4 readNWISmeas This function is the field measurement service function which pulls manual measurements for streamflow and gage height. It has a limited number of arguments and requires a site number. Follow along with the example below or see ?readNWISmeas for more information. Historic surface water measurements for a site near Dade City, Florida. # Major filter: site number, 02311500 fl_site_meas &lt;- readNWISmeas(siteNumbers=&quot;02311500&quot;) # Names of columns returned: names(fl_site_meas) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; ## [3] &quot;measurement_nu&quot; &quot;measurement_dt&quot; ## [5] &quot;measurement_tm&quot; &quot;tz_cd_reported&quot; ## [7] &quot;q_meas_used_fg&quot; &quot;party_nm&quot; ## [9] &quot;site_visit_coll_agency_cd&quot; &quot;gage_height_va&quot; ## [11] &quot;discharge_va&quot; &quot;measured_rating_diff&quot; ## [13] &quot;gage_va_change&quot; &quot;gage_va_time&quot; ## [15] &quot;control_type_cd&quot; &quot;discharge_cd&quot; ## [17] &quot;measurement_dateTime&quot; &quot;tz_cd&quot; 7.11.5 readNWISpCode This function returns the parameter information associated with a parameter code. It only has one argument - the parameter code. See the example below or ?readNWISpCode for more information. Get information about the parameters gage height, specific conductance, and total phosphorus. This function only has one argument, the parameter code. You can supply one or multiple and you will get a dataframe with information about each parameter. # gage height, 00065 readNWISpCode(&quot;00065&quot;) ## parameter_cd parameter_group_nm parameter_nm casrn srsname ## 1521 00065 Physical Gage height, feet &lt;NA&gt; Height, gage ## parameter_units ## 1521 ft # specific conductance and total phosphorus, 00095 and 00665 readNWISpCode(c(&quot;00095&quot;, &quot;00665&quot;)) ## parameter_cd parameter_group_nm ## 1536 00095 Physical ## 2740 00665 Nutrient ## parameter_nm ## 1536 Specific conductance, water, unfiltered, microsiemens per centimeter at 25 degrees Celsius ## 2740 Phosphorus, water, unfiltered, milligrams per liter as phosphorus ## casrn srsname parameter_units ## 1536 &lt;NA&gt; Specific conductance uS/cm @25C ## 2740 7723-14-0 Phosphorus mg/l as P 7.11.6 readNWISpeak This function is the peak flow service function. It has a limited number of arguments and requires a site number. Follow along with the example below or see ?readNWISpeak for more information. The default settings will return data where the date of the peak flow is known. To see peak flows with incomplete dates, change convertType to FALSE. This allows the date column to come through as character, keeping any incomplete or incorrect dates. Peak flow values for a site near Cassia, Florida. # Major filter: site number, 02235200 fl_site_peak &lt;- readNWISpeak(siteNumbers=&quot;02235200&quot;) fl_site_peak$peak_dt ## [1] &quot;1962-10-06&quot; &quot;1964-09-13&quot; &quot;1965-08-11&quot; &quot;1966-08-15&quot; &quot;1967-08-30&quot; ## [6] &quot;1968-09-01&quot; &quot;1968-10-22&quot; &quot;1969-10-05&quot; &quot;1971-02-10&quot; &quot;1972-04-02&quot; ## [11] &quot;1973-09-16&quot; &quot;1974-09-07&quot; &quot;1975-09-01&quot; &quot;1976-06-06&quot; NA ## [16] &quot;1978-08-08&quot; &quot;1979-09-29&quot; &quot;1980-04-04&quot; &quot;1981-09-18&quot; &quot;1982-04-12&quot; ## [21] &quot;1983-04-24&quot; &quot;1984-04-11&quot; &quot;1985-09-21&quot; &quot;1986-01-14&quot; &quot;1987-04-01&quot; ## [26] &quot;1988-09-11&quot; &quot;1989-01-24&quot; &quot;1990-02-27&quot; &quot;1991-06-02&quot; &quot;1991-10-08&quot; ## [31] &quot;1993-03-27&quot; &quot;1994-09-12&quot; &quot;1994-11-18&quot; &quot;1995-10-12&quot; &quot;1996-10-12&quot; ## [36] &quot;1998-02-21&quot; &quot;1998-10-05&quot; &quot;1999-10-08&quot; &quot;2001-09-17&quot; &quot;2002-08-16&quot; ## [41] &quot;2003-03-10&quot; &quot;2004-09-13&quot; &quot;2004-10-01&quot; &quot;2005-10-25&quot; &quot;2007-07-21&quot; ## [46] &quot;2008-08-26&quot; &quot;2009-05-26&quot; &quot;2010-03-16&quot; &quot;2011-04-07&quot; &quot;2012-08-30&quot; ## [51] &quot;2012-10-08&quot; &quot;2014-07-31&quot; &quot;2015-09-20&quot; &quot;2016-02-06&quot; &quot;2017-09-12&quot; ## [56] &quot;2018-07-06&quot; &quot;2019-08-17&quot; # Compare complete with incomplete/incorrect dates fl_site_peak_incomp &lt;- readNWISpeak(siteNumbers=&quot;02235200&quot;, convertType = FALSE) fl_site_peak_incomp$peak_dt[is.na(fl_site_peak$peak_dt)] ## [1] &quot;1977-00-00&quot; 7.11.7 readNWISqw This function is the water quality service function. It has a limited number of arguments and requires a site number and a parameter code. Follow along with the two examples below or see ?readNWISqw for more information. Dissolved oxygen for two sites near the Columbia River in Oregon for water year 2016 # Major filter: site numbers, 455415119314601 and 454554119121801 # Parameter: dissolved oxygen in mg/L, 00300 # Begin date: October 1, 2015 # End date: September 30, 2016 or_site_do &lt;- readNWISqw(siteNumbers=c(&quot;455415119314601&quot;, &quot;454554119121801&quot;), parameterCd=&quot;00300&quot;, startDate=&quot;2015-10-01&quot;, endDate=&quot;2016-09-30&quot;) ncol(or_site_do) ## [1] 34 head(or_site_do[,c(&quot;site_no&quot;,&quot;sample_dt&quot;,&quot;result_va&quot;)]) ## site_no sample_dt result_va ## 1 455415119314601 2015-10-28 0.7 ## 2 455415119314601 2016-01-20 0.1 ## 3 455415119314601 2016-03-18 0.1 ## 4 455415119314601 2016-04-21 0.4 ## 5 455415119314601 2016-06-22 0.5 ## 6 455415119314601 2016-07-28 0.0 Post Clean Water Act lead and mercury levels in McGaw, Ohio. # Major filter: site number, 03237280 # Parameter: mercury and lead in micrograms/liter, 71890 and 01049 # Begin date: January 1, 1972 oh_site_cwa &lt;- readNWISqw(siteNumbers=&quot;03237280&quot;, parameterCd=c(&quot;71890&quot;, &quot;01049&quot;), startDate=&quot;1972-01-01&quot;) nrow(oh_site_cwa) ## [1] 76 ncol(oh_site_cwa) ## [1] 34 head(oh_site_cwa[,c(&quot;parm_cd&quot;,&quot;sample_dt&quot;,&quot;result_va&quot;)]) ## parm_cd sample_dt result_va ## 1 01049 1972-06-20 0.0 ## 2 01049 1973-06-21 NA ## 3 71890 1973-06-21 0.5 ## 4 01049 1973-10-31 NA ## 5 71890 1973-10-31 0.5 ## 6 01049 1980-03-04 10.0 7.11.8 readNWISrating This function is the rating curve service function. It has a limited number of arguments and requires a site number. Follow along with the three examples below or see ?readNWISrating for more information. There are three different types of rating tables that can be accessed using the argument type. They are base, corr (corrected), and exsa (shifts). For type==&quot;base&quot; (the default), the result is a data frame with 3 columns: INDEP, DEP, and STOR. For type==&quot;corr&quot;, the resulting data frame will have 3 columns: INDEP, CORR, and CORRINDEP. For type==&quot;exsa&quot;, the data frame will have 4 columns: INDEP, DEP, STOR, and SHIFT. See below for definitions of each column. INDEP is the gage height in feet DEP is the streamflow in cubic feet per second STOR &quot;*&quot; indicates a fixed point of the rating curve, NA for non-fixed points SHIFT indicates shifting in rating for the corresponding INDEP value CORR are the corrected values of INDEP CORRINDEP are the corrected values of CORR There are also a number of attributes associated with the data.frame returned - url, queryTime, comment, siteInfo, and RATING. RATING will only be included when type is base. See this section for how to access attributes of dataRetrieval data frames. Rating tables for Mississippi River at St. Louis, MO # Major filter: site number, 07010000 # Type: default, base miss_rating_base &lt;- readNWISrating(siteNumber=&quot;07010000&quot;) head(miss_rating_base) ## INDEP DEP STOR ## 1 -10.01 31961.29 * ## 2 11.75 200384.50 * ## 3 25.00 400400.00 * ## 4 34.34 600000.00 * ## 5 45.18 919697.20 * ## 6 50.00 1100000.00 * # Major filter: site number, 07010000 # Type: corr miss_rating_corr &lt;- readNWISrating(siteNumber=&quot;07010000&quot;, type=&quot;corr&quot;) head(miss_rating_corr) ## INDEP CORR CORRINDEP ## 1 -10.23 0 -10.23 ## 2 -10.22 0 -10.22 ## 3 -10.21 0 -10.21 ## 4 -10.20 0 -10.20 ## 5 -10.19 0 -10.19 ## 6 -10.18 0 -10.18 # Major filter: site number, 07010000 # Type: exsa miss_rating_exsa &lt;- readNWISrating(siteNumber=&quot;07010000&quot;, type=&quot;exsa&quot;) head(miss_rating_exsa) ## INDEP SHIFT DEP STOR ## 1 -10.23 0.22 31961.29 &lt;NA&gt; ## 2 -10.22 0.22 32002.44 &lt;NA&gt; ## 3 -10.21 0.22 32043.63 &lt;NA&gt; ## 4 -10.20 0.22 32084.84 &lt;NA&gt; ## 5 -10.19 0.22 32126.08 &lt;NA&gt; ## 6 -10.18 0.22 32167.35 &lt;NA&gt; 7.11.9 readNWISsite This function is pulls data from a USGS site file. It only has one argument - the site number. Follow along with the example below or see ?readNWISsite for more information. Get metadata information for a site in Bronx, NY # site number, 01302020 readNWISsite(siteNumbers=&quot;01302020&quot;) ## agency_cd site_no station_nm site_tp_cd ## 1 USGS 01302020 BRONX RIVER AT NY BOTANICAL GARDEN AT BRONX NY ST ## lat_va long_va dec_lat_va dec_long_va coord_meth_cd coord_acy_cd ## 1 405144.3 735227.8 40.86231 -73.87439 N 1 ## coord_datum_cd dec_coord_datum_cd district_cd state_cd county_cd country_cd ## 1 NAD83 NAD83 36 36 005 US ## land_net_ds map_nm map_scale_fc alt_va alt_meth_cd alt_acy_va ## 1 NA FLUSHING, NY 24000 50 M 10 ## alt_datum_cd huc_cd basin_cd topo_cd instruments_cd ## 1 NAVD88 02030102 &lt;NA&gt; &lt;NA&gt; YNNNYNNNNYNNNNNNNNNNNNNNNNNNNN ## construction_dt inventory_dt drain_area_va contrib_drain_area_va tz_cd ## 1 NA NA 38.4 NA EST ## local_time_fg reliability_cd gw_file_cd nat_aqfr_cd aqfr_cd aqfr_type_cd ## 1 N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## well_depth_va hole_depth_va depth_src_cd project_no ## 1 NA NA &lt;NA&gt; NA 7.11.10 readNWISstat This function is the statistics service function. It has a limited number of arguments and requires a site number and parameter code. Follow along with the example below or see ?readNWISstat for more information. The NWIS Statistics web service is currently in Beta mode, so use at your own discretion. Additionally, “mean” is the only statType that can be used for annual or monthly report types at this time. Historic annual average discharge near Mississippi River outlet # Major filter: site number, 07374525 # Parameter: discharge in cfs, 00060 # Time division: annual # Statistic: average, &quot;mean&quot; mississippi_avgQ &lt;- readNWISstat(siteNumbers=&quot;07374525&quot;, parameterCd=&quot;00060&quot;, statReportType=&quot;annual&quot;, statType=&quot;mean&quot;) head(mississippi_avgQ) ## agency_cd site_no parameter_cd ts_id loc_web_ds year_nu mean_va ## 1 USGS 07374525 00060 61182 NA 2009 618800 ## 2 USGS 07374525 00060 61182 NA 2010 563700 ## 3 USGS 07374525 00060 61182 NA 2012 362100 ## 4 USGS 07374525 00060 61182 NA 2014 489000 ## 5 USGS 07374525 00060 61182 NA 2015 625800 7.11.11 readNWISuse This function is the water use service function. The water use data web service requires a state and/or county as the major filter. The default will return all years and all categories available. The following table shows the water-use categories and their corresponding abbreviation for county and state data. Note that categories have changed over time, and vary by data sets requested. National and site-specific data sets exist, but only county/state data are available through this service. Please visit the USGS National Water Use Information Program website for more information. Table 2. Water-use category names and abbreviations. Name Abbreviation Aquaculture AQ Commercial CO Domestic DO Hydroelectric Power HY Irrigation, Crop IC Irrigation, Golf Courses IG Industrial IN Total Irrigation IT Livestock (Animal Specialties) LA Livestock LI Livestock (Stock) LS Mining MI Other Industrial OI Thermoelectric Power (Closed-loop cooling) PC Fossil-fuel Thermoelectric Power PF Geothermal Thermoelectric Power PG Nuclear Thermoelectric Power PN Thermoelectric Power (Once-through cooling) PO Public Supply PS Total Power PT Total Thermoelectric Power PT Reservoir Evaporation RE Total Population TP Wastewater Treatment WW Follow along with the example below or see ?readNWISuse for more information. Las Vegas historic water use # Major filter: Clark County, NV # Water-use category: public supply, PS vegas_wu &lt;- readNWISuse(stateCd=&quot;NV&quot;, countyCd=&quot;Clark&quot;, categories=&quot;PS&quot;) ncol(vegas_wu) ## [1] 26 names(vegas_wu) ## [1] &quot;state_cd&quot; ## [2] &quot;state_name&quot; ## [3] &quot;county_cd&quot; ## [4] &quot;county_nm&quot; ## [5] &quot;year&quot; ## [6] &quot;Public.Supply.population.served.by.groundwater..in.thousands&quot; ## [7] &quot;Public.Supply.population.served.by.surface.water..in.thousands&quot; ## [8] &quot;Public.Supply.total.population.served..in.thousands&quot; ## [9] &quot;Public.Supply.self.supplied.groundwater.withdrawals..fresh..in.Mgal.d&quot; ## [10] &quot;Public.Supply.self.supplied.groundwater.withdrawals..saline..in.Mgal.d&quot; ## [11] &quot;Public.Supply.total.self.supplied.withdrawals..groundwater..in.Mgal.d&quot; ## [12] &quot;Public.Supply.self.supplied.surface.water.withdrawals..fresh..in.Mgal.d&quot; ## [13] &quot;Public.Supply.self.supplied.surface.water.withdrawals..saline..in.Mgal.d&quot; ## [14] &quot;Public.Supply.total.self.supplied.withdrawals..surface.water..in.Mgal.d&quot; ## [15] &quot;Public.Supply.total.self.supplied.withdrawals..fresh..in.Mgal.d&quot; ## [16] &quot;Public.Supply.total.self.supplied.withdrawals..saline..in.Mgal.d&quot; ## [17] &quot;Public.Supply.total.self.supplied.withdrawals..total..in.Mgal.d&quot; ## [18] &quot;Public.Supply.deliveries.to.domestic..in.Mgal.d&quot; ## [19] &quot;Public.Supply.deliveries.to.commercial..in.Mgal.d&quot; ## [20] &quot;Public.Supply.deliveries.to.industrial..in.Mgal.d&quot; ## [21] &quot;Public.Supply.deliveries.to.thermoelectric..in.Mgal.d&quot; ## [22] &quot;Public.Supply.total.deliveries..in.Mgal.d&quot; ## [23] &quot;Public.Supply.public.use.and.losses..in.Mgal.d&quot; ## [24] &quot;Public.Supply.per.capita.use..in.gallons.person.day&quot; ## [25] &quot;Public.Supply.reclaimed.wastewater..in.Mgal.d&quot; ## [26] &quot;Public.Supply.number.of.facilities&quot; head(vegas_wu[,1:7]) ## state_cd state_name county_cd county_nm year ## 1 32 Nevada 003 Clark County 1985 ## 2 32 Nevada 003 Clark County 1990 ## 3 32 Nevada 003 Clark County 1995 ## 4 32 Nevada 003 Clark County 2000 ## 5 32 Nevada 003 Clark County 2005 ## 6 32 Nevada 003 Clark County 2010 ## Public.Supply.population.served.by.groundwater..in.thousands ## 1 149.770 ## 2 108.140 ## 3 128.010 ## 4 176.850 ## 5 - ## 6 - ## Public.Supply.population.served.by.surface.water..in.thousands ## 1 402.210 ## 2 618.000 ## 3 844.060 ## 4 1169.600 ## 5 - ## 6 - 7.11.12 readNWISuv This function is the unit value (or instantaneous) service function. It has a limited number of arguments and requires a site number and parameter code. Follow along with the example below or see ?readNWISuv for more information. Turbidity and discharge for April 2016 near Lake Tahoe in California. # Major filter: site number, 10336676 # Parameter: discharge in cfs and turbidity in FNU, 00060 and 63680 # Begin date: April 1, 2016 # End date: April 30, 2016 ca_site_do &lt;- readNWISuv(siteNumbers=&quot;10336676&quot;, parameterCd=c(&quot;00060&quot;, &quot;63680&quot;), startDate=&quot;2016-04-01&quot;, endDate=&quot;2016-04-30&quot;) nrow(ca_site_do) ## [1] 2880 head(ca_site_do) ## agency_cd site_no dateTime X_00060_00000 X_00060_00000_cd ## 1 USGS 10336676 2016-04-01 07:00:00 28.9 A ## 2 USGS 10336676 2016-04-01 07:15:00 28.2 A ## 3 USGS 10336676 2016-04-01 07:30:00 28.2 A ## 4 USGS 10336676 2016-04-01 07:45:00 28.9 A ## 5 USGS 10336676 2016-04-01 08:00:00 28.9 A ## 6 USGS 10336676 2016-04-01 08:15:00 28.2 A ## X_63680_00000 X_63680_00000_cd tz_cd ## 1 1.2 A UTC ## 2 1.3 A UTC ## 3 1.2 A UTC ## 4 1.1 A UTC ## 5 1.2 A UTC ## 6 1.3 A UTC 7.12 Additional Features 7.12.1 Accessing attributes dataRetrieval returns a lot of useful information as “attributes” to the data returned. This includes site metadata information, the NWIS url used, date and time the query was performed, and more. First, you want to use attributes() to see what information is available. It returns a list of all the metadata information. Then you can use attr() to actually get that information. Let’s use the base rating table example from before to illustrate this. It has a special attribute called “RATING”. # Major filter: site number, 07010000 # Type: default, base miss_rating_base &lt;- readNWISrating(siteNumber=&quot;07010000&quot;) # how many attributes are there and what are they? length(attributes(miss_rating_base)) ## [1] 9 names(attributes(miss_rating_base)) ## [1] &quot;names&quot; &quot;class&quot; &quot;row.names&quot; &quot;comment&quot; &quot;queryTime&quot; &quot;url&quot; ## [7] &quot;header&quot; &quot;RATING&quot; &quot;siteInfo&quot; # look at the site info attr(miss_rating_base, &quot;siteInfo&quot;) ## agency_cd site_no station_nm site_tp_cd lat_va ## 1 USGS 07010000 Mississippi River at St. Louis, MO ST 383744.4 ## long_va dec_lat_va dec_long_va coord_meth_cd coord_acy_cd coord_datum_cd ## 1 901047.2 38.629 -90.17978 N 5 NAD83 ## dec_coord_datum_cd district_cd state_cd county_cd country_cd ## 1 NAD83 29 29 510 US ## land_net_ds map_nm map_scale_fc alt_va alt_meth_cd ## 1 S T45N R07E 5 GRANITE CITY 24000 379.58 L ## alt_acy_va alt_datum_cd huc_cd basin_cd topo_cd ## 1 0.05 NAVD88 07140101 &lt;NA&gt; &lt;NA&gt; ## instruments_cd construction_dt inventory_dt drain_area_va ## 1 NNNNYNNNNNNNNNNNNNNNNNNNNNNNNN NA 19891229 697000 ## contrib_drain_area_va tz_cd local_time_fg reliability_cd gw_file_cd ## 1 NA CST Y C NNNNNNNN ## nat_aqfr_cd aqfr_cd aqfr_type_cd well_depth_va hole_depth_va depth_src_cd ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; ## project_no ## 1 NA # now look at the special RATING attribute attr(miss_rating_base, &quot;RATING&quot;) ## [1] &quot;ID=18.0&quot; ## [2] &quot;TYPE=STGQ&quot; ## [3] &quot;NAME=stage-discharge&quot; ## [4] &quot;AGING=Working&quot; ## [5] &quot;REMARKS= developed for trend left of rating in the 35&#39;range&quot; ## [6] &quot;EXPANSION=logarithmic&quot; ## [7] &quot;OFFSET1=-2.800000E+01&quot; All attributes are an R object once you extract them. They can be lists, data.frames, vectors, etc. If we want to use information from one of the attributes, index it just like you would any other object of that type. For example, we want the drainage area for this Mississippi site: # save site info metadata as its own object miss_site_info &lt;- attr(miss_rating_base, &quot;siteInfo&quot;) class(miss_site_info) ## [1] &quot;data.frame&quot; # extract the drainage area miss_site_info$drain_area_va ## [1] 697000 7.12.2 Using lists as input readNWISdata allows users to give a list of named arguments as the input to the call. This is especially handy if you would like to build up a list of arguments and use it in multiple calls. This only works in readNWISdata, none of the other readNWIS... functions have this ability. chicago_q_args &lt;- list(siteNumbers=c(&quot;05537500&quot;, &quot;05536358&quot;, &quot;05531045&quot;), startDate=&quot;2015-10-01&quot;, endDate=&quot;2015-12-31&quot;, parameterCd=&quot;00060&quot;) # query for unit value data with those arguments chicago_q_uv &lt;- readNWISdata(chicago_q_args, service=&quot;uv&quot;) nrow(chicago_q_uv) ## [1] 14488 # same query but for daily values chicago_q_dv &lt;- readNWISdata(chicago_q_args, service=&quot;dv&quot;) nrow(chicago_q_dv) ## [1] 151 7.12.3 Helper functions There are currently 3 helper functions: renameNWIScolumns, addWaterYear, and zeroPad. renameNWIScolumns takes some of the default column names and makes them more human-readable (e.g. “X_00060_00000” becomes “Flow_Inst”). addWaterYear adds an additional column of integers indicating the water year. zeroPad is used to add leading zeros to any string that is missing them, and is not restricted to dataRetrieval output. renameNWIScolumns renameNWIScolumns can be used in two ways: it can be a standalone function following the dataRetrieval call or it can be piped (as long as magrittr or dplyr are loaded). Both examples are shown below. Note that renameNWIScolumns is intended for use with columns named using pcodes. It does not work with all possible data returned. # get discharge and temperature data for July 2016 in Ft Worth, TX ftworth_qt_july &lt;- readNWISuv(siteNumbers=&quot;08048000&quot;, parameterCd=c(&quot;00060&quot;, &quot;00010&quot;), startDate=&quot;2016-07-01&quot;, endDate=&quot;2016-07-31&quot;) names(ftworth_qt_july) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; &quot;dateTime&quot; &quot;X_00010_00000&quot; ## [5] &quot;X_00010_00000_cd&quot; &quot;X_00060_00000&quot; &quot;X_00060_00000_cd&quot; &quot;tz_cd&quot; # now rename columns ftworth_qt_july_renamed &lt;- renameNWISColumns(ftworth_qt_july) names(ftworth_qt_july_renamed) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; &quot;dateTime&quot; &quot;Wtemp_Inst&quot; ## [5] &quot;Wtemp_Inst_cd&quot; &quot;Flow_Inst&quot; &quot;Flow_Inst_cd&quot; &quot;tz_cd&quot; Now try with a pipe. Remember to load a packge that uses %&gt;%. library(magrittr) # get discharge and temperature data for July 2016 in Ft Worth, TX # pipe straight into rename function ftworth_qt_july_pipe &lt;- readNWISuv(siteNumbers=&quot;08048000&quot;, parameterCd=c(&quot;00060&quot;, &quot;00010&quot;), startDate=&quot;2016-07-01&quot;, endDate=&quot;2016-07-31&quot;) %&gt;% renameNWISColumns() names(ftworth_qt_july_pipe) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; &quot;dateTime&quot; &quot;Wtemp_Inst&quot; ## [5] &quot;Wtemp_Inst_cd&quot; &quot;Flow_Inst&quot; &quot;Flow_Inst_cd&quot; &quot;tz_cd&quot; addWaterYear Similar to renameNWIScolumns, addWaterYear can be used as a standalone function or with a pipe. This function defines a water year as October 1 of the previous year to September 30 of the current year. Additionally, addWaterYear is limited to data.frames with date columns titled “dateTime”, “Date”, “ActivityStartDate”, and “ActivityEndDate”. # mean daily discharge on the Colorado River in Grand Canyon National Park for fall of 2014 # The dates in Sept should be water year 2014, but the dates in Oct and Nov are water year 2015 co_river_q_fall &lt;- readNWISdv(siteNumber=&quot;09403850&quot;, parameterCd=&quot;00060&quot;, startDate=&quot;2014-09-28&quot;, endDate=&quot;2014-11-30&quot;) head(co_river_q_fall) ## agency_cd site_no Date X_00060_00003 X_00060_00003_cd ## 1 USGS 09403850 2014-09-28 319.00 A ## 2 USGS 09403850 2014-09-29 411.00 A ## 3 USGS 09403850 2014-09-30 375.00 A ## 4 USGS 09403850 2014-10-01 26.40 A ## 5 USGS 09403850 2014-10-02 10.80 A ## 6 USGS 09403850 2014-10-03 5.98 A # now add the water year column co_river_q_fall_wy &lt;- addWaterYear(co_river_q_fall) head(co_river_q_fall_wy) ## agency_cd site_no Date waterYear X_00060_00003 X_00060_00003_cd ## 1 USGS 09403850 2014-09-28 2014 319.00 A ## 2 USGS 09403850 2014-09-29 2014 411.00 A ## 3 USGS 09403850 2014-09-30 2014 375.00 A ## 4 USGS 09403850 2014-10-01 2015 26.40 A ## 5 USGS 09403850 2014-10-02 2015 10.80 A ## 6 USGS 09403850 2014-10-03 2015 5.98 A unique(co_river_q_fall_wy$waterYear) ## [1] 2014 2015 Now try with a pipe. # mean daily discharge on the Colorado River in Grand Canyon National Park for fall of 2014 # pipe straight into rename function co_river_q_fall_pipe &lt;- readNWISdv(siteNumber=&quot;09403850&quot;, parameterCd=&quot;00060&quot;, startDate=&quot;2014-09-01&quot;, endDate=&quot;2014-11-30&quot;) %&gt;% addWaterYear() names(co_river_q_fall_pipe) ## [1] &quot;agency_cd&quot; &quot;site_no&quot; &quot;Date&quot; &quot;waterYear&quot; ## [5] &quot;X_00060_00003&quot; &quot;X_00060_00003_cd&quot; head(co_river_q_fall_pipe) ## agency_cd site_no Date waterYear X_00060_00003 X_00060_00003_cd ## 1 USGS 09403850 2014-09-01 2014 3.79 A e ## 2 USGS 09403850 2014-09-02 2014 3.50 A e ## 3 USGS 09403850 2014-09-03 2014 3.54 A e ## 4 USGS 09403850 2014-09-04 2014 3.63 A e ## 5 USGS 09403850 2014-09-05 2014 3.85 A e ## 6 USGS 09403850 2014-09-06 2014 3.76 A e zeroPad zeroPad is designed to work on any string, so it is not specific to dataRetrieval data.frame output like the previous helper functions. Oftentimes, when reading in Excel or other local data, leading zeros are dropped from site numbers. This function allows you to put them back in. x is the string you would like to pad, and padTo is the total number of characters the string should have. For instance if an 8-digit site number was read in as numeric, we could pad that by: siteNum &lt;- 02121500 class(siteNum) ## [1] &quot;numeric&quot; siteNum ## [1] 2121500 siteNum_fix &lt;- zeroPad(siteNum, 8) class(siteNum_fix) ## [1] &quot;character&quot; siteNum_fix ## [1] &quot;02121500&quot; The next lesson looks at how to use dataRetrieval functions for Water Quality Portal retrievals. 7.13 readWQP functions After discovering Water Quality Portal (WQP) data in the data discovery section, we can now read it in using the desired parameters. There are two functions to do this in dataRetrieval. Table 1 describes them below. Table 1. readWQP function definitions Function Description Arguments readWQPdata Most general WQP data import function. Users must define each parameter. …, querySummary, tz, ignore_attributes readWQPqw Used for querying by site numbers and parameter codes only. siteNumbers, parameterCd, startDate, endDate, tz, querySummary The main difference between these two functions is that readWQPdata is general and accepts any of the paremeters described in the WQP Web Services Guide. In contrast, readWQPqw has five arguments and users can only use this if they know the site number(s) and parameter code(s) for which they want data. The following are examples of how to use each of the readWQP family of functions. Don’t forget to load the dataRetrieval library if you are in a new session. readWQPdata, state, site type, and characteristic name readWQPdata, county and characteristic group readWQPdata, bbox, characteristic name, and start date readWQPqw 7.13.1 readWQPdata The generic function used to pull Water Quality Portal data. This function is very flexible. You can specify any of the parameters from the WQP Web Service Guide. To learn what the possible values for each, see the table of domain values. Follow along with the three examples below or see ?readWQPdata for more information. Phosphorus data in Wisconsin lakes for water year 2010 # This takes about 3 minutes to complete. WI_lake_phosphorus_2010 &lt;- readWQPdata(statecode=&quot;WI&quot;, siteType=&quot;Lake, Reservoir, Impoundment&quot;, characteristicName=&quot;Phosphorus&quot;, startDate=&quot;2009-10-01&quot;, endDate=&quot;2010-09-30&quot;) # What columns are available? names(WI_lake_phosphorus_2010) ## [1] &quot;OrganizationIdentifier&quot; ## [2] &quot;OrganizationFormalName&quot; ## [3] &quot;ActivityIdentifier&quot; ## [4] &quot;ActivityTypeCode&quot; ## [5] &quot;ActivityMediaName&quot; ## [6] &quot;ActivityMediaSubdivisionName&quot; ## [7] &quot;ActivityStartDate&quot; ## [8] &quot;ActivityStartTime.Time&quot; ## [9] &quot;ActivityStartTime.TimeZoneCode&quot; ## [10] &quot;ActivityEndDate&quot; ## [11] &quot;ActivityEndTime.Time&quot; ## [12] &quot;ActivityEndTime.TimeZoneCode&quot; ## [13] &quot;ActivityDepthHeightMeasure.MeasureValue&quot; ## [14] &quot;ActivityDepthHeightMeasure.MeasureUnitCode&quot; ## [15] &quot;ActivityDepthAltitudeReferencePointText&quot; ## [16] &quot;ActivityTopDepthHeightMeasure.MeasureValue&quot; ## [17] &quot;ActivityTopDepthHeightMeasure.MeasureUnitCode&quot; ## [18] &quot;ActivityBottomDepthHeightMeasure.MeasureValue&quot; ## [19] &quot;ActivityBottomDepthHeightMeasure.MeasureUnitCode&quot; ## [20] &quot;ProjectIdentifier&quot; ## [21] &quot;ActivityConductingOrganizationText&quot; ## [22] &quot;MonitoringLocationIdentifier&quot; ## [23] &quot;ActivityCommentText&quot; ## [24] &quot;SampleAquifer&quot; ## [25] &quot;HydrologicCondition&quot; ## [26] &quot;HydrologicEvent&quot; ## [27] &quot;SampleCollectionMethod.MethodIdentifier&quot; ## [28] &quot;SampleCollectionMethod.MethodIdentifierContext&quot; ## [29] &quot;SampleCollectionMethod.MethodName&quot; ## [30] &quot;SampleCollectionEquipmentName&quot; ## [31] &quot;ResultDetectionConditionText&quot; ## [32] &quot;CharacteristicName&quot; ## [33] &quot;ResultSampleFractionText&quot; ## [34] &quot;ResultMeasureValue&quot; ## [35] &quot;ResultMeasure.MeasureUnitCode&quot; ## [36] &quot;MeasureQualifierCode&quot; ## [37] &quot;ResultStatusIdentifier&quot; ## [38] &quot;StatisticalBaseCode&quot; ## [39] &quot;ResultValueTypeName&quot; ## [40] &quot;ResultWeightBasisText&quot; ## [41] &quot;ResultTimeBasisText&quot; ## [42] &quot;ResultTemperatureBasisText&quot; ## [43] &quot;ResultParticleSizeBasisText&quot; ## [44] &quot;PrecisionValue&quot; ## [45] &quot;ResultCommentText&quot; ## [46] &quot;USGSPCode&quot; ## [47] &quot;ResultDepthHeightMeasure.MeasureValue&quot; ## [48] &quot;ResultDepthHeightMeasure.MeasureUnitCode&quot; ## [49] &quot;ResultDepthAltitudeReferencePointText&quot; ## [50] &quot;SubjectTaxonomicName&quot; ## [51] &quot;SampleTissueAnatomyName&quot; ## [52] &quot;ResultAnalyticalMethod.MethodIdentifier&quot; ## [53] &quot;ResultAnalyticalMethod.MethodIdentifierContext&quot; ## [54] &quot;ResultAnalyticalMethod.MethodName&quot; ## [55] &quot;MethodDescriptionText&quot; ## [56] &quot;LaboratoryName&quot; ## [57] &quot;AnalysisStartDate&quot; ## [58] &quot;ResultLaboratoryCommentText&quot; ## [59] &quot;DetectionQuantitationLimitTypeName&quot; ## [60] &quot;DetectionQuantitationLimitMeasure.MeasureValue&quot; ## [61] &quot;DetectionQuantitationLimitMeasure.MeasureUnitCode&quot; ## [62] &quot;PreparationStartDate&quot; ## [63] &quot;ProviderName&quot; ## [64] &quot;ActivityStartDateTime&quot; ## [65] &quot;ActivityEndDateTime&quot; #How much data is returned? nrow(WI_lake_phosphorus_2010) ## [1] 3264 All nutrient data in Napa County, California # Use empty character strings to specify that you want the historic record. # This takes about 3 minutes to run. Napa_lake_nutrients_Aug2010 &lt;- readWQPdata(statecode=&quot;CA&quot;, countycode=&quot;055&quot;, characteristicType=&quot;Nutrient&quot;) #How much data is returned? nrow(Napa_lake_nutrients_Aug2010) ## [1] 4835 Everglades water temperature data since 2016 # Bounding box defined by a vector of Western-most longitude, Southern-most latitude, # Eastern-most longitude, and Northern-most longitude. # This takes about 3 minutes to run. Everglades_temp_2016_present &lt;- readWQPdata(bBox=c(-81.70, 25.08, -80.30, 26.51), characteristicName=&quot;Temperature, water&quot;, startDate=&quot;2016-01-01&quot;) #How much data is returned? nrow(Everglades_temp_2016_present) ## [1] 14507 7.13.2 readWQPqw This function has a limited number of arguments - it can only be used for pulling WQP data by site number and parameter code. By default, dates are set to pull the entire record available. When specifying USGS sites as siteNumbers to readWQP functions, precede the number with “USGS-”. See the example below or ?readWQPqw for more information. Dissolved oxygen data since 2010 for 2 South Carolina USGS sites # Using a few USGS sites, get dissolved oxygen data # This takes ~ 30 seconds to complete. SC_do_data_since2010 &lt;- readWQPqw(siteNumbers = c(&quot;USGS-02146110&quot;, &quot;USGS-325427080014600&quot;), parameterCd = &quot;00300&quot;, startDate = &quot;2010-01-01&quot;) # How much data was returned? nrow(SC_do_data_since2010) ## [1] 20 # What are the DO values and the dates the sample was collected? head(SC_do_data_since2010[, c(&quot;ResultMeasureValue&quot;, &quot;ActivityStartDate&quot;)]) ## ResultMeasureValue ActivityStartDate ## 1 5.0 2011-09-06 ## 2 5.9 2010-04-08 ## 3 6.3 2011-06-15 ## 4 6.8 2014-08-13 ## 5 7.2 2011-03-09 ## 6 7.2 2014-05-13 7.14 Attributes and metadata Similar to the data frames returned from readNWIS functions, there are attributes (aka metadata) attached to the data. Use attributes to see all of them and attr to extract a particular attribute. # What are the attributes available? wqp_attributes &lt;- attributes(Everglades_temp_2016_present) names(wqp_attributes) ## [1] &quot;names&quot; &quot;class&quot; &quot;row.names&quot; &quot;siteInfo&quot; &quot;variableInfo&quot; ## [6] &quot;url&quot; &quot;queryTime&quot; # Look at the variableInfo attribute head(attr(Everglades_temp_2016_present, &quot;variableInfo&quot;)) ## characteristicName parameterCd param_units valueType ## 1 Temperature, water 00010 deg C &lt;NA&gt; ## 2 Temperature, water &lt;NA&gt; deg C &lt;NA&gt; ## 3 Temperature, water &lt;NA&gt; deg C &lt;NA&gt; ## 4 Temperature, water &lt;NA&gt; deg C &lt;NA&gt; ## 5 Temperature, water &lt;NA&gt; deg C &lt;NA&gt; ## 6 Temperature, water &lt;NA&gt; deg C &lt;NA&gt; Let’s make a quick map to look at the stations that collected the Everglades data: siteInfo &lt;- attr(Everglades_temp_2016_present, &quot;siteInfo&quot;) library(maps) map(&#39;state&#39;, regions=&#39;florida&#39;) title(main=&quot;Everglade Sites&quot;) points(x=siteInfo$dec_lon_va, y=siteInfo$dec_lat_va) # Add a rectangle to see where your original query bounding box in relation to sites rect(-81.70, 25.08, -80.30, 26.51, col = NA, border = &#39;red&#39;) Figure 7.1: A map of NWIS site locations in the Everglades You can now find and download Water Quality Portal data from R! 7.15 USGS Coding Lab Exercises 7.15.1 Just how dry was the 2020 monsoon? Recently NOAA published a press release demonstrating that 2020 was both the hottest and driest summer on record for Arizona. In this lab we will look at USGS NWIS stream gauge and preciptitation data to investigate just how anomalous 2020 data are. 1: Use the readNWISstat function to retrieve the following statewide data for 2015-2020: 1. Precipitation total (inches/week) 2. Streamflow (ft^3/s) 2: Create two timeseries plots (one for preciptitation, one for streamflow), where color is a function of year (e.g. the x axis is month, the y axis is precipitation, legend shows color by year). 3. Calculate the monthly mean precipitation and streamflow from 2015-2019, and use that mean to calculate a 2020 anomaly timeseries. Create two new plots (like #2 above) with the 2015-2019 mean as a thick black line, and 2020 anomaly as a thin red line. 7.16 geoKnife - Introduction 7.17 Lesson Summary This lesson will explore how to find and download large gridded datasets via the R package geoknife. The package was created to allow easy access to data stored in the Geo Data Portal (GDP), or any gridded dataset available through the OPeNDAP protocol DAP2. geoknife refers to the gridded dataset as the fabric, the spatial feature of interest as the stencil, and the subset algorithm parameters as the knife (see below). figure illustrating definitions of fabric, stencil, and knife 7.18 Lesson Objectives Explore and query large gridded datasets for efficient and reproducible large-scale analyses. By the end of this lesson, the learner will be able to: Explain the differences between remote and local processing. Differentiate the three main concepts of this package: fabric, stencil, and knife. Use this package to retrieve pre-processed data from the Geo Data Portal (GDP) via geoknife. 7.19 Lesson Resources Publication: geoknife: reproducible web-processing of large gridded datasets Tutorial (vignette): geoknife vignette Source code: geoknife on GitHub Report a bug or suggest a feature: geoknife issues on GitHub 7.20 Remote processing The USGS Geo Data Portal is designed to perform web-service processing on large gridded datasets. geoknife allows R users to take advantage of these services by processing large data, such as data available in the GDP catalog or a user-defined dataset, on the web. This type of workflow has three main advantages: it allows the user to avoid downloading large datasets, it avoids reinventing the wheel for the creation and optimization of complex geoprocessing algorithms, and computing resources are dedicated elsewhere, so geoknife operations do not have much of an impact on a local computer. 7.21 geoknife components: fabric, stencil, knife The main components of a geoknife workflow are the fabric, stencil, and knife. These three components go into the final element, the geoknife “job” (geojob), which returns the processed data. The fabric is the gridded web dataset to be processed, the stencil is the feature of interest, and the knife is the processing algorithm parameters. Each of the geoknife components is created using a corresponding function: fabrics are created using webdata(), stencils are created using webgeom(), and knives are created using webprocess(). This lesson will focus on discovering what options exist for each of those components. The next lesson will teach you how to construct each component and put it together to get the processed data. Before continuing this lesson, load the geoknife library (install if necessary). # load the geoknife package library(geoknife) ## Warning: package &#39;geoknife&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;geoknife&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## id ## The following object is masked from &#39;package:stats&#39;: ## ## start ## The following object is masked from &#39;package:graphics&#39;: ## ## title ## The following object is masked from &#39;package:base&#39;: ## ## url 7.22 Available webdata 7.22.1 GDP datasets To learn what data is available through GDP, you can explore the online catalog or use the function query. Note that the access pattern for determining available fabrics differs slightly from stencils and knives. Running query(&quot;webdata&quot;) will return every dataset available through GDP: all_webdata &lt;- query(&quot;webdata&quot;) head(all_webdata) ## An object of class &quot;datagroup&quot;: ## [1] 4km Monthly Parameter-elevation Regressions on Independent Slopes Model Monthly Climate Data for the Continental United States. August 2018 Snapshot ## url: https://cida.usgs.gov/thredds/dodsC/prism_v2 ## [2] 4km Monthly Parameter-elevation Regressions on Independent Slopes Model Monthly Climate Data for the Continental United States. January 2012 Shapshot ## url: https://cida.usgs.gov/thredds/dodsC/prism ## [3] ACCESS 1980-1999 ## url: https://cida.usgs.gov/thredds/dodsC/notaro_ACCESS_1980_1999 ## [4] ACCESS 2040-2059 ## url: https://cida.usgs.gov/thredds/dodsC/notaro_ACCESS_2040_2059 ## [5] ACCESS 2080-2099 ## url: https://cida.usgs.gov/thredds/dodsC/notaro_ACCESS_2080_2099 ## [6] Bias Corrected Constructed Analogs V2 Daily Future CMIP5 Climate Projections ## url: https://cida.usgs.gov/thredds/dodsC/cmip5_bcca/future length(all_webdata) ## [1] 532 Notice that the object returned is a special geoknife class of datagroup. There are specific geoknife functions that only operate on an object of this class, see ?title and ?abstract. These two functions are used to extract metadata information about each of the available GDP datasets. With 532 datasets available, it is likely that reading through each to find ones that are of interest to you would be time consuming. You can use grep along with the functions title and abstract to figure out which datasets you would like to use for processing. Let’s say that we were interested in evapotranspiration data. To search for which GDP datasets might contain evapotranspiration data, you can use the titles and abstracts. # notice that you cannot perform a grep on all_webdata - it is because it is a special class # `grep(&quot;evapotranspiration&quot;, all_webdata)` will fail # you need to perform pattern matching on vectors all_titles &lt;- title(all_webdata) which_titles &lt;- grep(&quot;evapotranspiration&quot;, all_titles) evap_titles &lt;- all_titles[which_titles] head(evap_titles) ## [1] &quot;Monthly Conterminous U.S. actual evapotranspiration data&quot; ## [2] &quot;Yearly Conterminous U.S. actual evapotranspiration data&quot; all_abstracts &lt;- abstract(all_webdata) which_abstracts &lt;- grep(&quot;evapotranspiration&quot;, all_abstracts) evap_abstracts &lt;- all_abstracts[which_abstracts] evap_abstracts[1] ## [1] &quot;Reference evapotranspiration (ET0), like potential evapotranspiration, is a measure of atmospheric evaporative demand. It was used in the context of this study to evaluate drought conditions that can lead to wildfire activity in Alaska using the Evaporative Demand Drought Index (EDDI) and the Standardized Precipitation Evapotranspiration Index (SPEI). The ET0 data are on a 20km grid with daily temporal resolution and were computed using the meteorological inputs from the dynamically downscaled ERA-Interim reanalysis and two global climate model projections (CCSM4 and GFDL-CM3). The model projections are from CMIP5 and use the RCP8.5 scenario. The dynamically downscaled data are available at https://registry.opendata.aws/wrf-alaska-snap/. The ET0 was computed following the American Society of Civil Engineers Standardized Reference Evapotranspiration Equation based on the downscaled daily temperature, humidity and winds. The full details of the computation of ET0, an evaluation of the underlying data, and the assessment of the fire indices are described in Ziel et al. (2020; https://doi.org/10.3390/f11050516).&quot; 13 possible datasets to look through is a lot more manageable than 532. Let’s say the dataset titled “Yearly Conterminous U.S. actual evapotranspiration data” interested us enough to explore more. We have now identified a fabric of interest. We might want to know more about the dataset, such as what variables and time periods are available. To actually create the fabric, you will need to use webdata and supply the appropriate datagroup object as the input. This should result in an object with a class of webdata. The following functions will operate only on an object of class webdata. evap_fabric &lt;- webdata(all_webdata[&quot;Yearly Conterminous U.S. actual evapotranspiration data&quot;]) class(evap_fabric) ## [1] &quot;webdata&quot; ## attr(,&quot;package&quot;) ## [1] &quot;geoknife&quot; Now that we have a defined fabric, we can explore what variables and time period are within that data. First, we use query to determine what variables exist. You’ll notice that the function variable returns NA. This is fine when you are just exploring available data; however, exploring available times requires that the variable be defined. Thus, we need to set which variable from the dataset will be used. Then, we can explore times that are available in the data. # no variables defined yet variables(evap_fabric) ## [1] NA # find what variables are available query(evap_fabric, &quot;variables&quot;) ## [1] &quot;et&quot; # trying to find available times before setting the variable results in an error # `query(evap_fabric, &quot;times&quot;)` will fail # only one variable, &quot;et&quot; variables(evap_fabric) &lt;- &quot;et&quot; variables(evap_fabric) ## [1] &quot;et&quot; # now that the variable is set, we can explore available times query(evap_fabric, &quot;times&quot;) ## [1] &quot;2000-01-01 UTC&quot; &quot;2018-01-01 UTC&quot; 7.22.2 Datasets not in GDP Any gridded dataset available online that follows OPeNDAP protocol and some additional conventions can be used with geoknife. These datasets can be found through web searches or other catalogs and require finding out the OPeNDAP endpoint (URL) for the dataset. This url is used as the input to the argument url in webdata. Please see the Custom Dataset Use Guidelines documentation for more information about compatibility, or contact the GDP development team (gdp@usgs.gov). We searched NOAA’s OPenDAP data catalog and found this data from the Center for Operational Oceanographic Products and Services THREDDS server. It includes forecasts for water levels, water currents, water temperatures, and salinity levels for Delaware Bay. Since it is forecast data, the times associated with the data will change. To create a webdata object from this dataset, just use the OPeNDAP url. Then query variables and time as we did before. DelBay_fabric &lt;- webdata(url=&quot;http://opendap.co-ops.nos.noaa.gov/thredds/dodsC/DBOFS/fmrc/Aggregated_7_day_DBOFS_Fields_Forecast_best.ncd&quot;) query(DelBay_fabric, &quot;variables&quot;) ## character(0) # need to set the variable(s) in order to query the times variables(DelBay_fabric) &lt;- c(&quot;Vwind&quot;, &quot;temp&quot;) query(DelBay_fabric, &quot;times&quot;) ## [1] NA NA Here is a second example of using a non-GDP dataset. This data was found under the data section on Unidata’s website. This is aggregated satellite data from the UNIWISC (Unidata-Wisconsin) datastream for Earth’s “surface skin” temperature. skinT_fabric &lt;- webdata(url=&quot;http://thredds.ucar.edu/thredds/dodsC/satellite/SFC-T/SUPER-NATIONAL_1km&quot;) skinT_var &lt;- query(skinT_fabric, &quot;variables&quot;) # need to set the variable(s) in order to query the times variables(skinT_fabric) &lt;- skinT_var query(skinT_fabric, &quot;times&quot;) # your times might be different because this is forecast data ## [1] NA NA Both examples we’ve included here use aggregated data, meaning there is a single URL for all the data of this type on the server. Some data that you encounter might be non-aggregated, meaning there are multiple URLs to access the same data. In these cases, you will need to create more than one geojob and join data at the end. Now that we have explored options for our webdata, let’s look at what options exist for geospatial features. 7.23 Available webgeoms The next component to geoknife jobs is the spatial extent of the data, a.k.a. the stencil. The stencil is defined by using either of the functions simplegeom or webgeom. simplegeom is used to explicitly define an area by the user, but webgeom is used to specify an existing web feature service (WFS) as the geospatial extent. Defining your stencil using simplegeom will be covered in the next lesson. This lesson will just show you how to learn what available webgeoms exist. Users can use any WFS url to create their stencil, but there are a number of features that exist through GDP already. To determine what features exist, you can create a default webgeom object and then query its geom name, attributes, and values. This will return all available GDP default features. # setup a default stencil by using webgeom and not supplying any arguments default_stencil &lt;- webgeom() # now determine what geoms are available with the default default_geoms &lt;- query(default_stencil, &quot;geoms&quot;) length(default_geoms) ## [1] 34 head(default_geoms) ## [1] &quot;upload:Ag_Lands_ICC&quot; ## [2] &quot;upload:Ag_Lands_ICC_ForOnlineProcesses&quot; ## [3] &quot;upload:Ag_Lands_Reservation&quot; ## [4] &quot;upload:Airport_Polygon&quot; ## [5] &quot;sample:Alaska&quot; ## [6] &quot;upload:BB_LPK&quot; You will notice a pattern with the names of the geoms: a category followed by :, and then a specific name. These category-name combinations are the strings you would use to define your geom. Additionally, webgeom can accept a URL that points directly to a WFS. The categories you should be familiar with are sample and upload. sample geoms are any that are available through geoknife by default. upload geoms are custom shapefiles that someone uploaded through GDP. If you would like to upload a specific shapefile to GDP, follow these instructions. Be aware that uploaded shapefiles are wiped from the server at regular intervals (could be as often as weekly). To use your own shapefile: upload it, execute your job and then save the output; re-upload your shapefile the next time you need it on GDP. Similar to fabrics where you could not query times without setting the variables, you cannot query attributes of stencils before defining the geoms. Likewise, you cannot query for values of a stencil until you have set the attributes. Attributes give the metadata associated with the stencil and it’s geom. Values tell you the individual spatial features available in that attribute of the geom. # add a geom to see what values are available geom(default_stencil) ## [1] NA geom(default_stencil) &lt;- &quot;sample:CONUS_states&quot; # now that geom is set, you can query for available attributes query(default_stencil, &quot;attributes&quot;) ## [1] &quot;STATE&quot; &quot;FIPS&quot; attribute(default_stencil) &lt;- &quot;STATE&quot; # now that attribute is set, you can query for available values STATE_values &lt;- query(default_stencil, &quot;values&quot;) head(STATE_values) ## [1] &quot;Alabama&quot; &quot;Arizona&quot; &quot;Arkansas&quot; &quot;California&quot; &quot;Colorado&quot; ## [6] &quot;Connecticut&quot; # switch the stencil to see the differences ecoreg_stencil &lt;- default_stencil geom(ecoreg_stencil) &lt;- &quot;sample:Ecoregions_Level_III&quot; query(ecoreg_stencil, &quot;attributes&quot;) ## [1] &quot;LEVEL3_NAM&quot; attribute(ecoreg_stencil) &lt;- &quot;LEVEL3_NAM&quot; ecoreg_values &lt;- query(ecoreg_stencil, &quot;values&quot;) head(ecoreg_values) ## [1] &quot;Ahklun And Kilbuck Mountains&quot; &quot;Alaska Peninsula Mountains&quot; ## [3] &quot;Alaska Range&quot; &quot;Aleutian Islands&quot; ## [5] &quot;Arctic Coastal Plain&quot; &quot;Arctic Foothills&quot; # now set the values to the Driftless Area and Blue Ridge ecoregions values(ecoreg_stencil) &lt;- ecoreg_values[c(12, 33)] values(ecoreg_stencil) ## [1] &quot;Blue Ridge&quot; &quot;Driftless Area&quot; There are some built-in templates that allow stencils to be defined more specifically. Currently, the package only supports US States, Level III Ecoregions, or HUC8s. Below are shortcuts to setting the geom, attribute, and values. # creating geoms from the available templates webgeom(&#39;state::Wisconsin&#39;) ## An object of class &quot;webgeom&quot;: ## url: https://cida.usgs.gov/gdp/geoserver/wfs ## geom: sample:CONUS_states ## attribute: STATE ## values: Wisconsin ## wfs version: 1.1.0 webgeom(&#39;state::Wisconsin,Maine&#39;) # multiple states separated by comma ## An object of class &quot;webgeom&quot;: ## url: https://cida.usgs.gov/gdp/geoserver/wfs ## geom: sample:CONUS_states ## attribute: STATE ## values: Wisconsin, Maine ## wfs version: 1.1.0 webgeom(&#39;HUC8::09020306,14060009&#39;) # multiple HUCs separated by comma ## An object of class &quot;webgeom&quot;: ## url: https://cida.usgs.gov/gdp/geoserver/wfs ## geom: sample:simplified_huc8 ## attribute: HUC_8 ## values: 09020306, 14060009 ## wfs version: 1.1.0 webgeom(&#39;ecoregion::Colorado Plateaus,Driftless Area&#39;) # multiple ecoregions separated by comma ## An object of class &quot;webgeom&quot;: ## url: https://cida.usgs.gov/gdp/geoserver/wfs ## geom: sample:Ecoregions_Level_III ## attribute: LEVEL3_NAM ## values: Colorado Plateaus, Driftless Area ## wfs version: 1.1.0 7.24 Available webprocesses The final component to a geojob is the process algorithm used to aggregate the data across the defined stencil. Web process algorithms can be defined by the user, but let’s explore the defaults available through GDP. # setup a default knife by using webprocess and not supplying any arguments default_knife &lt;- webprocess() # now determine what web processing algorithms are available with the default default_algorithms &lt;- query(default_knife, &#39;algorithms&#39;) length(default_algorithms) ## [1] 6 head(default_algorithms) ## $`Timeseries Service Subset` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureTimeSeriesAlgorithm&quot; ## ## $`Categorical Coverage Fraction` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureCategoricalGridCoverageAlgorithm&quot; ## ## $`OPeNDAP Subset` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureCoverageOPeNDAPIntersectionAlgorithm&quot; ## ## $`Area Grid Statistics (unweighted)` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureGridStatisticsAlgorithm&quot; ## ## $`Area Grid Statistics (weighted)` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm&quot; ## ## $`WCS Subset` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureCoverageIntersectionAlgorithm&quot; From this list, you can define which algorithm you would like the webprocess component to use. Definitions of each of the default algorithms can be found in the Geo Data Portal Algorithm Summaries section of the home page for GDP documentation. For example, we want to use the OPeNDAP subsetting algorithm, “OPeNDAP Subset”. # algorithm actually has a default of the weighted average algorithm(default_knife) ## $`Area Grid Statistics (weighted)` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureWeightedGridStatisticsAlgorithm&quot; # change the algorithm to OPeNDAP&#39;s subset algorithm(default_knife) &lt;- default_algorithms[&#39;OPeNDAP Subset&#39;] algorithm(default_knife) ## $`OPeNDAP Subset` ## [1] &quot;gov.usgs.cida.gdp.wps.algorithm.FeatureCoverageOPeNDAPIntersectionAlgorithm&quot; Now that we can explore all of our options, we will learn how to construct each component and execute a geojob in the next lesson. 7.25 Setting up a geojob A geojob is the object that contains all of the necessary processing information to execute a data request to GDP. The geojob is made up of the stencil, fabric, and knife (if you need to learn what these components are, please visit the previous lesson). To create a geojob, use the function geoknife and give the three components as arguments. stencil and fabric must be indicated, but knife has a default. Any additional arguments are specifications for the webprocessing step. See ?'webprocess-class' for options. This lesson will not discuss all of the options. # load the geoknife package library(geoknife) Let’s setup a geojob to find the unweighted annual evapotranspiration rates for the state of Indiana using the annual evapotranspiration data we saw in the previous lesson. Since we have seen the URL and know the available variables and times, we can set all of that manually in the webdata function. Note: the times field must be a vector of start then end date, and be class POSIXct. # create fabric evap_fabric_info &lt;- list(times = as.POSIXct(c(&quot;2005-01-01&quot;, &quot;2015-01-01&quot;)), variables = &quot;et&quot;, url = &#39;https://cida.usgs.gov/thredds/dodsC/ssebopeta/yearly&#39;) evap_fabric &lt;- webdata(evap_fabric_info) # create stencil evap_stencil &lt;- webgeom(&#39;state::Indiana&#39;) # create knife (which defaults to weighted) evap_knife &lt;- webprocess() # find unweighted algorithm all_algorithms &lt;- query(evap_knife, &#39;algorithms&#39;) unw_algorithm &lt;- all_algorithms[grep(&#39;unweighted&#39;, names(all_algorithms))] # set knife algorithm to unweighted algorithm(evap_knife) &lt;- unw_algorithm # create the geojob evap_geojob &lt;- geoknife(evap_stencil, evap_fabric, evap_knife) 7.26 Checking the geojob status The geojob has been created and started on the server. Now, you can check to see if the processing is complete by using check. check(evap_geojob) ## $status ## [1] &quot;Process successful&quot; ## ## $URL ## [1] &quot;https://cida.usgs.gov:443/gdp/process/RetrieveResultServlet?id=fe471946-fad9-4055-9c1f-a1b186e4e36eOUTPUT&quot; ## ## $statusType ## [1] &quot;ProcessSucceeded&quot; ## ## $percentComplete ## [1] &quot;100&quot; Other helpful functions to get status information about the job are running (returns T/F to say if the job is still processing), error (returns T/F to say if there was an error during the processing), and successful (returns T/F indicating whether the job process was able to complete without any issues). Only one of these can return TRUE at a time. running(evap_geojob) ## [1] FALSE error(evap_geojob) ## [1] FALSE successful(evap_geojob) ## [1] TRUE The results of all the status checks say that our job was successful! These status checks are useful if you put a geojob in a script and want to fail gracefully when there is an error in the job. 7.27 Getting geojob data Since this job has finished processing and was successful, you can now get the data. You’ll notice that evap_geojob does not actually contain any data. It only contains information about the job that you submitted. To get the data, you need to use result or download. The feature summary algorithms will return simple tabular data, so you can use result to automatically take the output and parse it into an R data.frame. We used a feature summary algorithm in the evapotranspiration example, which returned tabular data. So, let’s use result to get the geojob output. evap_data &lt;- result(evap_geojob) nrow(evap_data) ## [1] 11 head(evap_data) ## DateTime Indiana variable statistic ## 1 2005-01-01 603.7376 et MEAN ## 2 2006-01-01 688.1866 et MEAN ## 3 2007-01-01 576.3254 et MEAN ## 4 2008-01-01 691.2009 et MEAN ## 5 2009-01-01 689.2706 et MEAN ## 6 2010-01-01 630.4045 et MEAN There are additional algorithms that return subsets of the raw data as netcdf or geotiff formats. These formats will require you to handle the output manually using download. Use this function to download the output to a file and then read it using your preferred data parsing method. download can also be used for tabular data if you have a parsing method that differs from what is used in result. See ?download for more information. 7.28 wait and email This was not a computationally or spatially intensive request, so the job finished almost immediately. However, if we had setup a more complex job, it could still be running. Even though the processing of these large gridded datasets uses resources on a remote server, your workflow needs to account for processing time before using the results. There are a few scenarios to consider: You are manually executing a job and manually checking it. You are running a script that kicks off a geoknife process followed by lines of code that use the returned data. You are running a long geoknife process and want to be able to close R/RStudio and come back to a completed job later. For the first scenario, the workflow from above was fine. If you are manually checking that the job has completed before trying to extract results, then nothing should fail. For the second scenario, your code will fail because it will continue to execute the code line by line after starting the job. So, your code will fail at the code that gets the data (result/download) since the job is still running. You can prevent scripts from continuing until the job is complete by using the function wait. This function makes a call to GDP at specified intervals to see if the job is complete, and allows the code to continue once the job is complete. This function has two arguments: the geojob object and sleep.time. sleep.time defines the interval at which to check the status of the job in seconds (the default for sleep.time is 5 seconds). Please try to adjust sleep.time to limit the number of calls to GDP, e.g. if you know the job will take about an hour, set sleep.time=120 (2 min) because every 5 seconds would be excessive. # typical wait workflow evap_geojob &lt;- geoknife(evap_stencil, evap_fabric, evap_knife) wait(evap_geojob, sleep.time = 20) evap_data &lt;- result(evap_geojob) If you know ahead of time that your process will be long, you can tell the job to wait when defining your knife (the default is to not wait). sleep.time can be specified as an argument to webprocess. The following is functionally the same as the use of wait() from above. # create knife with the args wait and sleep.time evap_knife &lt;- webprocess(wait=TRUE, sleep.time=20) # follow the same code from before to get the unweighted algorithm all_algorithms &lt;- query(evap_knife, &#39;algorithms&#39;) unw_algorithm &lt;- all_algorithms[grep(&#39;unweighted&#39;, names(all_algorithms))] algorithm(evap_knife) &lt;- unw_algorithm # create geojob using knife w/ wait evap_geojob &lt;- geoknife(evap_stencil, evap_fabric, evap_knife) # get result evap_data &lt;- result(evap_geojob) As in the third scenario, if you have a job that will take a long time and plan to close R in the interim, you can specify the argument email when creating the knife. Then when you use your new knife in the geoknife call, it will send an email with appropriate information upon job completion (you will see gdp_data@usgs.gov as the sender). # example of how to specify an email address to get a job completion alert knife_willemail &lt;- webprocess(email=&#39;fake.email@gmail.com&#39;) knife_willemail ## An object of class &quot;webprocess&quot;: ## url: https://cida.usgs.gov/gdp/process/WebProcessingService ## algorithm: Area Grid Statistics (weighted) ## web processing service version: 1.0.0 ## process inputs: ## SUMMARIZE_TIMESTEP: false ## SUMMARIZE_FEATURE_ATTRIBUTE: false ## REQUIRE_FULL_COVERAGE: true ## STATISTICS: ## GROUP_BY: ## DELIMITER: COMMA ## wait: FALSE ## email: fake.email@gmail.com The email alert will contain the completed job URL. Since this process requires you to leave R and get information from an email, it is often only recommended if you don’t plan to do further analysis in R. Otherwise, we recommend using the wait() function in a script. Use the URL as a string in this workflow to get your results: geojob &lt;- geojob(&quot;my url as a string&quot;) check(geojob) mydata &lt;- result(geojob) 7.29 USGS NWIS Culmination Write Up Write up a 1-page derived data product or research pipeline proposal summary of a project that you might want to explore using USGS NWIS data. Include the types of data that you would need to implement this project and how you would retrieve them. Save this summary as you will be refining and adding to your ideas over the course of the semester. Suggestions: Tables or bullet lists of specific data products An overarching high-quality figure to show how these data align One paragraph summarizing how this data or analysis is useful to you and/or the infrastructure. "]
]
