[
["index.html", "Environmental Informatics Using Research Infrastructures and their Data Preface Acknowledgements", " Environmental Informatics Using Research Infrastructures and their Data Dr. Katharyn Duffy, Dr. Ben Ruddell Preface This textbook provides an introduction to environmental and ecological informatics in the context of “big science”- that is, in the context of research infrastructures and observatories that collect and publish reams of observations and derived data products about the earth and its environment. The reader will learn how to make use of environmental infrastructures’ data products. This textbook introduces a framework of learning outcomes that covers the broad context of these data products that is necessary to make proper use of the information: The infrastructure’s organizational structure and scientific scope; Instrumentation, quality control, metadata, data catalogs, API’s, and data products; Key references, key tutorials, and informatics best practices; Professional career tracks in informatics; This textbook is intended primarily for graduate students enrolled in computer science and informatics programs but engaged in studies and research on environmental and geoscience topics- with a special emphasis on ecological topics. Advanced undergraduates and other graduate students enrolled in STEM programs may also be well-served if they have a strong background in programming and computing. Additionally, professional scientists may find this textbook useful as a reference and as a training manual when they encounter the need to make use of the research infrastructures and data that are directly covered by the book’s content- or find the need to train a junior scientist on the use of these infrastructures’ data. The scope of the infrastructures and data products is mostly U.S. focused in this edition, but some of these infrastructures have a global reach, and the material is almost as useful for students in other countries as for U.S. students. The textbook can be tackled one unit at a time as a lab manual within a university course, or- in its intended application- a standard semester-long three-credit-hour graduate course should be offered to cover the entire textbook from start to finish. Digital supplements are provided with examples of successful projects. Efforts have been made to select activities using data products, software, and tools that are relatively mature and stable. Even so, because this textbook covers a rapidly moving field, portions will, unfortunately, become dated quickly. It is the authors’ intent to release frequent editions that update and expand the material to keep pace with the rapid development of our field. Informatics is arguably the key scientific discipline of the 21st century, and research infrastructures are the source of the raw natural resource fueling the informatics revolution: observational data. Most 21st century scientists and scientific staff will spend their careers immersed in the data revolution. We sincerely hope that this textbook provides the launchpad you need for your career or for your next project in environmental science. Acknowledgements This first version of the textbook was developed to offer INF550, a graduate course in the School of Informatics Computing and Cyber Systems at Northern Arizona University in Fall 2020, with funding and leadership from the National Science Foundation funded National Research Traineeship “T3” option in Ecological and Environmental Informatics within a PhD program in Informatics and Computing (NRT-HDR #1829075, PI’s Ogle, Barber, Richardson, Ruddell, and Sankey). Our research infrastructure partners were critical to the creation of the material. Our partners at NEON - Battelle deserve special gratitude for anchoring the project. The opinions expressed are those of the researchers, and not necessarily the funding agencies. Special thanks to Megan Jones and Donal O’Leary at NEON-Battelle for their support in pulling NEON materials. Key Contributors Alphabetized by organization, then last name: Alycia Crall, NEON - Battelle Chris Florian, NEON - Battelle Megan Jones, NEON - Battelle Hank Loescher, NEON - Battelle Paula Mabee, NEON - Battelle Donal O’Leary, NEON - Battelle Kate Thibault, NEON - Battelle Andrew Richardson, PhenoCam - NAU Bijan Seyednasrollah, PhenoCam - NAU Theresa Crimmons, USA-NPN Kathy Gerst, USA-NPN Lee Marsh, USA-NPN "],
["pre-course-setup-ecoinformatics-tools.html", "Pre-Course Setup: EcoInformatics Tools 0.1 Pre-Course Skills &amp; Setup 0.2 Linux R/RStudio Setup 0.3 Installing and Setting up Git &amp; Github on Your Machine 0.4 Installing Atom 0.5 Linking RStudio to Git 0.6 How we will be Conducting this Course 0.7 Exercises:", " Pre-Course Setup: EcoInformatics Tools The purpose of this course is to train you in key ecoinformatics practices. Therefore, as an Ecoinformatician you need to be able to: Pull data from Application Programming Interfaces (APIs) More on this in Chapter 2 Organize and document your code and data Version control your code to avoid disaster and make it reproducible For you, your collaborators, and/or the wider community Push your code up to public-facing repositories Pull others code from public repositories. More thoughts on the benefits and power of reproducibility can be found here To be successful, both in this course and in your careers you will need these skills. This is why they are a requirement for this course. If you are already using these skills on a daily basis, fantastic! If you don’t feel that you have mastery in the workflows listed above we have placed lesson links throughout this chapter so that you can build these skills and be successful in this course. 0.1 Pre-Course Skills &amp; Setup For the purpose of this course we will largely be using the following tools to access, pull, and explore data: R &amp; Rstudio Git, GitHub, &amp; Atom.io Markdown &amp; Rmarkdown As such we will need to install and/or update these tools on your personal computer before our first day of class. While we chose R for this course, nearly all of the packages and data are fully available and transferable to Python or other languages. If you’d like to brush up on your R skills I highly recommend Data Carpentry Boot camp’s free R for Reproducible Scientific Analysis course. 0.1.1 Installing or Updating R Please check your version of R. You will need R 3.6.0+ How to check your version in R or RStudio if you already have it: &gt; version _ platform x86_64-apple-darwin15.6.0 arch x86_64 os darwin15.6.0 system x86_64, darwin15.6.0 status major 3 minor 5.1 year 2018 month 07 day 02 svn rev 74947 language R version.string R version 3.5.1 (2018-07-02) nickname Feather Spray If you don’t already have R or need to update it do so here. 0.1.2 Windows R/RStudio Setup After you have downloaded R, run the .exe file that was just downloaded Go to the RStudio Download page Under Installers select RStudio X.XX.XXX - e.g. Windows Vista/7/8/10 Double click the file to install it Once R and RStudio are installed, click to open RStudio. If you don’t get any error messages you are set. If there is an error message, you will need to re-install the program. 0.1.3 Mac R/RStudio Setup After you have downloaded R, double click on the file that was downloaded and R will install Go to the RStudio Download page Under Installers select RStudio 1.2.1135 - Mac OS X XX.X (64-bit) to download it. Once it’s downloaded, double click the file to install it. Once R and RStudio are installed, click to open RStudio. If you don’t get any error messages you are set. If there is an error message, you will need to re-install the program. 0.2 Linux R/RStudio Setup R is available through most Linux package managers. You can download the binary files for your distribution from CRAN. Or you can use your package manager. e.g. for Debian/Ubuntu run sudo apt-get install r-base and for Fedora run sudo yum install R To install RStudio, go to the RStudio Download page Under Installers select the version for your distribution. Once it’s downloaded, double click the file to install it Once R and RStudio are installed, click to open RStudio. If you don’t get any error messages you are set. If there is an error message, you will need to re-install the program. 0.2.1 Install basic packages for this course You can run the following script to make sure all the required packages are properly installed on your computer. # list of required packages list.of.packages &lt;- c( &#39;data.table&#39;, &#39;tidyverse&#39;, &#39;jsonlite&#39;, &#39;jpeg&#39;, &#39;png&#39;, &#39;raster&#39;, &#39;rgdal&#39;, &#39;rmarkdown&#39;, &#39;knitr&#39; ) # identify new (not installed) packages new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] # install new (not installed) packages if(length(new.packages)) install.packages(new.packages, repos=&#39;http://cran.rstudio.com/&#39;) # load all of the required libraries sapply(list.of.packages, library, character.only = T) Note: On some operating systems, you may need to install the Geospatial Data Abstraction Library (GDAL). More information about GDAL can be found from here. 0.3 Installing and Setting up Git &amp; Github on Your Machine For this course you will need: 1. Git installed on your local machine 2. Very basic bash scripting 3. A linked GitHub account 4. To link RStudio to git via RStudio or Atom.io As we will be using these skills constantly, they are a pre-requisite for this course. If you don’t yet have these skills it’s okay! You can learn everything that you need to know via the following freely available resources: The Unix Shell Version Control with Git Happy Git with R If you are learning these skills from scratch I estimate that you will need to devote ~4-6 hours to get set up and comfortable with the various workflows. Also remember that I have code office hours every week and that Stack Exchange is your friend. 0.4 Installing Atom Atom.io is a powerful and useful text editor for the following reasons: It is language agnostic It fully integrates with git and github + You can use it to push/pull/resolve conflicts and write code all in one space. 0.5 Linking RStudio to Git Happy Git with R has a fantastic tutorial to help you link Rstudio-Git-Github on your local machine and push/pull from or to public repositories. 0.6 How we will be Conducting this Course If you find a broken link or error in this course text submit an issue on the course github repository. At the end of each chapter you will find a set of Exercises. At the end of the assigned chapter you will be expected to submit via BBLearn two files: An RMarkdown file with the naming convention: LASTNAME_COURSECODE_Section#.Rmd, and A knitted .PDF with the same naming convention: LASTNAME_COURSECODE_Section#.pdf To generate these files you have two options: Click on the pencil and pad logo in the top of this text, copy the exercise section code, and drop it into your own .Rmd. Git clone our course Github Repository, navigate to the ’_Exercises’ folder, and use that .Rmd as a template. Note: Exercises submitted in any other format, or those missing questions will not be graded To generate your .PDF to upload, in your RMarkdown file simply push the ‘Knit’ button at the top of your document. 0.7 Exercises: Navigate to our course github git fork our repo onto your own personal github account. git clone the repo onto your own personal machine in a place that is functional and not temporary (e.g. not your downloads folder). #hints cd `Your/Path/Here&#39; git clone &#39;repo HTTPS&#39; Add 2-3 sentences introducing yourself in the _Course-participants folder. For example: *** Hi, I&#39;m Dr. Katharyn Duffy. I have a Ph.D in Earth Science from Northern Arizona University. Over the past two years I&#39;ve worked as an open-source software engineer in the PhenoCam lab, and now I&#39;m the coding and lab support for your course. I really look forward to working with all of you! *** Submit a pull request to add your introduction to our course participants folder. #hints git add ... git commit ... git status.... git push --set-upstream git remote -v git remote add upstream... Note: You may complete these either on the command line or via a program like Atom.io. If you haven’t yet made commits to a remote repository or submitted pull requests please reference the resources listed above. "],
["why-ecoinformatics.html", "Chapter 1 Why ‘EcoInformatics’? 1.1 The Framework of this Course 1.2 Final Course Project: Proposed Derived Data Product", " Chapter 1 Why ‘EcoInformatics’? Portions of the following introduction were adapted from Michener &amp; Jones 2012, Trends in Ecology &amp; Evolution ’Ecoinformatics: supporting ecology as a data-intensive science Ecology is increasingly becoming a data-intensive science, relying on massive amounts of data collected by both remote-sensing platforms and sensor networks embedded in the environment. New observatory networks, such as the US National Ecological Observatory Network (NEON), provide research platforms that enable scientists to examine phenomena across diverse ecosystem types through access to thousands of sensors collecting diverse environmental observations. These networks spatially and temporaly overlap with a number of other networks and infrastructures ranging from remote sensing, to citizen science, and so on. It has been argued that data-intensive science represents the fourth scientific paradigm following the empirical (i.e. description of natural phenomena), theoretical (e.g. modeling and generalization) and computational (e.g. simulation) scientific approaches, and comprises an approach for unifying theory, experimentation and simulation. Ecologists increasingly address questions at broader scales that have both scientific and societal relevance. For example, the 40 top priorities for science that can inform conservation and management policy in the USA rely principally on a sound foundation of ecological research, and the ability to scale knowledge and inter-connect data. Continental-scale patterns and dynamics result from climate and people as broad-scale drivers interacting with finer-scale vectors that redistribute materials within and among linked terrestrial and aquatic systems. Climate and land-use change interact with patterns and processes at multiple, finer scales (blue arrows). (a) These drivers can influence broad-scale patterns directly, and these constraints may act to overwhelm heterogeneity and processes at (b) mesoscales and at (c) the finer scale of local sites. Broad-scale drivers can also exert an indirect impact on broad-scale patterns through their interactions with disturbances, including (d) the spread of invasive species, (e) pattern–process relationships at meso-scales, or (f) at finer scales within a site. Connectivity imparted by the transfer of materials occurs both at (g) the meso-scale and at (h) finer scales within sites where terrestrial and aquatic systems are connected. These dynamics at fine scales can propagate to influence larger spatial extents (red arrows). Feedbacks occur throughout the system. The term “drivers” refers to both forcing functions that are part of the system and to external drivers. Peters et al., 2008 Ecology is also affected by changes that are occurring throughout science as a whole. In particular, scientists, professional societies and research sponsors are recognizing the value of data as a product of the scientific enterprise and placing increased emphasis on data stewardship, data sharing, openness and supporting study repeatability. Data on ecological and environmental systems are (A) acquired, checked for quality, documented using an acquisition workflow, and then both the raw and derived data products are versioned and deposited in the DataONE federated data archive (red dashed arrows). Researchers discover and access data from the federation and then (B) integrate and process the data in an analysis workflow, resulting in derived data products, visualizations, and scholarly papers that are in turn archived in the data federation (red dashed arrows). Other researchers directly cite any of the versioned data, workflows, and visualizations that are archived in the DataONE federation. Richman et al., 2011 The changes that are occurring in ecology create challenges with respect to acquiring, managing and analyzing the large volumes of data that are collected by scientists worldwide. One challenge that is particularly daunting lies in dealing with the scope of ecology and the enormous variability in scales that is encountered, spanning microbial community dynamics, communities of organisms inhabiting a single plant or square meter, and ecological processes occurring at the scale of the continent and biosphere. The diversity in scales studied and the ways in which studies are carried out results in large numbers of small, idiosyncratic data sets that accumulate from the thousands of scientists that collect relevant biological, ecological and environmental data. A proposed high-level architecture for ecological and environmental data management is shown consisting of three primary levels. Data stored within distributed data repositories (a) is mediated by standard metadata and ontologies (b) to power software tools used by scientists and data managers (c). Software applications use community-endorsed ontologies and metadata standards from the middle level to provide tools that are more effective for publishing, querying, integrating and analyzing data. Ontologies are separated into framework ontologies and domain-specific extensions, enabling contributions from multiple research groups, disciplines and individuals. Cross-disciplinary data are maintained in local repositories, but made accessible to the broader research community through distributed systems based on shared, open protocols (such as Metacat). Example repositories include the LTER network, National Ecological Observatory Network, United States Geographical Survey and SEEK’s EarthGrid. Madin et al. 2008, Ecoinformatics is a framework that enables scientists to generate new knowledge through innovative tools and approaches for: discovering, managing, integrating, analyzing, visualizing, and preserving relevant biological, environmental, and socioeconomic data and information. Many ecoinformatics solutions have been developed over the past decade, increasing scientists’ efficiency and supporting faster and easier data discovery, integration and analysis; however, many challenges remain, especially in relation to installing ecoinformatics practices into mainstream research and education. And that, course participants, is why we are here. 1.1 The Framework of this Course Over the duration of this course we will survey a wide array of observation platforms and networks and build hands-on experience with the framework of Ecoinformatics. For coherance we will cover the following overarching themes: Each network’s mission and design Each network’s spatial design e.g. opprtunistic vs. planned, citizen science vs. orbital sensors The types of data that stream from each network e.g. sensors, derived products, metadata How to access that data e.g. APIs, landing pages, r packages etc. Opportunities to interact with or contribute to each network e.g. RFP’s coming down the pipeline, internships, and post-doctoral scholar programs. At the conclusion of each network’s section you will be asked to write a 1-page summary reviewing the above framework for each network, and highlight how it potentially aligns with your own research. These series of 1-page summaries will then culminate into a final presentation where you propose to derive your own data product for your own research touching upon multiple networks and accounting for differences in spatial footprints, frequency of observations, and important data cross-walks. 1.2 Final Course Project: Proposed Derived Data Product For your final project, you will present a 4-6 minute IGNITE-style derived data product pitch, followed by 2-3 minutes of questions from your audience (which will include members from the infrastrures we’ve covered). Think of this project as your ‘sales pitch’ to the research infrastructure whose data you are using, and/or the scientific community as a whole. In the IGNITE theme of ‘Enlighten us but make it quick’, you will construct a series of slides that auto-advance every 30 seconds. Specific instructions for the content of each slide are below. Ideally, this final presentation will feed upon a number of the ‘culmination write-ups’ you have conducted over the course of the semester. Ideally, this derived data product will utilize data from a number of sources, either covered within this course or external to it. Ideally, it will also convince your audience that your idea is novel, useful, and possible. In order to complete this presentation, you will need to have worked with the various data products you propose, have an in-depth understanding of them, and their challenges, along with original, clean, high-level summary graphics. Further, giving an IGNITE-style presentation takes practice. IGNITE-style presentations are powerful, as they keep you moving forward, and give your audience a high-level understanding of your topic. We fully recommend rehearsing your presentation many times before giving it live and recording yourself to learn how you can improve. Here’s an example (of an even faster) ignite talk from one of your book’s authors: In your derived data product pitch you will cover these themes: The need for the derived data that you are proposing to produce. What data you will use to derive this product, including the justification for this exact data. The processing pipeline for this product, along with estimates for a timeline. Potential hurdles you will have to overcome. How this product will serve the infrastructure and/or the scientific community. Specific slide criteria are as follows: Slide 1: Title, authors (including contacts at infrastructures covered if applicable) Slide 2: Justification for the derived data product; the gap or need that it fills Slides 3-x: 1 slide per data product used including: The exact data product (e.g. NEON data product id and full title) A 1 sentence summary of the data product and its justification for this purpose An original, clean, polished high-level plot, gif or .mp4 of the data Slide x + 1: A high-level workflow diagram of the processing pipeline E.g.: Original data and how you pull it in (API, r-package etc) Filtering process using QA/QC or metadata Orthorectification in time or space Example generated using draw.io: Slide x + 2: A clean plot of all of the data you mentioned together, and/or the derived data product itself with a 1 sentence summary Example: Slide x+3: Summary: Circle back on how this derived data product serves your research, the infrastructure, and the wider science community (no more than 10 words, suggestion: graphics or bullet points) Slide x + 4: Data citations for all data used in proposed derived data product An example slide deck with specific ideas can be found here The rubric for your final presentation grade is as follows: Presentation meets all requirements and criteria: 60% Aesthetics and craft of presentation: 10% Live presentation of materials: 30% "],
["introduction-to-neon-its-data.html", "Chapter 2 Introduction to NEON &amp; its Data 2.1 Learning Objectives 2.2 The NEON Project Mission &amp; Design 2.3 The Science and Design of NEON 2.4 NEON’s Spatial Design 2.5 How NEON Collects Data 2.6 Accessing NEON Data 2.7 Hands on: Accessing NEON Data &amp; User Tokens 2.7.1 Additional Resources 2.8 Hands on: NEON TOS Data 2.9 Intro to NEON Exercises Part 1 2.10 Part 2: Pulling NEON Data via the API 2.11 What is an API? 2.12 Stacking NEON data 2.13 Intro to NEON Exercises: Written Questions 2.14 Intro to NEON Exercises Part 2 2.15 NEON Coding Lab Part 2 2.16 Intro to NEON Culmination Activity", " Chapter 2 Introduction to NEON &amp; its Data Estimated Time: 2 hours Course participants: As you review this information, please consider the final course project that you will work on the over this semester. At the end of this section you will document an initial research question, or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 2.1 Learning Objectives At the end of this activity, you will be able to: Explain the mission of the National Ecological Observatory Network (NEON). Explain the how sites are located within the NEON project design. Determine how the different types of data that are collected and provided by NEON, and how they align with your own research. Pull NEON data from the API and neonUtilities package [@R-neonUtilites] 2.2 The NEON Project Mission &amp; Design To capture ecological heterogeneity across the United States, NEON’s design divides the continent into 20 statistically different eco-climatic domains. Each NEON field site is located within an eco-climatic domain. 2.3 The Science and Design of NEON To gain a better understanding of the broad scope of NEON watch this 4:08 minute long video. 2.4 NEON’s Spatial Design Watch this 4:22 minute video exploring the spatial design of NEON field sites. Please read the following page about NEON’s Spatial Design: Read this primer on NEON’s Sampling Design Read about the different types of field sites - core and relocatable 2.4.1 NEON Samples All 20 Eco-Regions Explore the NEON Field Site map taking note of the locations of: Aquatic &amp; terrestrial field sites. Core &amp; relocatable field sites. Click here to view the NEON Field Site Map Explore the NEON field site map. Do the following: Zoom in on a study area of interest to see if there are any NEON field sites that are nearby. Click the “More” button in the upper right hand corner of the map to filter sites by name, site host, domain or state. Select one field site of interest. Click on the marker in the map. Then click on the name of the field site to jump to the field site landing page. Data Tip: You can download maps, kmz, or shapefiles of the field sites here. 2.5 How NEON Collects Data Watch this 3:06 minute video exploring the data that NEON collects. Read the Data Collection Methods page to learn more about the different types of data that NEON collects and provides. Then, follow the links below to learn more about each collection method: Aquatic Observation System (AOS) Aquatic Instrument System (AIS) Terrestrial Instrument System (TIS) – Flux Tower Terrestrial Instrument System (TIS) – Soil Sensors and Measurements Terrestrial Organismal System (TOS) Airborne Observation Platform (AOP) All data collection protocols and processing documents are publicly available. Read more about the standardized protocols and how to access these documents. 2.5.1 Specimens &amp; Samples NEON also collects samples and specimens from which the other data products are based. These samples are also available for research and education purposes. Learn more: NEON Biorepository. 2.5.2 Airborne Remote Sensing Watch this 4:02 minute video to better understand the NEON Airborne Observation Platform (AOP). Data Tip: NEON also provides support to your own research including proposals to fly the AOP over other study sites, a mobile tower/instrumentation setup and others. Learn more here the Assignable Assets programs . 2.6 Accessing NEON Data NEON data are processed and go through quality assurance quality control checks at NEON headquarters in Boulder, CO. NEON carefully documents every aspect of sampling design, data collection, processing and delivery. This documentation is freely available through the NEON data portal. Visit the NEON Data Portal - data.neonscience.org Read more about the quality assurance and quality control processes for NEON data and how the data are processed from raw data to higher level data products. Explore NEON Data Products. On the page for each data product in the catalog you can find the basic information about the product, find the data collection and processing protocols, and link directly to downloading the data. Additionally, some types of NEON data are also available through the data portals of other organizations. For example, NEON Terrestrial Insect DNA Barcoding Data is available through the Barcode of Life Datasystem (BOLD). Or NEON phenocam images are available from the Phenocam network site. More details on where else the data are available from can be found in the Availability and Download section on the Product Details page for each data product (visit Explore Data Products to access individual Product Details pages). 2.6.1 Pathways to access NEON Data There are several ways to access data from NEON: Via the NEON data portal. Explore and download data. Note that much of the tabular data is available in zipped .csv files for each month and site of interest. To combine these files, use the neonUtilities package (R tutorial, Python tutorial). Use R or Python to programmatically access the data. NEON and community members have created code packages to directly access the data through an API. Learn more about the available resources by reading the Code Resources page or visiting the NEONScience GitHub repo. Using the NEON API. Access NEON data directly using a custom API call. Access NEON data through partner’s portals. Where NEON data directly overlap with other community resources, NEON data can be accessed through the portals. Examples include Phenocam, BOLD, Ameriflux, and others. You can learn more in the documentation for individual data products. 2.7 Hands on: Accessing NEON Data &amp; User Tokens 2.7.1 Via the NEON API, with your User Token NEON data can be downloaded from either the NEON Data Portal or the NEON API. When downloading from the Data Portal, you can create a user account. Read about the benefits of an account on the User Account page. You can also use your account to create a token for using the API. Your token is unique to your account, so don’t share it. While using a token is optional in general, it is required for this course. Using a token when downloading data via the API, including when using the neonUtilities package, links your downloads to your user account, as well as enabling faster download speeds. For more information about token usage and benefits, see the NEON API documentation page. For now, in addition to faster downloads, using a token helps NEON to track data downloads. Using anonymized user information, they can then calculate data access statistics, such as which data products are downloaded most frequently, which data products are downloaded in groups by the same users, and how many users in total are downloading data. This information helps NEON to evaluate the growth and reach of the observatory, and to advocate for training activities, workshops, and software development. Tokens can (and should) be used whenever you use the NEON API. In this tutorial, we’ll focus on using tokens with the neonUtilities R package. 2.7.1 Objectives After completing this section, you will be able to: Create a NEON API token Use your token when downloading data with neonUtilities 2.7.1 Things You’ll Need To Complete This Tutorial You will need a version of R (3.4.1 or higher) and RStudio loaded on your computer. 2.7.1 Install R Packages neonUtilities: install.packages(&quot;neonUtilities&quot;) 2.7.1 Additional Resources NEON Data Portal NEONScience GitHub Organization neonUtilities tutorial If you’ve never downloaded NEON data using the neonUtilities package before, we recommend starting with the Download and Explore tutorial before proceeding with this tutorial. In the next sections, we’ll get an API token from the NEON Data Portal, and then use it in neonUtilities when downloading data. 2.7.2 Get a NEON API Token The first step is create a NEON user account, if you don’t have one. Follow the instructions on the Data Portal User Accounts page. If you do already have an account, go to the NEON Data Portal, sign in, and go to your My Account profile page. Once you have an account, you can create an API token for yourself. At the bottom of the My Account page, you should see this bar: Click the ‘GET API TOKEN’ button. After a moment, you should see this: Click on the Copy button to copy your API token to the clipboard. 2.7.3 Use the API token in neonUtilities In the next section, we’ll walk through saving your token somewhere secure but accessible to your code. But first let’s try out using the token the easy way. First, we need to load the neonUtilities package and set the working directory: # install neonUtilities - can skip if already installed, but # API tokens are only enabled in neonUtilities v1.3.4 and higher # if your version number is lower, re-install install.packages(&quot;neonUtilities&quot;) # load neonUtilities library(neonUtilities) # set working directory wd &lt;- &quot;~/data&quot; # this will depend on your local machine setwd(wd) NEON API tokens are very long, so it would be annoying to keep pasting the entire text string into functions. Assign your token an object name: NEON_TOKEN &lt;- &quot;PASTE YOUR TOKEN HERE&quot; Now we’ll use the loadByProduct() function to download data. Your API token is entered as the optional token input parameter. For this example, we’ll download Plant foliar traits (DP1.10026.001). foliar &lt;- loadByProduct(dpID=&quot;DP1.10026.001&quot;, site=&quot;all&quot;, package=&quot;expanded&quot;, check.size=F, token=NEON_TOKEN) You should now have data saved in the foliar object; the API silently used your token. If you’ve downloaded data without a token before, you may notice this is faster! This format applies to all neonUtilities functions that involve downloading data or otherwise accessing the API; you can use the token input with all of them. For example, when downloading remote sensing data: chm &lt;- byTileAOP(dpID=&quot;DP3.30015.001&quot;, site=&quot;WREF&quot;, year=2017, check.size=F, easting=c(571000,578000), northing=c(5079000,5080000), savepath=wd, token=NEON_TOKEN) 2.7.4 Token management for open code Your API token is unique to your account, so don’t share it! If you’re writing code that will be shared with colleagues or available publicly, such as in a GitHub repository or supplemental materials of a published paper, you can’t include the line of code above where we assigned your token to NEON_TOKEN, since your token is fully visible in the code there. Instead, you’ll need to save your token locally on your computer, and pull it into your code without displaying it. There are a few ways to do this, we’ll show two options here. Option 1: Save the token in a local file, and source() that file at the start of every script. This is fairly simple but requires a line of code in every script. Option 2: Add the token to a .Renviron file to create an environment variable that gets loaded when you open R. This is a little harder to set up initially, but once it’s done, it’s done globally, and it will work in every script you run. 2.7.4.1 Option 1: Save token in a local file Open a new, empty R script (.R). Put a single line of code in the script: NEON_TOKEN &lt;- &quot;PASTE YOUR TOKEN HERE&quot; Save this file within your current R project and call the file neon_token_source.R. So that you don’t accidently push your token up to GitHub, move over to the command line or Atom.io and add it to your .gitignore file: knitr::include_graphics(&#39;./docs/images/git_ignore.png&#39;) Now, whenever you want to pull NEON data via the API, at the start of any analysis you would place this line of code: source(&#39;neon_token_source.R&#39;) Then you’ll be able to use token=NEON_TOKEN when you run neonUtilities functions, and you can share your code without accidentally sharing your token. 2.7.4.2 Option 2: Save your toekn to your R environment Instructions for finding and editing your .Renviron can be found in this tutorial in NEON’s Data Tutorials section. 2.8 Hands on: NEON TOS Data 2.8.1 Pull in Tree Data from NEON’s TOS and investigate relationships Adapted from Claire Lunch’s ‘Compare tree height measured from the ground to a Lidar-based Canopy Height Model’ tutorial Later in this course we will be working with NEON’s LiDAR-based Canopy Height Model (CHM) data from their extensive Airborne Observation Platform (AOP). In this section we will pull in DP1.10098.001, Woody plant vegetation structure from NEON’s Terrestrial Observation Sampling (TOS) data and explore the data, from requesting it to plotting it. Generalized TOS sampling schematic, showing the placement of Distributed, Tower, and Gradient Plots from the NEON GUIDE TO WOODY PLANT VEGETATION STRUCTURE, 2018 The vegetation structure data are collected by by field staff on the ground. This data product contains the quality-controlled, native sampling resolution data from in-situ measurements of live and standing dead woody individuals and shrub groups, from all terrestrial NEON sites with qualifying woody vegetation. The exact measurements collected per individual depend on growth form, and these measurements are focused on enabling biomass and productivity estimation, estimation of shrub volume and biomass, and calibration / validation of multiple NEON airborne remote-sensing data products. In general, comparatively large individuals that are visible to remote-sensing instruments are mapped, tagged and measured, and other smaller individuals are tagged and measured but not mapped. Smaller individuals may be subsampled according to a nested subplot approach in order to standardize the per plot sampling effort. Structure and mapping data are reported per individual per plot; sampling metadata, such as per growth form sampling area, are reported per plot. Illustration of a 20 m x 20 m Distributed/Gradient/Tower base plot (left), a 40 m x 40 m Tower base plot (right), and associated nested subplots used for measuring woody stem vegetation. Locations of subplots are denoted with plain text numbers, and locations of nested subplots are denoted with italic numbers from the NEON GUIDE TO WOODY PLANT VEGETATION STRUCTURE, 2018 For the purpose of this hands-on activity we will be using data from the Wind River Experimental Forest NEON field site located in Washington state. The predominant vegetation at that site is tall evergreen conifers. Note: this is also a core site for many other networks such as AmeriFlux and FLUXNET, which we will cover later. Image of the Wind River Crane Flux Tower from Ameriflux Let’s begin by: Installing the geoNEON package Making sure that the packages that we need are loaded, and Supressing ‘strings as factors’ in R, as factors make all sorts of functions in R ‘cranky’. options(stringsAsFactors=F) #install.packages(&quot;devtools&quot;) #uncomment if you don&#39;t yet have devtools #devtools::install_github(&quot;NEONScience/NEON-geolocation/geoNEON&quot;) library(neonUtilities) ## Warning: package &#39;neonUtilities&#39; was built under R version 3.6.2 library(geoNEON) library(sp) ## Warning: package &#39;sp&#39; was built under R version 3.6.2 Now lets begin by pulling in the vegetation structure data using the loadByProduct() function in the neonUtilities package. Inputs needed to the function are: dpID: data product ID; (woody vegetation structure = DP1.10098.001 site: 4-letter site code; Wind River = WREF package: basic or expanded; we’ll begin with a basic here veglist &lt;- loadByProduct(dpID=&quot;DP1.10098.001&quot;, site=&quot;WREF&quot;, package=&quot;basic&quot;, check.size=FALSE, token = NEON_TOKEN) ## Finding available files ## | | | 0% | |======================= | 33% | |=============================================== | 67% | |======================================================================| 100% ## ## Downloading files totaling approximately 518.3 KiB ## Downloading 3 files ## | | | 0% | |=================================== | 50% | |======================================================================| 100% ## ## Unpacking zip files using 1 cores. ## Stacking operation across a single core. ## Stacking table vst_apparentindividual ## Stacking table vst_mappingandtagging ## Stacking table vst_perplotperyear ## Copied the most recent publication of validation file to /stackedFiles ## Copied the most recent publication of categoricalCodes file to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 3 data tables and 3 metadata tables! ## Stacking took 0.40101 secs ## All unzipped monthly data folders have been removed. Now, use the getLocTOS() function in the geoNEON package to get precise locations for the tagged plants. You can refer to the package documentation for more details. vegmap &lt;- getLocTOS(veglist$vst_mappingandtagging, &quot;vst_mappingandtagging&quot;) ## | | | 0% | | | 1% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |=== | 4% | |=== | 5% | |==== | 5% | |==== | 6% | |===== | 7% | |====== | 8% | |====== | 9% | |======= | 10% | |======== | 11% | |======== | 12% | |========= | 13% | |========== | 14% | |========== | 15% | |=========== | 16% | |============ | 17% | |============ | 18% | |============= | 18% | |============= | 19% | |============== | 20% | |============== | 21% | |=============== | 21% | |=============== | 22% | |================ | 22% | |================ | 23% | |================ | 24% | |================= | 24% | |================= | 25% | |================== | 25% | |================== | 26% | |=================== | 26% | |=================== | 27% | |=================== | 28% | |==================== | 28% | |==================== | 29% | |===================== | 29% | |===================== | 30% | |====================== | 31% | |====================== | 32% | |======================= | 32% | |======================= | 33% | |======================== | 34% | |========================= | 35% | |========================= | 36% | |========================== | 37% | |=========================== | 38% | |=========================== | 39% | |============================ | 40% | |============================= | 41% | |============================= | 42% | |============================== | 43% | |=============================== | 44% | |=============================== | 45% | |================================ | 45% | |================================ | 46% | |================================= | 47% | |================================= | 48% | |================================== | 48% | |================================== | 49% | |=================================== | 49% | |=================================== | 50% | |=================================== | 51% | |==================================== | 51% | |==================================== | 52% | |===================================== | 52% | |===================================== | 53% | |====================================== | 54% | |====================================== | 55% | |======================================= | 55% | |======================================= | 56% | |======================================== | 57% | |========================================= | 58% | |========================================= | 59% | |========================================== | 60% | |=========================================== | 61% | |=========================================== | 62% | |============================================ | 63% | |============================================= | 64% | |============================================= | 65% | |============================================== | 66% | |=============================================== | 67% | |=============================================== | 68% | |================================================ | 68% | |================================================ | 69% | |================================================= | 70% | |================================================= | 71% | |================================================== | 71% | |================================================== | 72% | |=================================================== | 72% | |=================================================== | 73% | |=================================================== | 74% | |==================================================== | 74% | |==================================================== | 75% | |===================================================== | 75% | |===================================================== | 76% | |====================================================== | 76% | |====================================================== | 77% | |====================================================== | 78% | |======================================================= | 78% | |======================================================= | 79% | |======================================================== | 79% | |======================================================== | 80% | |========================================================= | 81% | |========================================================= | 82% | |========================================================== | 82% | |========================================================== | 83% | |=========================================================== | 84% | |============================================================ | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 88% | |============================================================== | 89% | |=============================================================== | 90% | |================================================================ | 91% | |================================================================ | 92% | |================================================================= | 93% | |================================================================== | 94% | |================================================================== | 95% | |=================================================================== | 95% | |=================================================================== | 96% | |==================================================================== | 97% | |==================================================================== | 98% | |===================================================================== | 98% | |===================================================================== | 99% | |======================================================================| 99% | |======================================================================| 100% Now we need to merge the mapped locations of individuals (the vst_mappingandtagging table) with the annual measurements of height, diameter, etc (the vst_apparentindividual table). The two tables join based on individualID, the identifier for each tagged plant, but we’ll include namedLocation, domainID, siteID, and plotID in the list of variables to merge on, to avoid ending up with duplicates of each of those columns. Refer to the variables table and to the Data Product User Guide for Woody plant vegetation structure for more information about the contents of each data table. veg &lt;- merge(veglist$vst_apparentindividual, vegmap, by=c(&quot;individualID&quot;,&quot;namedLocation&quot;, &quot;domainID&quot;,&quot;siteID&quot;,&quot;plotID&quot;)) What did you just pull in? Are you sure you know what you’re working with? A best practice is to always do a quick visualization to make sure that you have the right data and that you understand its spread: symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$stemDiameter[which(veg$plotID==&quot;WREF_075&quot;)]/100/2, inches=F, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;) A key component of any measurement, and therefore a reoccuring theme in this course, is an estimate of uncertainty. Let’s overlay estimates of uncertainty for the location of each stem in blue: symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$stemDiameter[which(veg$plotID==&quot;WREF_075&quot;)]/100/2, inches=F, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;) symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], circles=veg$adjCoordinateUncertainty[which(veg$plotID==&quot;WREF_075&quot;)], inches=F, add=T, fg=&quot;lightblue&quot;) 2.9 Intro to NEON Exercises Part 1 2.9.1 Computational 2.9.1.1 Part 1: Sign up for and Use an NEON API Token: Submit via .Rmd and .pdf a simple script that uses a HIDDEN token to access NEON data. Example: source(&#39;neon_token_source.R&#39;) veglist &lt;- loadByProduct(dpID=&quot;DP1.10098.001&quot;, site=&quot;WREF&quot;, package=&quot;basic&quot;, check.size=FALSE, token = NEON_TOKEN) ## Finding available files ## | | | 0% | |======================= | 33% | |=============================================== | 67% | |======================================================================| 100% ## ## Downloading files totaling approximately 518.3 KiB ## Downloading 3 files ## | | | 0% | |=================================== | 50% | |======================================================================| 100% ## ## Unpacking zip files using 1 cores. ## Stacking operation across a single core. ## Stacking table vst_apparentindividual ## Stacking table vst_mappingandtagging ## Stacking table vst_perplotperyear ## Copied the most recent publication of validation file to /stackedFiles ## Copied the most recent publication of categoricalCodes file to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 3 data tables and 3 metadata tables! ## Stacking took 0.359781 secs ## All unzipped monthly data folders have been removed. summary(veglist) ## Length Class Mode ## categoricalCodes_10098 5 data.table list ## readme_10098 1 spec_tbl_df list ## validation_10098 8 data.table list ## variables_10098 9 data.table list ## vst_apparentindividual 40 data.frame list ## vst_mappingandtagging 29 data.frame list ## vst_perplotperyear 38 data.frame list 2.9.1.2 Part 2: Further Investigation of NEON TOS Vegetation Structure Data Suggested Timing: Complete this exercise before our next class session In the following section all demonstration code uses the iris dataset for R as examples. In this exercise the iris data is merely used for example code to get your started, you will complete all plots and models using the NEON TOS vegetation structure data Convert the above diameter plot into a ggplot: If you need some refreshers on ggplot Derek Sonderegger’s Introductory Data Science using R: Graphing Part II is a wonderful resource. I’ve pulled some of his plotting examples here. library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.2 print (&#39;your code here&#39;) ## [1] &quot;your code here&quot; Set the color your circles to be a function of each species: #hints: data(&quot;iris&quot;) ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point() Generate a histogram of tree heights for each plot. Color your stacked bar as a function of each species: #hints for faceting: ggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point() + facet_grid( . ~ Species ) Use dplyr to remove dead trees: library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union #hints: #veg=veg%&gt;% #filter(..... !=....) Create a simple linear model that uses Diameter at Breast Height (DBH) and height to predict allometries. Print the summary information of your model: #hints mdl=lm(Some_diameter + Some_height, data=something) #Question: looking at the metadata which &#39;height&#39; and &#39;diameter&#39; variables should you use? print(mdl) Plot your linear model: # hints: mdl &lt;- lm( Petal.Length ~ Sepal.Length * Species, data = iris ) iris &lt;- iris %&gt;% select( -matches(&#39;fit&#39;), -matches(&#39;lwr&#39;), -matches(&#39;upr&#39;) ) %&gt;% cbind( predict(mdl, newdata=., interval=&#39;confidence&#39;) ) head(iris, n=3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species fit lwr ## 1 5.1 3.5 1.4 0.2 setosa 1.474373 1.398783 ## 2 4.9 3.0 1.4 0.2 setosa 1.448047 1.371765 ## 3 4.7 3.2 1.3 0.2 setosa 1.421721 1.324643 ## upr ## 1 1.549964 ## 2 1.524329 ## 3 1.518798 ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point() + geom_line( aes(y=fit) ) + geom_ribbon( aes( ymin=lwr, ymax=upr, fill=Species), alpha=.3 ) # alpha is the ribbon transparency Answer the following questions: What do you think about your simile linear model? What are its limitations? How many unique species are present at WREF? What are the top_5 trees based on height? Diameter? What proportion of sampled trees are dead? 2.10 Part 2: Pulling NEON Data via the API This section covers pulling data from the NEON API or Application Programming Interface using R and the R package httr, but the core information about the API is applicable to other languages and approaches. As a reminder, there are 3 basic categories of NEON data: Observational - Data collected by a human in the field, or in an analytical laboratory, e.g. beetle identification, foliar isotopes Instrumentation - Data collected by an automated, streaming sensor, e.g. net radiation, soil carbon dioxide Remote sensing - Data collected by the airborne observation platform, e.g. LIDAR, surface reflectance This lab covers all three types of data, it is required to complete these sections in order and not skip ahead, since the query principles are explained in the first section, on observational data. 2.10 Objectives After completing this activity, you will be able to: Pull observational, instrumentation, and geolocation data from the NEON API. Transform API-accessed data from JSON to tabular format for analyses. 2.10 Things You’ll Need To Complete This Section To complete this tutorial you will need the most current version of R and, preferably, RStudio loaded on your computer. 2.10 Install R Packages httr: install.packages(&quot;httr&quot;) jsonlite: install.packages(&quot;jsonlite&quot;) dplyr: install.packages(&quot;dplyr&quot;) devtools: install.packages(&quot;devtools&quot;) downloader: install.packages(&quot;downloader&quot;) geoNEON: devtools::install_github(&quot;NEONScience/NEON-geolocation/geoNEON&quot;) neonUtilities: devtools::install_github(&quot;NEONScience/NEON-utilities/neonUtilities&quot;) Note, you must have devtools installed &amp; loaded, prior to loading geoNEON or neonUtilities. 2.10 Additional Resources Webpage for the NEON API GitHub repository for the NEON API ROpenSci wrapper for the NEON API (not covered in this tutorial) 2.11 What is an API? The following material was adapted from: “Using the NEON API in R” description: “Tutorial for getting data from the NEON API, using R and the R package httr” dateCreated: 2017-07-07 authors: [Claire K. Lunch] contributors: [Christine Laney, Megan A. Jones] If you are unfamiliar with the concept of an API, think of an API as a ‘middle person’ that provides a communication path for a software application to obtain information from a digital data source. APIs are becoming a very common means of sharing digital information. Many of the apps that you use on your computer or mobile device to produce maps, charts, reports, and other useful forms of information pull data from multiple sources using APIs. In the ecological and environmental sciences, many researchers use APIs to programmatically pull data into their analyses. (Quoted from the NEON Observatory Blog story: API and data availability viewer now live on the NEON data portal.) There are actually many types or constructions of APIs. If you’re interested you can read a little more about them here 2.11.1 Anatomy of an API call An example API call: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 This includes the base URL, endpoint, and target. 2.11.1.1 Base URL: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 Specifics are appended to this in order to get the data or metadata you’re looking for, but all calls to this API will include the base URL. For the NEON API, this is http://data.neonscience.org/api/v0 – not clickable, because the base URL by itself will take you nowhere! 2.11.1.2 Endpoints: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 What type of data or metadata are you looking for? ~/products Information about one or all of NEON’s data products ~/sites Information about data availability at the site specified in the call ~/locations Spatial data for the NEON locations specified in the call ~/data Data! By product, site, and date (in monthly chunks). 2.11.2 Targets: http://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07 The specific data product, site, or location you want to get data for. 2.11.3 Observational data (OS) Which product do you want to get data for? Consult the Explore Data Products page. We’ll pick Breeding landbird point counts, DP1.10003.001 First query the products endpoint of the API to find out which sites and dates have data available. In the products endpoint, the target is the numbered identifier for the data product: # Load the necessary libraries library(httr) ## Warning: package &#39;httr&#39; was built under R version 3.6.2 library(jsonlite) ## Warning: package &#39;jsonlite&#39; was built under R version 3.6.2 library(dplyr, quietly=T) library(downloader) # Request data using the GET function &amp; the API call req &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.10003.001&quot;) req ## Response [https://data.neonscience.org/api/v0/products/DP1.10003.001] ## Date: 2020-10-07 21:10 ## Status: 200 ## Content-Type: application/json;charset=UTF-8 ## Size: 24.2 kB The object returned from GET() has many layers of information. Entering the name of the object gives you some basic information about what you downloaded. The content() function returns the contents in the form of a highly nested list. This is typical of JSON-formatted data returned by APIs. We can use the names() function to view the different types of information within this list. # View requested data req.content &lt;- content(req, as=&quot;parsed&quot;) names(req.content$data) ## [1] &quot;productCodeLong&quot; &quot;productCode&quot; ## [3] &quot;productCodePresentation&quot; &quot;productName&quot; ## [5] &quot;productDescription&quot; &quot;productStatus&quot; ## [7] &quot;productCategory&quot; &quot;productHasExpanded&quot; ## [9] &quot;productScienceTeamAbbr&quot; &quot;productScienceTeam&quot; ## [11] &quot;productPublicationFormatType&quot; &quot;productAbstract&quot; ## [13] &quot;productDesignDescription&quot; &quot;productStudyDescription&quot; ## [15] &quot;productBasicDescription&quot; &quot;productExpandedDescription&quot; ## [17] &quot;productSensor&quot; &quot;productRemarks&quot; ## [19] &quot;themes&quot; &quot;changeLogs&quot; ## [21] &quot;specs&quot; &quot;keywords&quot; ## [23] &quot;siteCodes&quot; You can see all of the infoamtion by running the line print(req.content), but this will result in a very long printout in your console. Instead, you can view list items individually. Here, we highlight a couple of interesting examples: # View Abstract req.content$data$productAbstract ## [1] &quot;This data product contains the quality-controlled, native sampling resolution data from NEON&#39;s breeding landbird sampling. Breeding landbirds are defined as “smaller birds (usually exclusive of raptors and upland game birds) not usually associated with aquatic habitats” (Ralph et al. 1993). The breeding landbird point counts product provides records of species identification of all individuals observed during the 6-minute count period, as well as metadata which can be used to model detectability, e.g., weather, distances from observers to birds, and detection methods. The NEON point count method is adapted from the Integrated Monitoring in Bird Conservation Regions (IMBCR): Field protocol for spatially-balanced sampling of landbird populations (Hanni et al. 2017; http://bit.ly/2u2ChUB). For additional details, see the user guide, protocols, and science design listed in the Documentation section in [this data product&#39;s details webpage](https://data.neonscience.org/data-products/DP1.10003.001). \\n\\nLatency:\\nThe expected time from data and/or sample collection in the field to data publication is as follows, for each of the data tables (in days) in the downloaded data package. See the Data Product User Guide for more information.\\n \\nbrd_countdata: 120\\n\\nbrd_perpoint: 120\\n\\nbrd_personnel: 120\\n\\nbrd_references: 120&quot; # View Available months and associated URLs for Onaqui, Utah - ONAQ req.content$data$siteCodes[[27]] ## $siteCode ## [1] &quot;ONAQ&quot; ## ## $availableMonths ## $availableMonths[[1]] ## [1] &quot;2017-05&quot; ## ## $availableMonths[[2]] ## [1] &quot;2018-05&quot; ## ## $availableMonths[[3]] ## [1] &quot;2018-06&quot; ## ## $availableMonths[[4]] ## [1] &quot;2019-05&quot; ## ## ## $availableDataUrls ## $availableDataUrls[[1]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2017-05&quot; ## ## $availableDataUrls[[2]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-05&quot; ## ## $availableDataUrls[[3]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-06&quot; ## ## $availableDataUrls[[4]] ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2019-05&quot; To get a more accessible view of which sites have data for which months, you’ll need to extract data from the nested list. There are a variety of ways to do this, in this tutorial we’ll explore a couple of them. Here we’ll use fromJSON(), in the jsonlite package, which doesn’t fully flatten the nested list, but gets us the part we need. To use it, we need a text version of the content. The text version is not as human readable but is readable by the fromJSON() function. # make this JSON readable -&gt; &quot;text&quot; req.text &lt;- content(req, as=&quot;text&quot;) # Flatten data frame to see available data. avail &lt;- jsonlite::fromJSON(req.text, simplifyDataFrame=T, flatten=T) avail ## $data ## $data$productCodeLong ## [1] &quot;NEON.DOM.SITE.DP1.10003.001&quot; ## ## $data$productCode ## [1] &quot;DP1.10003.001&quot; ## ## $data$productCodePresentation ## [1] &quot;NEON.DP1.10003&quot; ## ## $data$productName ## [1] &quot;Breeding landbird point counts&quot; ## ## $data$productDescription ## [1] &quot;Count, distance from observer, and taxonomic identification of breeding landbirds observed during point counts&quot; ## ## $data$productStatus ## [1] &quot;ACTIVE&quot; ## ## $data$productCategory ## [1] &quot;Level 1 Data Product&quot; ## ## $data$productHasExpanded ## [1] TRUE ## ## $data$productScienceTeamAbbr ## [1] &quot;TOS&quot; ## ## $data$productScienceTeam ## [1] &quot;Terrestrial Observation System (TOS)&quot; ## ## $data$productPublicationFormatType ## [1] &quot;TOS Data Product Type&quot; ## ## $data$productAbstract ## [1] &quot;This data product contains the quality-controlled, native sampling resolution data from NEON&#39;s breeding landbird sampling. Breeding landbirds are defined as “smaller birds (usually exclusive of raptors and upland game birds) not usually associated with aquatic habitats” (Ralph et al. 1993). The breeding landbird point counts product provides records of species identification of all individuals observed during the 6-minute count period, as well as metadata which can be used to model detectability, e.g., weather, distances from observers to birds, and detection methods. The NEON point count method is adapted from the Integrated Monitoring in Bird Conservation Regions (IMBCR): Field protocol for spatially-balanced sampling of landbird populations (Hanni et al. 2017; http://bit.ly/2u2ChUB). For additional details, see the user guide, protocols, and science design listed in the Documentation section in [this data product&#39;s details webpage](https://data.neonscience.org/data-products/DP1.10003.001). \\n\\nLatency:\\nThe expected time from data and/or sample collection in the field to data publication is as follows, for each of the data tables (in days) in the downloaded data package. See the Data Product User Guide for more information.\\n \\nbrd_countdata: 120\\n\\nbrd_perpoint: 120\\n\\nbrd_personnel: 120\\n\\nbrd_references: 120&quot; ## ## $data$productDesignDescription ## [1] &quot;Depending on the size of the site, sampling for this product occurs either at either randomly distributed individual points or grids of nine points each. At larger sites, point count sampling occurs at five to fifteen 9-point grids, with grid centers collocated with distributed base plot centers (where plant, beetle, and/or soil sampling may also occur), if possible. At smaller sites (i.e., sites that cannot accommodate a minimum of 5 grids) point counts occur at the southwest corner (point 21) of 5-25 distributed base plots. Point counts are conducted once per breeding season at large sites and twice per breeding season at smaller sites. Point counts are six minutes long, with each minute tracked by the observer, following a two-minute settling-in period. All birds are recorded to species and sex, whenever possible, and the distance to each individual or flock is measured with a laser rangefinder, except in the case of flyovers.&quot; ## ## $data$productStudyDescription ## [1] &quot;This sampling occurs at all NEON terrestrial sites.&quot; ## ## $data$productBasicDescription ## [1] &quot;The basic package contains the per point metadata table that includes data pertaining to the observer and the weather conditions and the count data table that includes all of the observational data.&quot; ## ## $data$productExpandedDescription ## [1] &quot;The expanded package includes two additional tables and two additional fields within the count data table. The personnel table provides institutional information about each observer, as well as their performance on identification quizzes, where available. The references tables provides the list of resources used by an observer to identify birds. The additional fields in the countdata table are family and nativeStatusCode, which are derived from the NEON master list of birds.&quot; ## ## $data$productSensor ## NULL ## ## $data$productRemarks ## [1] &quot;Queries for this data product will return data collected during the date range specified for `brd_perpoint` and `brd_countdata`, but will return data from all dates for `brd_personnel` (quiz scores may occur over time periods which are distinct from when sampling occurs) and `brd_references` (which apply to a broad range of sampling dates). A record from `brd_perPoint` should have 6+ child records in `brd_countdata`, at least one per pointCountMinute. Duplicates or missing data may exist where protocol and/or data entry aberrations have occurred; users should check data carefully for anomalies before joining tables. Taxonomic IDs of species of concern have been &#39;fuzzed&#39;; see data package readme files for more information.&quot; ## ## $data$themes ## [1] &quot;Organisms, Populations, and Communities&quot; ## ## $data$changeLogs ## NULL ## ## $data$specs ## specId specNumber ## 1 3656 NEON.DOC.000916vC ## 2 2565 NEON_bird_userGuide_vA ## 3 3729 NEON.DOC.014041vJ ## ## $data$keywords ## [1] &quot;vertebrates&quot; &quot;birds&quot; &quot;diversity&quot; ## [4] &quot;taxonomy&quot; &quot;community composition&quot; &quot;distance sampling&quot; ## [7] &quot;avian&quot; &quot;species composition&quot; &quot;population&quot; ## [10] &quot;Aves&quot; &quot;Chordata&quot; &quot;point counts&quot; ## [13] &quot;landbirds&quot; &quot;invasive&quot; &quot;introduced&quot; ## [16] &quot;native&quot; &quot;animals&quot; &quot;Animalia&quot; ## ## $data$siteCodes ## siteCode ## 1 ABBY ## 2 BARR ## 3 BART ## 4 BLAN ## 5 BONA ## 6 CLBJ ## 7 CPER ## 8 DCFS ## 9 DEJU ## 10 DELA ## 11 DSNY ## 12 GRSM ## 13 GUAN ## 14 HARV ## 15 HEAL ## 16 JERC ## 17 JORN ## 18 KONA ## 19 KONZ ## 20 LAJA ## 21 LENO ## 22 MLBS ## 23 MOAB ## 24 NIWO ## 25 NOGP ## 26 OAES ## 27 ONAQ ## 28 ORNL ## 29 OSBS ## 30 PUUM ## 31 RMNP ## 32 SCBI ## 33 SERC ## 34 SJER ## 35 SOAP ## 36 SRER ## 37 STEI ## 38 STER ## 39 TALL ## 40 TEAK ## 41 TOOL ## 42 TREE ## 43 UKFS ## 44 UNDE ## 45 WOOD ## 46 WREF ## 47 YELL ## availableMonths ## 1 2017-05, 2017-06, 2018-06, 2018-07, 2019-05 ## 2 2017-07, 2018-07, 2019-06 ## 3 2015-06, 2016-06, 2017-06, 2018-06, 2019-06 ## 4 2017-05, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06 ## 5 2017-06, 2018-06, 2018-07, 2019-06 ## 6 2017-05, 2018-04, 2019-04, 2019-05 ## 7 2013-06, 2015-05, 2016-05, 2017-05, 2017-06, 2018-05, 2019-06 ## 8 2017-06, 2017-07, 2018-07, 2019-06, 2019-07 ## 9 2017-06, 2018-06, 2019-06 ## 10 2015-06, 2017-06, 2018-05, 2019-06 ## 11 2015-06, 2016-05, 2017-05, 2018-05, 2019-05 ## 12 2016-06, 2017-05, 2017-06, 2018-05, 2019-05 ## 13 2015-05, 2017-05, 2018-05, 2019-05, 2019-06 ## 14 2015-05, 2015-06, 2016-06, 2017-06, 2018-06, 2019-06 ## 15 2017-06, 2018-06, 2018-07, 2019-06, 2019-07 ## 16 2016-06, 2017-05, 2018-06, 2019-06 ## 17 2017-04, 2017-05, 2018-04, 2018-05, 2019-04 ## 18 2018-05, 2018-06, 2019-06 ## 19 2017-06, 2018-05, 2018-06, 2019-06 ## 20 2017-05, 2018-05, 2019-05, 2019-06 ## 21 2017-06, 2018-05, 2019-06 ## 22 2018-06, 2019-05 ## 23 2015-06, 2017-05, 2018-05, 2019-05 ## 24 2015-07, 2017-07, 2018-07, 2019-07 ## 25 2017-07, 2018-07, 2019-07 ## 26 2017-05, 2017-06, 2018-04, 2018-05, 2019-05 ## 27 2017-05, 2018-05, 2018-06, 2019-05 ## 28 2016-05, 2016-06, 2017-05, 2018-06, 2019-05 ## 29 2016-05, 2017-05, 2018-05, 2019-05 ## 30 2018-04 ## 31 2017-06, 2017-07, 2018-06, 2018-07, 2019-06, 2019-07 ## 32 2015-06, 2016-05, 2016-06, 2017-05, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06 ## 33 2017-05, 2017-06, 2018-05, 2019-05 ## 34 2017-04, 2018-04, 2019-04 ## 35 2017-05, 2018-05, 2019-05 ## 36 2017-05, 2018-04, 2018-05, 2019-04 ## 37 2016-05, 2016-06, 2017-06, 2018-05, 2018-06, 2019-05, 2019-06 ## 38 2013-06, 2015-05, 2016-05, 2017-05, 2018-05, 2019-05, 2019-06 ## 39 2015-06, 2016-07, 2017-06, 2018-06, 2019-05 ## 40 2017-06, 2018-06, 2019-06, 2019-07 ## 41 2017-06, 2018-07, 2019-06 ## 42 2016-06, 2017-06, 2018-06, 2019-06 ## 43 2017-06, 2018-06, 2019-06 ## 44 2016-06, 2016-07, 2017-06, 2018-06, 2019-06 ## 45 2015-07, 2017-07, 2018-07, 2019-06, 2019-07 ## 46 2018-06, 2019-05, 2019-06 ## 47 2018-06, 2019-06 ## availableDataUrls ## 1 https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2019-05 ## 2 https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2019-06 ## 3 https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2019-06 ## 4 https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/BLAN/2019-06 ## 5 https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/BONA/2019-06 ## 6 https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2019-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/CLBJ/2019-05 ## 7 https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2013-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/CPER/2019-06 ## 8 https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DCFS/2019-07 ## 9 https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DEJU/2019-06 ## 10 https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DELA/2019-06 ## 11 https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/DSNY/2019-05 ## 12 https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GRSM/2019-05 ## 13 https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/GUAN/2019-06 ## 14 https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HARV/2019-06 ## 15 https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/HEAL/2019-07 ## 16 https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/JERC/2019-06 ## 17 https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2017-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/JORN/2019-04 ## 18 https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONA/2019-06 ## 19 https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/KONZ/2019-06 ## 20 https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LAJA/2019-06 ## 21 https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/LENO/2019-06 ## 22 https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/MLBS/2019-05 ## 23 https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/MOAB/2019-05 ## 24 https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2015-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NIWO/2019-07 ## 25 https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/NOGP/2019-07 ## 26 https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OAES/2019-05 ## 27 https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ONAQ/2019-05 ## 28 https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/ORNL/2019-05 ## 29 https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/OSBS/2019-05 ## 30 https://data.neonscience.org/api/v0/data/DP1.10003.001/PUUM/2018-04 ## 31 https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/RMNP/2019-07 ## 32 https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SCBI/2019-06 ## 33 https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SERC/2019-05 ## 34 https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2017-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SJER/2019-04 ## 35 https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SOAP/2019-05 ## 36 https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2018-04, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/SRER/2019-04 ## 37 https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STEI/2019-06 ## 38 https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2013-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2015-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2016-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2017-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2018-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/STER/2019-06 ## 39 https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2015-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2016-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TALL/2019-05 ## 40 https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TEAK/2019-07 ## 41 https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/TOOL/2019-06 ## 42 https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/TREE/2019-06 ## 43 https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UKFS/2019-06 ## 44 https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2016-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2016-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2017-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/UNDE/2019-06 ## 45 https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2015-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2017-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2018-07, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2019-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/WOOD/2019-07 ## 46 https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2019-05, https://data.neonscience.org/api/v0/data/DP1.10003.001/WREF/2019-06 ## 47 https://data.neonscience.org/api/v0/data/DP1.10003.001/YELL/2018-06, https://data.neonscience.org/api/v0/data/DP1.10003.001/YELL/2019-06 The object contains a lot of information about the data product, including: keywords under $data$keywords, references for documentation under $data$specs, data availability by site and month under $data$siteCodes, and specific URLs for the API calls for each site and month under $data$siteCodes$availableDataUrls. We need $data$siteCodes to tell us what we can download. $data$siteCodes$availableDataUrls allows us to avoid writing the API calls ourselves in the next steps. # get data availability list for the product bird.urls &lt;- unlist(avail$data$siteCodes$availableDataUrls) length(bird.urls) #total number of URLs ## [1] 204 bird.urls[1:10] #show first 10 URLs available ## [1] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-05&quot; ## [2] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2017-06&quot; ## [3] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-06&quot; ## [4] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2018-07&quot; ## [5] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/ABBY/2019-05&quot; ## [6] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2017-07&quot; ## [7] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2018-07&quot; ## [8] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BARR/2019-06&quot; ## [9] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2015-06&quot; ## [10] &quot;https://data.neonscience.org/api/v0/data/DP1.10003.001/BART/2016-06&quot; These are the URLs showing us what files are available for each month where there are data. Let’s look at the bird data from Woodworth (WOOD) site from July 2015. We can do this by using the above code but now specifying which site/date we want using the grep() function. Note that if there were only one month of data from a site, you could leave off the date in the function. If you want data from more than one site/month you need to iterate this code, GET fails if you give it more than one URL. # get data availability for WOOD July 2015 brd &lt;- GET(bird.urls[grep(&quot;WOOD/2015-07&quot;, bird.urls)]) brd.files &lt;- jsonlite::fromJSON(content(brd, as=&quot;text&quot;)) # view just the available data files brd.files$data$files ## name ## 1 NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.basic.20191107T152331Z.csv ## 2 NEON.D09.WOOD.DP1.10003.001.readme.20191107T152331Z.txt ## 3 NEON.D09.WOOD.DP1.10003.001.2015-07.basic.20191107T152331Z.zip ## 4 NEON.D09.WOOD.DP0.10003.001.validation.20191107T152331Z.csv ## 5 NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20191107T152331Z.xml ## 6 NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.basic.20191107T152331Z.csv ## 7 NEON.D09.WOOD.DP1.10003.001.variables.20191107T152331Z.csv ## 8 NEON.D09.WOOD.DP1.10003.001.2015-07.expanded.20191107T152331Z.zip ## 9 NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.expanded.20191107T152331Z.csv ## 10 NEON.D09.WOOD.DP1.10003.001.readme.20191107T152331Z.txt ## 11 NEON.Bird_Conservancy_of_the_Rockies.brd_personnel.csv ## 12 NEON.D09.WOOD.DP1.10003.001.brd_references.expanded.20191107T152331Z.csv ## 13 NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.expanded.20191107T152331Z.csv ## 14 NEON.D09.WOOD.DP1.10003.001.variables.20191107T152331Z.csv ## 15 NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20191107T152331Z.xml ## 16 NEON.D09.WOOD.DP0.10003.001.validation.20191107T152331Z.csv ## size md5 crc32 ## 1 23521 f37931d46213246dccf2a161211c9afe NA ## 2 12784 d84b496cf950b5b96e762473beda563a NA ## 3 67816 4438e5e050fc7be5949457f42089a397 NA ## 4 10084 6d15da01c03793da8fc6d871e6659ea8 NA ## 5 70539 df102cb4cfdce092cda3c0942c9d9b67 NA ## 6 346679 e0adb3146b5cce59eea09864145efcb1 NA ## 7 7337 e67f1ae72760a63c616ec18108453aaa NA ## 8 79998 22e3353dabb8b154768dc2eee9873718 NA ## 9 367402 2ad379ae44f4e87996bdc3dee70a0794 NA ## 10 13063 680a2f53c0a9d1b0ab4f8814bda5b399 NA ## 11 46349 a2c47410a6a0f49d0b1cf95be6238604 NA ## 12 1012 d76cfc5443ac27a058fab1d319d31d34 NA ## 13 23521 f37931d46213246dccf2a161211c9afe NA ## 14 7337 e67f1ae72760a63c616ec18108453aaa NA ## 15 78750 6ba91b6e109ff14d1911dcaad9febeb9 NA ## 16 10084 6d15da01c03793da8fc6d871e6659ea8 NA ## url ## 1 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.basic.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=1378763481b4727000e9dbf2b97ea6782a94b04461862d9f0951025a5e8e0203 ## 2 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.readme.20191107T152331Z.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=38919a5ea892862edc1a44ce6a8a912c45970cac0063e705a083be263a8a67e1 ## 3 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.2015-07.basic.20191107T152331Z.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=bd3be05a5ef0801c1ed9b92aec8c3f191194a14daebd8d18bb95b19667113de3 ## 4 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP0.10003.001.validation.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=d4e6f71246aba5f69bc6c89202bcdd6cefaee6b94544c79c115497eb93770950 ## 5 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20191107T152331Z.xml?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=6432872495c75637bce890a9fd3f3af10e9d8dd497bf01231c305a433aaaa7e2 ## 6 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.basic.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=1d3379cdbe73bc9f8fbe094801bf3f60a2ca27df1fc26c379f3b74b26ca97d55 ## 7 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/basic/NEON.D09.WOOD.DP1.10003.001.variables.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3599&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=eb211794952872ab0c262d61bff0d6f8f43bf78979d5c1fc0021094dc5697198 ## 8 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.2015-07.expanded.20191107T152331Z.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=c13988b226738a70b776a7d07ff475fbe41ba2c0b14e3ca7bf7e01813c03fdc4 ## 9 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_countdata.2015-07.expanded.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=cdbd49737d2eb497430b0719b6fdfc4509ac8f8111058b01626d057798555544 ## 10 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.readme.20191107T152331Z.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=c28356ff7a49d8dc9ac23df635973b6bf37c3b6f8465e2236f1ddfdcfeef26f6 ## 11 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.Bird_Conservancy_of_the_Rockies.brd_personnel.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=bf8f185d83b41319cfcb58f1ba4884ff7ddcba564bea40d4fc242658e0d579a4 ## 12 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_references.expanded.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=49cb31e973301621bf5ce22c805dbb7305001329e84cbcf05615003a2f52d992 ## 13 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.brd_perpoint.2015-07.expanded.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=b90140cf6b0ff98745a4a962da7102a2729f864fe2f9f961e116bf0a52d18047 ## 14 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.variables.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=188cd66df4554dcc6f1c1adc631f56c106c2becce9a9e3236cd1ba824059ddfe ## 15 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP1.10003.001.EML.20150701-20150705.20191107T152331Z.xml?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=5fbccd9e7091685af751e9b865598f2d0c8a19822361621c807ea60b33287c56 ## 16 https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.10003.001/PROV/WOOD/20150701T000000--20150801T000000/expanded/NEON.D09.WOOD.DP0.10003.001.validation.20191107T152331Z.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200925T172057Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=pub-internal-read%2F20200925%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Signature=9b741d073a1e22c6f01dc7d16f312ceb5cb5bd9e6486608f695419f6dc6ef557 In this output, name and url are key fields. It provides us with the names of the files available for this site and month, and URLs where we can get the files. We’ll use the file names to pick which ones we want. The available files include both data and metadata, and both the basic and expanded data packages. Typically the expanded package includes additional quality or uncertainty data, either in additional files or additional fields than in the basic files. Basic and expanded data packages are available for most NEON data products (some only have basic). Metadata are described by file name below. The format for most of the file names is: NEON.[domain number].[site code].[data product ID].[file-specific name]. [date of file creation] Some files omit the domain and site, since they’re not specific to a location, like the data product readme. The date of file creation uses the ISO6801 format, in this case 20170720T182547Z, and can be used to determine whether data have been updated since the last time you downloaded. Available files in our query for July 2015 at Woodworth are all of the following (leaving off the initial NEON.D09.WOOD.10003.001): ~.2015-07.expanded.20170720T182547Z.zip: zip of all files in the expanded package ~.brd_countdata.2015-07.expanded.20170720T182547Z.csv: count data table, expanded package version: counts of birds at each point ~.brd_perpoint.2015-07.expanded.20170720T182547Z.csv: point data table, expanded package version: metadata at each observation point NEON.Bird Conservancy of the Rockies.brd_personnel.csv: personnel data table, accuracy scores for bird observers ~.2015-07.basic.20170720T182547Z.zip: zip of all files in the basic package ~.brd_countdata.2015-07.basic.20170720T182547Z.csv: count data table, basic package version: counts of birds at each point ~.brd_perpoint.2015-07.basic.20170720T182547Z.csv: point data table, basic package version: metadata at each observation point NEON.DP1.10003.001_readme.txt: readme for the data product (not specific to dates or location). Appears twice in the list, since it’s in both the basic and expanded package ~.20150101-20160613.xml: Ecological Metadata Language (EML) file. Appears twice in the list, since it’s in both the basic and expanded package ~.validation.20170720T182547Z.csv: validation file for the data product, lists input data and data entry rules. Appears twice in the list, since it’s in both the basic and expanded package ~.variables.20170720T182547Z.csv: variables file for the data product, lists data fields in downloaded tables. Appears twice in the list, since it’s in both the basic and expanded package We’ll get the data tables for the point data and count data in the basic package. The list of files doesn’t return in the same order every time, so we won’t use position in the list to select. Plus, we want code we can re-use when getting data from other sites and other months. So we select files based on the data table name and the package name. # Get both files brd.count &lt;- read.delim(brd.files$data$files$url [intersect(grep(&quot;countdata&quot;, brd.files$data$files$name), grep(&quot;basic&quot;, brd.files$data$files$name))], sep=&quot;,&quot;) brd.point &lt;- read.delim(brd.files$data$files$url [intersect(grep(&quot;perpoint&quot;, brd.files$data$files$name), grep(&quot;basic&quot;, brd.files$data$files$name))], sep=&quot;,&quot;) Now we have the data and can access it in R. Just to show that the files we pulled have actual data in them, let’s make a quick graphic: # Cluster by species clusterBySp &lt;- brd.count %&gt;% dplyr::group_by(scientificName) %&gt;% dplyr::summarise(total=sum(clusterSize, na.rm=T)) ## `summarise()` ungrouping output (override with `.groups` argument) # Reorder so list is ordered most to least abundance clusterBySp &lt;- clusterBySp[order(clusterBySp$total, decreasing=T),] # Plot barplot(clusterBySp$total, names.arg=clusterBySp$scientificName, ylab=&quot;Total&quot;, cex.names=0.5, las=2) Wow! There are lots of Agelaius phoeniceus (Red-winged Blackbirds) at WOOD in July. 2.11.4 Instrumentation data (IS) The process is essentially the same for sensor data. We’ll do the same series of queries for Soil Temperature, DP1.00041.001. Let’s use data from Moab in June 2017 this time. # Request soil temperature data availability info req.soil &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.00041.001&quot;) # make this JSON readable # Note how we&#39;ve change this from two commands into one here avail.soil &lt;- jsonlite::fromJSON(content(req.soil, as=&quot;text&quot;), simplifyDataFrame=T, flatten=T) # get data availability list for the product temp.urls &lt;- unlist(avail.soil$data$siteCodes$availableDataUrls) # get data availability from location/date of interest tmp &lt;- GET(temp.urls[grep(&quot;MOAB/2017-06&quot;, temp.urls)]) tmp.files &lt;- jsonlite::fromJSON(content(tmp, as=&quot;text&quot;)) length(tmp.files$data$files$name) # There are a lot of available files ## [1] 190 tmp.files$data$files$name[1:10] # Let&#39;s print the first 10 ## [1] &quot;NEON.D13.MOAB.DP1.00041.001.002.508.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [2] &quot;NEON.D13.MOAB.DP1.00041.001.003.506.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [3] &quot;NEON.D13.MOAB.DP1.00041.001.001.504.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [4] &quot;NEON.D13.MOAB.DP1.00041.001.sensor_positions.20200620T070859Z.csv&quot; ## [5] &quot;NEON.D13.MOAB.DP1.00041.001.001.501.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [6] &quot;NEON.D13.MOAB.DP1.00041.001.004.505.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [7] &quot;NEON.D13.MOAB.DP1.00041.001.002.502.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [8] &quot;NEON.D13.MOAB.DP1.00041.001.003.505.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [9] &quot;NEON.D13.MOAB.DP1.00041.001.003.504.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; ## [10] &quot;NEON.D13.MOAB.DP1.00041.001.005.508.001.ST_1_minute.2017-06.expanded.20200620T070859Z.csv&quot; These file names start and end the same way as the observational files, but the middle is a little more cryptic. The structure from beginning to end is: NEON.[domain number].[site code].[data product ID].00000. [soil plot number].[depth].[averaging interval].[data table name]. [year]-[month].[data package].[date of file creation] So “NEON.D13.MOAB.DP1.00041.001.003.507.030.ST_30_minute.2017-06.expanded.20200620T070859Z.csv” is the: NEON (NEON.) Domain 13 (.D13.) Moab field site (.MOAB.) soil temperature data (.DP1.00041.001.) collected in Soil Plot 2, (.002.) at the 7th depth below the surface (.507.) and reported as a 30-minute mean of (.030. and .ST_30_minute.) only for the period of June 2017 (.2017-06.) and provided in a expanded data package (.basic.) published on June 20th, 2020 (.0200620T070859Z.). More information about interpreting file names can be found in the readme that accompanies each download. Let’s get data (and the URL) for only the 2nd depth described above by selecting 002.502.030 and the word basic in the file name. Go get it: soil.temp &lt;- read.delim(tmp.files$data$files$url [intersect(grep(&quot;002.502.030&quot;, tmp.files$data$files$name), grep(&quot;basic&quot;, tmp.files$data$files$name))], sep=&quot;,&quot;) Now we have the data and can use it to conduct our analyses. To take a quick look at it, let’s plot the mean soil temperature by date. # plot temp ~ date plot(soil.temp$soilTempMean~as.POSIXct(soil.temp$startDateTime, format=&quot;%Y-%m-%d T %H:%M:%S Z&quot;), pch=&quot;.&quot;, xlab=&quot;Date&quot;, ylab=&quot;T&quot;) As we’d expect we see daily fluctuation in soil temperature. 2.11.5 Remote sensing data (AOP) Again, the process of determining which sites and time periods have data, and finding the URLs for those data, is the same as for the other data types. We’ll go looking for High resolution orthorectified camera imagery, DP1.30010, and we’ll look at the flight over San Joaquin Experimental Range (SJER) in March 2017. # Request camera data availability info req.aop &lt;- GET(&quot;http://data.neonscience.org/api/v0/products/DP1.30010.001&quot;) # make this JSON readable # Note how we&#39;ve changed this from two commands into one here avail.aop &lt;- jsonlite::fromJSON(content(req.aop, as=&quot;text&quot;), simplifyDataFrame=T, flatten=T) # get data availability list for the product cam.urls &lt;- unlist(avail.aop$data$siteCodes$availableDataUrls) # get data availability from location/date of interest cam &lt;- GET(cam.urls[intersect(grep(&quot;SJER&quot;, cam.urls), grep(&quot;2017&quot;, cam.urls))]) cam.files &lt;- jsonlite::fromJSON(content(cam, as=&quot;text&quot;)) # this list of files is very long, so we&#39;ll just look at the first ten head(cam.files$data$files$name, 10) ## [1] &quot;17032816_EH021656(20170328180546)-0198_ort.tif&quot; ## [2] &quot;17032816_EH021656(20170328185907)-0614_ort.tif&quot; ## [3] &quot;17032816_EH021656(20170328180026)-0159_ort.tif&quot; ## [4] &quot;17032816_EH021656(20170328181528)-0264_ort.tif&quot; ## [5] &quot;17032816_EH021656(20170328185957)-0625_ort.tif&quot; ## [6] &quot;17032816_EH021656(20170328184657)-0515_ort.tif&quot; ## [7] &quot;17032816_EH021656(20170328192449)-0832_ort.tif&quot; ## [8] &quot;17032816_EH021656(20170328195229)-1073_ort.tif&quot; ## [9] &quot;17032816_EH021656(20170328190517)-0664_ort.tif&quot; ## [10] &quot;17032816_EH021656(20170328184904)-0542_ort.tif&quot; File names for AOP data are more variable than for IS or OS data; different AOP data products use different naming conventions. File formats differ by product as well. This particular product, camera imagery, is stored in TIFF files. Instead of reading a TIFF into R, we’ll download it to the working directory. This is one option for getting AOP files from the API. To download the TIFF file, we use the downloader package, and we’ll select a file based on the time stamp in the file name: 20170328192931 download(cam.files$data$files$url[grep(&quot;20170328192931&quot;, cam.files$data$files$name)], paste(getwd(), &quot;/SJER_image.tif&quot;, sep=&quot;&quot;), mode=&quot;wb&quot;) The image, below, of the San Joaquin Experimental Range should now be in your working directory. An example of camera data (DP1.30010.001) from the San Joaquin Experimental Range. Source: National Ecological Observatory Network (NEON) 2.11.6 Geolocation data You may have noticed some of the spatial data referenced above are a bit vague, e.g. “soil plot 2, 4th depth below the surface.” This section describes how to get spatial data and what to do with it depends on which type of data you’re working with. 2.11.6.1 Instrumentation data (both aquatic and terrestrial) Downloads of instrument system (IS) data include a file called sensor_positions.csv. The sensor positions file contains information about the coordinates of each sensor, relative to a reference location. While the specifics vary, techniques are generalizable for working with sensor data and the sensor_positions.csv file. Let’s look at the sensor locations for photosynthetically active radiation (PAR; DP1.00024.001) at the NEON Treehaven site (TREE) in July 2018. To reduce our file size, we’ll use the 30 minute averaging interval. Our final product from this section is to create a spatially explicit picture of light attenuation through the canopy. # load PAR data of interest par &lt;- loadByProduct(dpID=&quot;DP1.00024.001&quot;, site=&quot;TREE&quot;, startdate=&quot;2018-07&quot;, enddate=&quot;2018-07&quot;, avg=30, check.size=F, token=NEON_TOKEN) ## Finding available files ## | | | 0% | |======================================================================| 100% ## ## Downloading files totaling approximately 934.7 KiB ## Downloading 9 files ## | | | 0% | |========= | 12% | |================== | 25% | |========================== | 38% | |=================================== | 50% | |============================================ | 62% | |==================================================== | 75% | |============================================================= | 88% | |======================================================================| 100% ## ## Stacking operation across a single core. ## Stacking table PARPAR_30min ## Merged the most recent publication of sensor position files for each site and saved to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 1 data tables and 2 metadata tables! ## Stacking took 0.0997839 secs ## All unzipped monthly data folders have been removed. Now we can specifically look at the sensor positions file: # create object for sens. pos. file pos &lt;- par$sensor_positions_00024 # view names names(pos) ## [1] &quot;siteID&quot; &quot;HOR.VER&quot; &quot;name&quot; ## [4] &quot;description&quot; &quot;start&quot; &quot;end&quot; ## [7] &quot;referenceName&quot; &quot;referenceDescription&quot; &quot;referenceStart&quot; ## [10] &quot;referenceEnd&quot; &quot;xOffset&quot; &quot;yOffset&quot; ## [13] &quot;zOffset&quot; &quot;pitch&quot; &quot;roll&quot; ## [16] &quot;azimuth&quot; &quot;referenceLatitude&quot; &quot;referenceLongitude&quot; ## [19] &quot;referenceElevation&quot; &quot;publicationDate&quot; The sensor locations are indexed by the HOR.VER variable - see the file naming conventions page for more details. Using unique() we can view all the locations indexes in this file. # view names unique(pos$HOR.VER) ## [1] &quot;000.010&quot; &quot;000.020&quot; &quot;000.030&quot; &quot;000.040&quot; &quot;000.050&quot; &quot;000.060&quot; PAR data are collected at multiple levels of the NEON tower but along a single vertical plane. We see this reflected in the data where HOR=000 (all data collected) at the tower location. The VER index varies (VER = 010 to 060) showing that the vertical position is changing and that PAR is measured at six different levels. The x, y, and z offsets in the sensor positions file are the relative distance, in meters, to the reference latitude, longitude, and elevation in the file. The HOR and VER indices in the sensor positions file correspond to the verticalPosition and horizontalPosition fields in par$PARPAR_30min. Say we wanted to plot a profile of the PAR through the canopy, we would need to start by using the aggregate() function to calculate mean PAR at each vertical position on the tower over the month: # calc mean PAR at each level parMean &lt;- aggregate(par$PARPAR_30min$PARMean, by=list(par$PARPAR_30min$verticalPosition), FUN=mean, na.rm=T) Now we can plot mean PAR relative to height on the tower (or the zOffset): # plot PAR plot(parMean$x, parMean$Group.1, type=&quot;b&quot;, pch=20, xlab=&quot;Photosynthetically active radiation&quot;, ylab=&quot;Height above tower base (m)&quot;) 2.11.6.2 Observational data - Terrestrial Latitude, longitude, elevation, and associated uncertainties are included in data downloads (Remember NEON COding Lab part 1?). These are the coordinates and uncertainty of the sampling plot; for many protocols it is possible to calculate a more precise location. Instructions for doing this are in the respective data product user guides, and code is in the geoNEON package on GitHub. 2.11.7 Querying a single named location Let’s look at the named locations in the bird data we downloaded above. To do this, look for the field called namedLocation, which is present in all observational data products, both aquatic and terrestrial. # view named location head(brd.point$namedLocation) ## [1] &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; ## [4] &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; &quot;WOOD_013.birdGrid.brd&quot; Here we see the first six entries in the namedLocation column which tells us the names of the Terrestrial Observation plots where the bird surveys were conducted. We can query the locations endpoint of the API for the first named location, WOOD_013.birdGrid.brd. # location data req.loc &lt;- GET(&quot;http://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd&quot;) # make this JSON readable brd.WOOD_013 &lt;- jsonlite::fromJSON(content(req.loc, as=&quot;text&quot;)) brd.WOOD_013 ## $data ## $data$locationName ## [1] &quot;WOOD_013.birdGrid.brd&quot; ## ## $data$locationDescription ## [1] &quot;Plot \\&quot;WOOD_013\\&quot; at site \\&quot;WOOD\\&quot;&quot; ## ## $data$locationType ## [1] &quot;OS Plot - brd&quot; ## ## $data$domainCode ## [1] &quot;D09&quot; ## ## $data$siteCode ## [1] &quot;WOOD&quot; ## ## $data$locationDecimalLatitude ## [1] 47.13912 ## ## $data$locationDecimalLongitude ## [1] -99.23243 ## ## $data$locationElevation ## [1] 579.31 ## ## $data$locationUtmEasting ## [1] 482375.7 ## ## $data$locationUtmNorthing ## [1] 5220650 ## ## $data$locationUtmHemisphere ## [1] &quot;N&quot; ## ## $data$locationUtmZone ## [1] 14 ## ## $data$alphaOrientation ## [1] 0 ## ## $data$betaOrientation ## [1] 0 ## ## $data$gammaOrientation ## [1] 0 ## ## $data$xOffset ## [1] 0 ## ## $data$yOffset ## [1] 0 ## ## $data$zOffset ## [1] 0 ## ## $data$offsetLocation ## NULL ## ## $data$locationProperties ## locationPropertyName locationPropertyValue ## 1 Value for Coordinate source GeoXH 6000 ## 2 Value for Coordinate uncertainty 0.28 ## 3 Value for Country unitedStates ## 4 Value for County Stutsman ## 5 Value for Elevation uncertainty 0.48 ## 6 Value for Filtered positions 121 ## 7 Value for Geodetic datum WGS84 ## 8 Value for Horizontal dilution of precision 1 ## 9 Value for Maximum elevation 579.31 ## 10 Value for Minimum elevation 569.79 ## 11 Value for National Land Cover Database (2001) grasslandHerbaceous ## 12 Value for Plot dimensions 500m x 500m ## 13 Value for Plot ID WOOD_013 ## 14 Value for Plot size 250000 ## 15 Value for Plot subtype birdGrid ## 16 Value for Plot type distributed ## 17 Value for Positional dilution of precision 2.4 ## 18 Value for Reference Point Position B2 ## 19 Value for Slope aspect 238.91 ## 20 Value for Slope gradient 2.83 ## 21 Value for Soil type order Mollisols ## 22 Value for State province ND ## 23 Value for Subtype Specification ninePoints ## 24 Value for UTM Zone 14N ## ## $data$locationParent ## [1] &quot;WOOD&quot; ## ## $data$locationParentUrl ## [1] &quot;https://data.neonscience.org/api/v0/locations/WOOD&quot; ## ## $data$locationChildren ## [1] &quot;WOOD_013.birdGrid.brd.B2&quot; &quot;WOOD_013.birdGrid.brd.A2&quot; ## [3] &quot;WOOD_013.birdGrid.brd.C3&quot; &quot;WOOD_013.birdGrid.brd.A3&quot; ## [5] &quot;WOOD_013.birdGrid.brd.B3&quot; &quot;WOOD_013.birdGrid.brd.C1&quot; ## [7] &quot;WOOD_013.birdGrid.brd.A1&quot; &quot;WOOD_013.birdGrid.brd.B1&quot; ## [9] &quot;WOOD_013.birdGrid.brd.C2&quot; ## ## $data$locationChildrenUrls ## [1] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B2&quot; ## [2] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A2&quot; ## [3] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C3&quot; ## [4] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A3&quot; ## [5] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B3&quot; ## [6] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C1&quot; ## [7] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.A1&quot; ## [8] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.B1&quot; ## [9] &quot;https://data.neonscience.org/api/v0/locations/WOOD_013.birdGrid.brd.C2&quot; Note spatial information under $data$[nameOfCoordinate] and under $data$locationProperties. Also note $data$locationChildren: these are the finer scale locations that can be used to calculate precise spatial data for bird observations. For convenience, we’ll use the geoNEON package to make the calculations. First we’ll use getLocByName() to get the additional spatial information available through the API, and look at the spatial resolution available in the initial download: # load the geoNEON package library(geoNEON) # extract the spatial data brd.point.loc &lt;- getLocByName(brd.point) ## | | | 0% | |========== | 14% | |==================== | 29% | |============================== | 43% | |======================================== | 57% | |================================================== | 71% | |============================================================ | 86% | |======================================================================| 100% # plot bird point locations # note that decimal degrees is also an option in the data symbols(brd.point.loc$easting, brd.point.loc$northing, circles=brd.point.loc$coordinateUncertainty, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;, tck=0.01, inches=F) And use getLocTOS() to calculate the point locations of observations. brd.point.pt &lt;- getLocTOS(brd.point, &quot;brd_perpoint&quot;) ## | | | 0% | |= | 2% | |== | 3% | |=== | 5% | |==== | 6% | |====== | 8% | |======= | 10% | |======== | 11% | |========= | 13% | |========== | 14% | |=========== | 16% | |============ | 17% | |============= | 19% | |============== | 21% | |================ | 22% | |================= | 24% | |================== | 25% | |=================== | 27% | |==================== | 29% | |===================== | 30% | |====================== | 32% | |======================= | 33% | |======================== | 35% | |========================== | 37% | |=========================== | 38% | |============================ | 40% | |============================= | 41% | |============================== | 43% | |=============================== | 44% | |================================ | 46% | |================================= | 48% | |================================== | 49% | |==================================== | 51% | |===================================== | 52% | |====================================== | 54% | |======================================= | 56% | |======================================== | 57% | |========================================= | 59% | |========================================== | 60% | |=========================================== | 62% | |============================================ | 63% | |============================================== | 65% | |=============================================== | 67% | |================================================ | 68% | |================================================= | 70% | |================================================== | 71% | |=================================================== | 73% | |==================================================== | 75% | |===================================================== | 76% | |====================================================== | 78% | |======================================================== | 79% | |========================================================= | 81% | |========================================================== | 83% | |=========================================================== | 84% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 89% | |=============================================================== | 90% | |================================================================ | 92% | |================================================================== | 94% | |=================================================================== | 95% | |==================================================================== | 97% | |===================================================================== | 98% | |======================================================================| 100% # plot bird point locations # note that decimal degrees is also an option in the data symbols(brd.point.pt$adjEasting, brd.point.pt$adjNorthing, circles=brd.point.pt$adjCoordinateUncertainty, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;, tck=0.01, inches=F) Now you can see the individual points where the respective point counts were located. 2.11.8 Taxonomy NEON maintains accepted taxonomies for many of the taxonomic identification data we collect. NEON taxonomies are available for query via the API; they are also provided via an interactive user interface, the Taxon Viewer. NEON taxonomy data provides the reference information for how NEON validates taxa; an identification must appear in the taxonomy lists in order to be accepted into the NEON database. Additions to the lists are reviewed regularly. The taxonomy lists also provide the author of the scientific name, and the reference text used. The taxonomy endpoint of the API works a little bit differently from the other endpoints. In the “Anatomy of an API Call” section above, each endpoint has a single type of target - a data product number, a named location name, etc. For taxonomic data, there are multiple query options, and some of them can be used in combination. For example, a query for taxa in the Pinaceae family: http://data.neonscience.org/api/v0/taxonomy/?family=Pinaceae The available types of queries are listed in the taxonomy section of the API web page. Briefly, they are: taxonTypeCode: Which of the taxonomies maintained by NEON are you looking for? BIRD, FISH, PLANT, etc. Cannot be used in combination with the taxonomic rank queries. each of the major taxonomic ranks from genus through kingdom scientificname: Genus + specific epithet (+ authority). Search is by exact match only, see final example below. verbose: Do you want the short (false) or long (true) response offset: Skip this number of items in the list. Defaults to 50. limit: Result set will be truncated at this length. Defaults to 50. Staff on the NEON project have plans to modify the settings for offset and limit, such that offset will default to 0 and limit will default to ∞, but in the meantime users will want to set these manually. They are set to non-default values in the examples below. For the first example, let’s query for the loon family, Gaviidae, in the bird taxonomy. Note that query parameters are case-sensitive. loon.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?family=Gaviidae&amp;offset=0&amp;limit=500&quot;) Parse the results into a list using fromJSON(): loon.list &lt;- jsonlite::fromJSON(content(loon.req, as=&quot;text&quot;)) And look at the $data element of the results, which contains: The full taxonomy of each taxon The short taxon code used by NEON (taxonID/acceptedTaxonID) The author of the scientific name (scientificNameAuthorship) The vernacular name, if applicable The reference text used (nameAccordingToID) The terms used for each field are matched to Darwin Core (dwc) and the Global Biodiversity Information Facility (gbif) terms, where possible, and the matches are indicated in the column headers. loon.list$data ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 BIRD ARLO ARLO Gavia arctica ## 2 BIRD COLO COLO Gavia immer ## 3 BIRD PALO PALO Gavia pacifica ## 4 BIRD RTLO RTLO Gavia stellata ## 5 BIRD YBLO YBLO Gavia adamsii ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 (Linnaeus) species Arctic Loon ## 2 (Brunnich) species Common Loon ## 3 (Lawrence) species Pacific Loon ## 4 (Pontoppidan) species Red-throated Loon ## 5 (G. R. Gray) species Yellow-billed Loon ## dwc:nameAccordingToID dwc:kingdom dwc:phylum dwc:class dwc:order ## 1 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## 2 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## 3 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## 4 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## 5 doi: 10.1642/AUK-15-73.1 Animalia Chordata Aves Gaviiformes ## dwc:family dwc:genus gbif:subspecies gbif:variety ## 1 Gaviidae Gavia NA NA ## 2 Gaviidae Gavia NA NA ## 3 Gaviidae Gavia NA NA ## 4 Gaviidae Gavia NA NA ## 5 Gaviidae Gavia NA NA To get the entire list for a particular taxonomic type, use the taxonTypeCode query. Be cautious with this query, the PLANT taxonomic list has several hundred thousand entries. For an example, let’s look up the small mammal taxonomic list, which is one of the shorter ones, and use the verbose=true option to see a more extensive list of taxon data, including many taxon ranks that aren’t populated for these taxa. For space here, we display only the first 10 taxa: mam.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?taxonTypeCode=SMALL_MAMMAL&amp;offset=0&amp;limit=500&amp;verbose=true&quot;) mam.list &lt;- jsonlite::fromJSON(content(mam.req, as=&quot;text&quot;)) mam.list$data[1:10,] ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 SMALL_MAMMAL AMHA AMHA Ammospermophilus harrisii ## 2 SMALL_MAMMAL AMIN AMIN Ammospermophilus interpres ## 3 SMALL_MAMMAL AMLE AMLE Ammospermophilus leucurus ## 4 SMALL_MAMMAL AMLT AMLT Ammospermophilus leucurus tersus ## 5 SMALL_MAMMAL AMNE AMNE Ammospermophilus nelsoni ## 6 SMALL_MAMMAL AMSP AMSP Ammospermophilus sp. ## 7 SMALL_MAMMAL APRN APRN Aplodontia rufa nigra ## 8 SMALL_MAMMAL APRU APRU Aplodontia rufa ## 9 SMALL_MAMMAL ARAL ARAL Arborimus albipes ## 10 SMALL_MAMMAL ARLO ARLO Arborimus longicaudus ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 Audubon and Bachman species Harriss Antelope Squirrel ## 2 Merriam species Texas Antelope Squirrel ## 3 Merriam species Whitetailed Antelope Squirrel ## 4 Goldman subspecies &lt;NA&gt; ## 5 Merriam species Nelsons Antelope Squirrel ## 6 &lt;NA&gt; genus &lt;NA&gt; ## 7 Taylor subspecies &lt;NA&gt; ## 8 Rafinesque species Sewellel ## 9 Merriam species Whitefooted Vole ## 10 True species Red Tree Vole ## taxonProtocolCategory dwc:nameAccordingToID ## 1 opportunistic isbn: 978 0801882210 ## 2 opportunistic isbn: 978 0801882210 ## 3 opportunistic isbn: 978 0801882210 ## 4 opportunistic isbn: 978 0801882210 ## 5 opportunistic isbn: 978 0801882210 ## 6 opportunistic isbn: 978 0801882210 ## 7 non-target isbn: 978 0801882210 ## 8 non-target isbn: 978 0801882210 ## 9 target isbn: 978 0801882210 ## 10 target isbn: 978 0801882210 ## dwc:nameAccordingToTitle ## 1 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 2 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 3 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 4 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 5 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 6 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 7 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 8 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 9 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## 10 Wilson D. E. and D. M. Reeder. 2005. Mammal Species of the World; A Taxonomic and Geographic Reference. Third edition. Johns Hopkins University Press; Baltimore, MD. ## dwc:kingdom gbif:subkingdom gbif:infrakingdom gbif:superdivision ## 1 Animalia NA NA NA ## 2 Animalia NA NA NA ## 3 Animalia NA NA NA ## 4 Animalia NA NA NA ## 5 Animalia NA NA NA ## 6 Animalia NA NA NA ## 7 Animalia NA NA NA ## 8 Animalia NA NA NA ## 9 Animalia NA NA NA ## 10 Animalia NA NA NA ## gbif:division gbif:subdivision gbif:infradivision gbif:parvdivision ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## 7 NA NA NA NA ## 8 NA NA NA NA ## 9 NA NA NA NA ## 10 NA NA NA NA ## gbif:superphylum dwc:phylum gbif:subphylum gbif:infraphylum gbif:superclass ## 1 NA Chordata NA NA NA ## 2 NA Chordata NA NA NA ## 3 NA Chordata NA NA NA ## 4 NA Chordata NA NA NA ## 5 NA Chordata NA NA NA ## 6 NA Chordata NA NA NA ## 7 NA Chordata NA NA NA ## 8 NA Chordata NA NA NA ## 9 NA Chordata NA NA NA ## 10 NA Chordata NA NA NA ## dwc:class gbif:subclass gbif:infraclass gbif:superorder dwc:order ## 1 Mammalia NA NA NA Rodentia ## 2 Mammalia NA NA NA Rodentia ## 3 Mammalia NA NA NA Rodentia ## 4 Mammalia NA NA NA Rodentia ## 5 Mammalia NA NA NA Rodentia ## 6 Mammalia NA NA NA Rodentia ## 7 Mammalia NA NA NA Rodentia ## 8 Mammalia NA NA NA Rodentia ## 9 Mammalia NA NA NA Rodentia ## 10 Mammalia NA NA NA Rodentia ## gbif:suborder gbif:infraorder gbif:section gbif:subsection gbif:superfamily ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA NA NA NA ## 7 NA NA NA NA NA ## 8 NA NA NA NA NA ## 9 NA NA NA NA NA ## 10 NA NA NA NA NA ## dwc:family gbif:subfamily gbif:tribe gbif:subtribe dwc:genus ## 1 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 2 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 3 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 4 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 5 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 6 Sciuridae Xerinae Marmotini NA Ammospermophilus ## 7 Aplodontiidae &lt;NA&gt; &lt;NA&gt; NA Aplodontia ## 8 Aplodontiidae &lt;NA&gt; &lt;NA&gt; NA Aplodontia ## 9 Cricetidae Arvicolinae &lt;NA&gt; NA Arborimus ## 10 Cricetidae Arvicolinae &lt;NA&gt; NA Arborimus ## dwc:subgenus gbif:subspecies gbif:variety gbif:subvariety gbif:form ## 1 &lt;NA&gt; NA NA NA NA ## 2 &lt;NA&gt; NA NA NA NA ## 3 &lt;NA&gt; NA NA NA NA ## 4 &lt;NA&gt; NA NA NA NA ## 5 &lt;NA&gt; NA NA NA NA ## 6 &lt;NA&gt; NA NA NA NA ## 7 &lt;NA&gt; NA NA NA NA ## 8 &lt;NA&gt; NA NA NA NA ## 9 &lt;NA&gt; NA NA NA NA ## 10 &lt;NA&gt; NA NA NA NA ## gbif:subform speciesGroup dwc:specificEpithet dwc:infraspecificEpithet ## 1 NA &lt;NA&gt; harrisii &lt;NA&gt; ## 2 NA &lt;NA&gt; interpres &lt;NA&gt; ## 3 NA &lt;NA&gt; leucurus &lt;NA&gt; ## 4 NA &lt;NA&gt; leucurus tersus ## 5 NA &lt;NA&gt; nelsoni &lt;NA&gt; ## 6 NA &lt;NA&gt; sp. &lt;NA&gt; ## 7 NA &lt;NA&gt; rufa nigra ## 8 NA &lt;NA&gt; rufa &lt;NA&gt; ## 9 NA &lt;NA&gt; albipes &lt;NA&gt; ## 10 NA &lt;NA&gt; longicaudus &lt;NA&gt; To get information about a single taxon, use the scientificname query. This query will not do a ‘fuzzy match’, so you need to query the exact name of the taxon in the NEON taxonomy. Because of this, the query will be most useful when you already have NEON data in hand and are looking for more information about a specific taxon. Querying on scientificname is unlikely to be an efficient way to figure out if NEON recognizes a particular taxon. In addition, scientific names contain spaces, which are not allowed in a URL. The spaces need to be replaced with the URL encoding replacement, %20. For an example, let’s look up the little sand verbena, Abronia minor Standl. Searching for Abronia minor will fail, because the NEON taxonomy for this species includes the authority. The search will also fail with spaces. Search for Abronia%20minor%20Standl., and in this case we can omit offset and limit because we know there can only be a single result: am.req &lt;- GET(&quot;http://data.neonscience.org/api/v0/taxonomy/?scientificname=Abronia%20minor%20Standl.&quot;) am.list &lt;- jsonlite::fromJSON(content(am.req, as=&quot;text&quot;)) am.list$data ## taxonTypeCode taxonID acceptedTaxonID dwc:scientificName ## 1 PLANT ABMI2 ABMI2 Abronia minor Standl. ## dwc:scientificNameAuthorship dwc:taxonRank dwc:vernacularName ## 1 Standl. species little sand verbena ## dwc:nameAccordingToID dwc:kingdom dwc:phylum ## 1 http://plants.usda.gov (accessed 8/25/2014) Plantae Magnoliophyta ## dwc:class dwc:order dwc:family dwc:genus gbif:subspecies ## 1 Magnoliopsida Caryophyllales Nyctaginaceae Abronia NA ## gbif:variety ## 1 NA 2.12 Stacking NEON data At the top of this tutorial, we installed the neonUtilities package. This is a custom R package that stacks the monthly files provided by the NEON data portal into a single continuous file for each type of data table in the download. It currently handles files downloaded from the data portal, but not files pulled from the API. For a guide to using neonUtilities on data downloaded from the portal, look here. 2.13 Intro to NEON Exercises: Written Questions 2.14 Intro to NEON Exercises Part 2 2.14.1 Written Suggested Timing: Complete this exercise before our next class meeting Question 1: How does NEON address ‘dark data’ (Chapter 1)? Question 2: How might or does the NEON project intersect with your current research or future career goals? (1 paragraph) Question 3: Use the map in Chapter 2:Intro to NEON to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: Are there NEON field sites that are in study regions of interest to you? What domains are the sites located in? What NEON field sites do your current research or Capstone Project ideas coincide with? Is the site or sites core or relocatable? Are they terrestrial or aquatic? Are there data available for the NEON field site(s) that you are most interested in? What kind of data are available? Question 4: Consider either your current or future research, or a question you’d like to address durring this course and answer each of the following questions: Which types of NEON data may be more useful to address these questions? What non-NEON data resources could be combined with NEON data to help address your question? What challenges, if any, could you foresee when beginning to work with these data? Question 5: Use the Data Portal tools to investigate the data availability for the field sites you’ve already identified in the previous sections and answer each of the following questions: What types of aquatic or terrestrial data are currently available? Remote sensing data? Of these, what type of data are you most interested in working with for your project during this course? For what time period does the data cover? What format is the downloadable file available in? Where is the metadata to support this data? 2.14.2 NEON Coding Lab Part 2 2.15 NEON Coding Lab Part 2 Suggested Timing: Complete this exercise a few days before your NEON clumination write up Use the answers that you’ve provided above to select a single NEON site. e.g. ONAQ Use the answers that you’ve provided above to select 3 NEON data products from either the TOS, TIS or ARS (AOP) collection methods. Sumarize each product with its NEON identifier, along with a sumarry. e.g.: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD **DP1.10055.001**: Plant phenology observations: phenophase status and insensity of tagged plants. This data product contains the quality-controlled, native sampling resolution data from in-situ observations of plant leaf development and reproductive phenophases, at **D15.ONAQ**. Using the NEON Ulitites package or the API pull in those data along with metadata. Organize your data into data.frames and produce summaries for each of your data: Filter your data based on metadata and quality flags: DP1.10055.001: Plant phenology observations: phenophase status and intensity of tagged plants. This data product contains the quality-controlled, native sampling resolution data from in-situ observations of plant leaf development and reproductive phenophases, at D15.ONAQ. Here I will focus on the phenophase intensity data, which is a measure of how prevalent that particular phenophase is in the sampled plants. Using the NEON Ulitites package or the API pull in those data along with metadata. sitesOfInterest &lt;- c(&quot;ONAQ&quot;) dpid &lt;- as.character(&#39;DP1.10055.001&#39;) #phe data pheDat &lt;- loadByProduct(dpID=&quot;DP1.10055.001&quot;, site = sitesOfInterest, package = &quot;basic&quot;, check.size = FALSE, token=NEON_TOKEN) Organize your data into data.frames and produce summaries for each of your data: #NEON sends the data as a nested list, so I need to undo that # unlist all data frames list2env(pheDat ,.GlobalEnv) summary(phe_perindividualperyear) summary(phe_statusintensity) Filter and format your data based on metadata and quality flags: #remove duplicate records phe_statusintensity &lt;- select(phe_statusintensity, -uid) phe_statusintensity &lt;- distinct(phe_statusintensity) #Format dates (native format is &#39;factor&#39; silly R) phe_statusintensity$date &lt;- as.Date(phe_statusintensity$date, &quot;%Y-%m-%d&quot;) phe_statusintensity$editedDate &lt;- as.Date(phe_statusintensity$editedDate, &quot;%Y-%m-%d&quot;) phe_statusintensity$year &lt;- substr(phe_statusintensity$date, 1, 4) phe_statusintensity$monthDay &lt;- format(phe_statusintensity$date, format=&quot;%m-%d&quot;) Now I want to remove NA values so I can see what’s really going on: phe_statusintensity=phe_statusintensity%&gt;% filter(!is.na(phe_statusintensity$phenophaseIntensity)) Create minimum of 1 plot per data type (minimum of 3 plots total). These will vary based on that data that you’ve chosen. A non-exhastive list of ideas: 1. Your data as a function of height on the tower (FPAR example) 2. A map of the locations where your data is sampled (TOS tree example, bird example) 3. A model based on the data you’re interested in working work (Coding lab 1 example) 4. A timeseries of your data (example below) What is the frequency of the data you decided was of interest? How do the data align to answer a central question? What challenges did you run into when investigating these data? How will you address these challenges and document your code? One to two paragraphs Intro to NEON Culmination Activity What is the temporal frequency of observations in the data you decided was of interest? How do the data align to answer a central question? What challenges did you run into when investigating these data? How will you address these challenges and document your code? One to two paragraphs 2.16 Intro to NEON Culmination Activity Due before we start Chapter 3: USA-NPN &gt;&gt;&gt;&gt;&gt;&gt;&gt; upstream/master Write up a 1-page summary of a project that you might want to explore using NEON data over the duration of this course. Include: the types of NEON (and other data) that you will need to implement this project, including data product id numbers. If in your NEON coding lab part 2 you highlighted challenges to using these data, discuss methods to address those challenges. *e.g. If your site doesn’t yet have a long data recocrd, is it located close to a longer lived site from another network? (LTER, Ameriflux, LTAR etc) One high-level summary graphic including all of your data from the NEON Coding Lab Part 2 Save this summary as you will be refining and adding to your ideas over the course of the semester. "],
["introduction-to-usa-npn-its-data.html", "Chapter 3 Introduction to USA-NPN &amp; its Data 3 USA-NPN Learning Objectives 3 USA-NPN Project Mission &amp; Design: 3 Vision &amp; Mission 3 USA-NPN’s Spatial design: 3 Types of USA-NPN Data: 3 How to Access USA-NPN Data: 3 USA-NPN Written Questions 3.1 Hands on: Accessing USA-NPN Data via rNPN 3.2 Accumulated Growing Degree Day Products 3.3 Extended Spring Indices 3.4 Putting it all together: 3.5 Combine Point and Raster Data 3.6 Live Demo Code with Lee Marsh of USA-NPN 3.7 USA-NPN Coding Lab 3.8 NEON TOS Phenology Data Lecture 3.9 Understanding Observation Biases and Censoring in Citizen Science Data 3.10 So then how can we model censored data? 3.11 Intro to USA-NPN Culmination Activity", " Chapter 3 Introduction to USA-NPN &amp; its Data Estimated Time: 2 hours Course participants: As you review this information, please consider the final course project that you will build upon over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 3 USA-NPN Learning Objectives At the end of this activity, you will be able to: Understand the mission and purpose of the USA-National Phenology Network (USA-NPN) and the nature of the citizen science program from which the data is derived Access all of the various tools &amp; resources that are available to pull USA-NPN geospatial and observational data Effectively use the rNPN package to integrate and analyze NPN data with other similar datasets 3 USA-NPN Project Mission &amp; Design: The USA National Phenology Network (USA-NPN) collects, organizes, and shares phenological data and information to aid decision-making, scientific discovery, and a broader understanding of phenology from a diversity of perspectives. The USA National Phenology Network consists of a National Coordinating Office (NCO), thousands of volunteer observers and many partners, including research scientists, resource managers, educators, and policy-makers. Anyone who participates in Nature’s Notebook or collaborates with NCO staff to advance the science of phenology or to inform decisions is part of the USA-NPN. 3 Vision &amp; Mission USA-NPN’s vision is to provide data and information on the timing of seasonal events in plants and animals to ensure the well-being of humans, ecosystems, and natural resources. To support this and its mission the USA-NPN collects, organizes, and shares phenological data and information to aid decision-making, scientific discovery, and a broader understanding of phenology from a diversity of perspectives. 3 Relevant documents &amp; background information: USA-NPN Strategic Plan USA-NPN Information Sheet: Tracking seasonal changes to support science, natural resource management, and society 2019 USA-NPN Annual Report 3 USA-NPN’s Spatial design: Phenology datasets that are best suited for supporting scientific discovery and decision making are those that consist of observations of multiple life-cycle stages collected at regular intervals at the same locations over multiple years. The USA-NPN collects, stores, and shares high-quality observations of plant and animal phenology at a national scale by engaging observers in Nature’s Notebook, a national-scale, multi-taxon phenology observing program appropriate for both professional and volunteer participants. Because observations are entirely voluntary, the sampling design for observations is opportunistic. The Nature’s Notebook program has been adopted widely; data are collected at over 100 academic institutions, 78 National Ecological Observatory Network (NEON) sites, and by hundreds of researchers to contribute observations to support scientific discovery. The program is also used by tens of thousands of individual observers and members of federal, state, NGO, and private sector organizations as well as K-12 and higher-ed institutions. A unique aspect of Nature’s Notebook is that monitoring can be undertaken by individuals as well as by community or regionally-organized groups referred to as Local Phenology Programs (LPP). Organizations such as nature centers, arboreta, land conservancies, and National Wildlife Refuges use Nature’s Notebook to meet a diversity of outcomes, including asking and answering scientific questions about the impact of environmental change, informing natural resource management and decision-making, and educating and engaging the public. 3 Types of USA-NPN Data: 3 Observational Observational phenology data, consisting of observations made of phenological status on individual organisms, are collected and submitted by professional and citizen scientists, primarily through the USA-NPN plant and animal phenology observing program, Nature’s Notebook. These data are submitted to the USA-NPN and serve as the backbone of all USA-NPN observational data products. Observation protocols consist of status monitoring, in which observers visit a site at regular intervals to evaluate the phenological status of marked individual plants (or patches of plants) and animal species The protocols are described fully in Denny et al. (2014). In this system, phenological status is reported by yes or no answers to a series of questions, for example, “Do you see leaves?” or “Do you see active individuals?”. In addition to “Yes” or “No,” observers may also report “?”, indicating that they are uncertain of the phenophase status. Observers are also invited to document the degree to which the phenophase is expressed on an individual plant, or for animals, at a site. This intensity or abundance question takes the form of a count or percentage - for example, “95–100 percent” of a beech tree’s canopy is full with “Leaves,” or 12 “Active individual” robins are seen. USA-NPN observational data and derivative products are described in USA National Phenology Network Observational Data Documentation (Rosemartin et al. 2018). The three formats in which the USA-NPN observational data are made available include: status and intensity data, individual phenometrics, and site-level phenometrics. Visual comparison of data collected by monitoring phenological events, phenophase status, and phenophase status plus intensity. Event monitoring captures onset of a given phenophase, whereas status monitoring captures onset and duration. Status monitoring with intensity (or abundance) captures onset, duration, and magnitude of a phenophase. Examples are derived from 2012 data submitted in Nature’s Notebook for (a) sugar maple (Acer saccharum) leafing for one individual plant in Maine, and (b) forsythia flowering (Forsythia sp.) for one individual plant in Massachusetts. Each point represents one observation; black points indicate presence of the phenophase while white points indicate absence. (a) illustrates the date on which the first leaf appears (event), the period during which leaves are present (status), and the period and rate at which the canopy fills from 0 to 100 % capacity and then, empties back to 0 with leaf fall (status + intensity, circles and solid line) using estimates of canopy fullness. Also illustrated is the period and rate at which the canopy fills and empties of autumn colored leaves (status + intensity, triangles and dashed line). (b) illustrates the date on which the first open flower appears (event), the periods during which open flowers are present on the plant (status), and an estimate of the number of open flowers on the plant over the periods in which they are present (status + intensity). In both examples, the event point is calculated as the first date of the year where the phenophase was reported as present. Note that in (b) there are two distinct periods of flowering, the second of which would not have been captured using event monitoring alone. (Denny et al., 2014) 3 Status &amp; Intensity Data Status and intensity data consist of presence/absence records for individual phenophases on individual plants or species of animals at a site on a single visit. These records also include intensity and abundance measures. Individual Phenometrics and Site Phenometrics, which are synthesized sequentially from Status and Intensity data, provide estimated phenophase onset and end dates. Individual Phenometrics are derived estimates of phenophase onset and end dates for organisms within a given period of interest. Site Phenometrics are summary metrics of the onset and end date of phenophase activity across multiple individuals of the same species at a site within a given period of interest. Magnitude Phenometrics provide measures of the extent to which a phenophase is expressed across multiple individuals or sites, for a given time interval. These metrics include several approaches for capturing the shape of seasonal activity curves. In Nature’s Notebook, plants are marked and tracked through time, while animals are not, resulting in several key differences between the phenometric data types for plants and for animals. Individual Phenometrics and Site Phenometrics are nearly identical for animals, while for plants the former provide data for individual plants and the latter aggregate data across plants of the same species at a site. Magnitude Phenometrics provide additional information on animals, including correcting abundance values by search time and search area, which is not relevant for plants. As additional observational phenology data types are created by the USA-NPN, they are described at www.usanpn.org/data/new_data_products. USA-NPN Animal Phenological Data by Type from Rosemartin et al.,2018 3 Gridded Raster Data The USA-NPN offers a growing suite of gridded (raster) maps of phenological events, patterns, and trends. These products include historical, real-time, and short-term forecasts and anomalies in the timing of events such as the start of the spring season, and growing degree days. These products are described in the USA National Phenology Network gridded products documentation (Crimmins et al. 2017) Accumulated Growing Degree Days anomaly in 2018 3 Pheno-Forecasts USA-NPN Pheno-Forecasts include real-time maps and short-term forecasts of insect pest activity at management-relevant spatial and temporal resolutions and are based on accumulated temperature thresholds associated with critical life-cycle stages of econmically important pests. Pheno Forecasts indicate, for a specified day, the status of the insect’s target life-cycle stage in real time across the contiguous United States. The maps are available for 12 insect pest species including the invasive emerald ash borer, hemlock woolly adelgid, and gypsy moth. These products are described in “Short-term forecasts of insect phenology inform pest management” (Crimmins et al. 2020) &gt; Example of USA-NPN’s Hemlock Wolly Adelgid Pheno-Forecast for August, 2020. Pheno-Forecasts are also available for an invasive grasses, such as buffelgrass. The buffelgrass Pheno-Forecast is based on known precipitation thresholds for triggering green-up to a level where management actions are most effective. These maps are updated daily and predict green-up one to two weeks in the future. Land Surface Phenology products The USA-NPN offers maps derived from MODIS 6 land surface phenology data. Satellite observations can be linked to in-situ observations to help understand vegetation dynamics across large spatial scales. The MODIS Land Cover Dynamics Product (MLCD) provides global land surface phenology (LSP) data from 2001-present. MLCD serves a wide variety of applications and is currently the only source of operationally produced global LSP data. MLCD data have enabled important discoveries about the role of climate in driving seasonal vegetation changes, helped to create improved maps of land cover, and support ecosystem modeling efforts, among many other important applications. The LSP Climate Indicators (LSP-CI) dataset is a curated collection of the most relevant phenological indicators: a measure of spring and autumn timing and a measure of seasonal productivity. Statistically robust estimates of long-term normals (median and median absolute deviation, MAD), significance-screened trends (Theil-Sen slope magnitude where p&lt;=0.05), and interannual anomalies (in days as well as multiples of MAD) have been computed for these three phenological indicators. The data have been mosaiced across CONUS, reprojected and resampled to a more familiar spatial reference system that matches complementary datasets and delivered in the universally accessible GeoTIFF format. 3 How to Access USA-NPN Data: The USA-NPN makes the data they produce available through a number of different channels and tools. This is partly driven by the format of the data; GIS data, in many ways, can and should be managed differently than observational records, which can more easily be managed in a relational database. However, the need for these different venues is also driven by end-user need. The different tiers of tools makes the data accessible to anyone regardless of their level of technical experience. This is true from the casual observer that would like to use the visualization tool to see how their contributions to citizen science relate to the broader world, all the way to the data scientist that needs simple and standard APIs to integrate USA-NPN data into larger applications and analyses. 3 The USA-NPN Landing page A concise list of all available NPN data sets, tools, products. 3 APIs This is a set of standard web service calls that allows for programmatic access to NPN data independent of any particular programming language. *USA-NPN Web Service API Documentation *USA-NPN Geoserver Documentation *USA-NPN GeoServer API 3 Rnpn package This suite of R functions allows for programmatic access to both gridded and in-situ NPN data sets in an R environment. Full documentation available here: https://usa-npn.github.io/rnpn/ 3 Phenology Observation Portal (for observational data) This tool allows users to download customized datasets of observational data from the National Phenology Database, which includes phenology data collected via the Nature’s Notebook phenology program (2009-present for the United States), and additional integrated datasets, such as historical lilac and honeysuckle data (1955-present). Filters are available to specify dates, regions, species and phenophases of interest. This provides access to all phenometrics, which represents varying degrees of data aggregation. 3 Geospatial Request Builder (for raster data and image files) This tool simplifies the process of accessing NPN gridded data through standard WMS and WCS services. WMS services provide the data as basic graphic images, such as PNGs or TIFFs, whereas WCS services provide the same data in formats accessible to GIS applications. 3 Visualization Tool The Visualization Tool provides an easier way to explore phenology data and maps. The user-friendly interface is intended to allow for searching for comparing general trends and quick-and-easy access to map data/products. 3 USA-NPN Written Questions Suggested timing: Complete before lecture 2 of USA-NPN Hands on Coding Exercises Question 1: How might or does USA-NPN intersect with your current research or future career goals? (1 paragraph) Question 2: Use the USA-NPN visualization tool (www.usanpn.org/data/visualizations) to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: · Are there species, regions, or phenophases of interest to you? · Is there geospatial phenology data that is useful for your work (e.g. Spring Indices or Growing Degree Days)? · What is the timeframe of data you will need to address your research interests? · What is the spatial extent of data you will need? Question 3: Consider either your current or future research, or a question you’d like to address during this course: · What climate data or additional phenological datasets would be valuable to address your research interests? · What challenges, if any, could you foresee when beginning to work with these data? 3.1 Hands on: Accessing USA-NPN Data via rNPN 3.1.1 Introduction The USA National Phenology Network (USA-NPN) is a USGS funded organization that collects phenological observation records from volunteer and professional scientists to better understand the impact of changes in the environment on the timing of species’ life cycles. The USA-NPN also provides a number of raster-based climatological data sets and phenological models. These in-situ observation and geospatial, modeled datasets are available through a number of tools and data services. The USA-NPN R library, “rnpn”, is primarily a data access service for USA-NPN data products, serving as a wrapper to the USA-NPN REST based web services. This guide details how to use the library to access and work with all USA-NPN data types. install.packages(&quot;devtools&quot;) library(&#39;devtools&#39;) devtools::install_github(&quot;usa-npn/rnpn&quot;) library(&#39;rnpn&#39;) 3.1.2 Accessing USA-NPN Observational Data USA-NPN Observational data are collected on the ground by citizen and professional observers following standardized protocols, using the Nature’s Notebook platform. The data are available 2009 to present, and come in four formats or data types: Status &amp; Intensity, Individual Phenometrics, Site Phenometrics and Magnitude Phenometrics. An overview of the differences is provided in the figure below, and each type is detailed in the following sections. For a complete description of the USA-NPN approach and notes for working with each data type see the Open File Report on USA-NPN Observational Data. In Nature’s Notebook, observers register a location, and then at each location they register any number of individual plants or animal species. The expectation is that the user then takes regular observations on each individual/species at a regular interval. Phenological status is reported by yes or no answers to a series of questions, for example, “Do you see leaves?” or “Do you see active individuals?”. In contrast to traditional monitoring of annual “first” events (for example, date of first leaf or first robin), this approach captures absence data when the phenophase is not occurring and repeat events. Each observation is comprised of a series of 1, 0 and -1 values, representing yes/no/uncertain for each possible phenophase for the plant on that date. To explore data in this native “Status and Intensity” format, see the vignette by the same name. A few considerations and functions apply across all USA-NPN Observational data types. 3.1.2.1 Basic format for for Observational data calls The basic format for an observational data call in the rnpn library is: npn_download_[NAME OF DATA TYPE] ( request_source = [NULL] year = [NULL] species_ID = [NULL] ) ‘Request source’ should usually be populated with your full name or the name of the organization you represent. Species_ID is the unique identifier for all the available plants and animals in the USA-NPN database. You can create a table of all available species and their ID numbers: species &lt;- npn_species() Search for a species by common name from the full list: species[species$common_name==&quot;red maple&quot;,] ## # A tibble: 1 x 19 ## species_id common_name genus genus_id genus_common_na… species kingdom ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3 red maple Acer 372 &quot;&quot; rubrum Plantae ## # … with 12 more variables: itis_taxonomic_sn &lt;int&gt;, functional_type &lt;chr&gt;, ## # class_id &lt;int&gt;, class_common_name &lt;chr&gt;, class_name &lt;chr&gt;, order_id &lt;int&gt;, ## # order_common_name &lt;chr&gt;, order_name &lt;chr&gt;, family_id &lt;int&gt;, ## # family_name &lt;chr&gt;, family_common_name &lt;chr&gt;, species_type &lt;list&gt; There are many parameters which can be set beyond these basic ones, depending on the data type, and further detailed in the other vignettes featured in this package. 3.1.2.2 Required Parameters Note that specifying the year(s) of interest is a required parameter. There’s also another required field, “request_source”, which is a user-provided, self-identifying string. This allows the client to provide some information about who is accessing the data. Knowing who is using the data is very helpful for our staff to report the impact of the USA-NPN to the scientific community. The input provided here is entirely honor-based. 3.1.2.3 Find stations at which a species has been observed You can also now look up which stations have a registered plant for a particular species. In the example below, we use the species ID for red maple, which we were able to find through the npn_species() function, to find all stations with that species. npn_stations_with_spp (3) 3.1.3 Status and Intensity Data The Status and Intensity data type is the most direct presentation of the phenology data stored in the NPDb. Each row is comprised of a single record of the status (1/present/“Yes”, 0/absent/“No” or -1/uncertain/“?”) of a single phenophase on an individual plant or species of animal at a site on a single site visit, as well as the estimated intensity or abundance e.g., percent canopy fullness or number of individual robins observed respectively. Retrieving this kind of data using this package is easy, and heavily parameterized. It’s possible to filter data using a number of including year, geographic extent and species. In this example we get all records of bird observations in the New England states from 2018. npn_download_status_data( request_source = &#39;Your Name Here&#39;, years = c(&#39;2018&#39;), states = c(&quot;NY&quot;,&quot;PA&quot;,&quot;VT&quot;,&quot;MA&quot;), functional_types = &#39;Bird&#39; ) ‘states’ is an example of an optional parameter that allows you to filter data based on geographic location. Another example is ‘functional_types’ which allows you to get all available data for a group of similar species (e.g., all birds, shrubs or invasive species). The best place to review all available optional filters is the autogenerated package description. Another important optional parameter is called ‘download_path’. By default requests for data from the services are returned as a data frame that gets stored in memory as a variable. In some cases, it makes more sense to save the data to file for easy and fast retrieval later. The download_path parameter allows you to specify a file path to redirect the output from the service, without having to fuss with pesky I/O operations. Additionally, requests made this way streams the data returned, so if the dataset you’re working with is particularly large, it’s possible to redirect the stream of data to file instead of loading it all into memory which can be useful if your environment doesn’t have enough RAM to store the entire data set at once. npn_download_status_data( request_source = &#39;Your Name Here&#39;, years = c(&#39;2018&#39;), functional_types = &#39;Bird&#39;, additional_fields = &#39;Site_Name&#39;, download_path =&#39;Bird_data_2018_SiteName.csv&#39; ) Using this function to get observational records is the most basic presentation of the data, and is the most robust for doing analysis, but there are a number of other products offered through the data service which provide additional value to data end users, outlined in the next vignettes. 3.1.4 Individual Phenometrics While Status and Intensity data provide a direct and complete look at the observational data, some analyses rely on more synthesized output. Individual Phenometrics are derived from phenophase status data and provide estimates of phenophase onset and end dates based on the first and last “Yes” status values for organisms within a specified season of interest. Each row in this data type is comprised of values that are derived from a string of consecutive “Yes” status reports without an intervening “No” status report for a single phenophase for an individual plant or animal species at a site, called a “series”. For plants, this data type provides information on the onset and end of a phenophase on an individual plant. For animals, it provides information on the onset and end of the presence of an animal species at a site. As animal presence at a site is much more likely to be interrupted by absence than the presence of a phenophase on a plant, Status and Intensity data or Site Phenometrics may be more appropriate for investigating animal phenology. However, we provide animal phenology in the same format as individual plants in this data type to allow users to readily compare individual plant phenology with animal activity. Note that more than one series may exist for a given phenophase in an individual plant or animal species within a single growing season or year, this might occur in the case of leaf bud break followed by a killing frost and second round of breaking leaf buds. It could also occur at group sites where two or more observers are reporting on the same plant on sequential days but are not in agreement on phenophase status. Any call for individual phenometrics requires chronological bounds, usually a calendar year, as determining onset and end depend on knowing what the time frame of interest is. If you query the services directly (without the benefit of this library) it’s possible to specify arbitrary dates, in contrast this library allows you to specify a series of calendar years as input. Here’s an example of how to query the services for individual phenometrics data. Note that the overall structure and parameters are very similar to the call for status data. The biggest difference in this case is that start and end date parameters are now replaced with a ‘years’ array, which predictably takes a set of year values with which to query the service. npn_download_individual_phenometrics( request_source=&#39;Your Name Here&#39;, years=c(2013,2014,2015,2016), species_id=c(210), download_path=&quot;saguaro_data_2013_2016.csv&quot; ) ## using a custom handler function. ## opening curl input connection. ## Found 96 records... ## closing curl input connection. ## using a custom handler function. ## opening curl input connection. ## Found 136 records... ## closing curl input connection. ## using a custom handler function. ## opening curl input connection. ## Found 145 records... ## closing curl input connection. ## using a custom handler function. ## opening curl input connection. ## Found 103 records... ## closing curl input connection. ## NULL In this example, we’re able to see individual saguaro phenology for 2013 through 2016. The results returned from the service is a tabular set of records, giving start and end date by individual saguaro plant. By default, each record contains information about the location, species, phenophase, and start and end dates. Climate data from DayMet can also be acquired with Status &amp; Intensity, Individual Phenometrics and Site Phenometric data types, by setting the climate_data parameter to true. In this example, we are getting colored leaves (phenophase ID is 498) data for birches, using the four birch species IDs, for 2015: npn_download_individual_phenometrics( request_source = &#39;Your Name Here&#39;, years = c(&#39;2015&#39;), species_ids = c(97, 98, 99, 430), phenophase_ids = c(498), climate_data = TRUE, download_path = &#39;Betula_data_2015.csv&#39; ) ## using a custom handler function. ## opening curl input connection. ## Found 146 records... ## closing curl input connection. ## NULL To show what this looks like, we can plot the day of year of the first observation of colored leaves in birches (genus Betula) against summer Tmax. BetulaLeaf &lt;-read.csv( &#39;Betula_data_2015.csv&#39;, header = TRUE, na=-9999, stringsAsFactors = FALSE ) plot( first_yes_doy~tmax_summer, data=BetulaLeaf, ylab=c(&quot;Day of Year&quot;), xlab=c(&quot;Tmax Summer&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21 ) 3.1.5 Site Phenometrics Site Phenometrics, derived from Individual Phenometrics, provide summary metrics of the onset and end date of phenophase activity for a species at a site. Observers are directed to create sites that represent uniform habitat and are no larger than 15 acres. For plants, this metric is calculated as an average for all individuals of a species at the site. For animals, where individuals are not tracked, this metric represents the first and last recorded appearance of the species during the season of interest. For instance, if you asked for red maple leafing data, and there was a site with three red maple trees being observed, then the data would be the average onset date for all three of those red maple trees at that site. Here’s an example of how to query the services for site phenometrics data, for cloned lilacs, breaking leaf buds, 2013. The call is very similar to the call for individual phenometrics data, however, in addition you can supply the quality control filter for the number of days between a yes record and preceding no record (also applies to the last yes and following no), for the observation to be included in the calculations. Typically this is set to 7, 14 or 30, as when downloading data using the USA-NPN Phenology Observation Portal. If you do not set this parameter, it defaults to 30 days. Note that in this example the results are stored in memory, rather than output as a file. LilacLeafPoints2013&lt;-npn_download_site_phenometrics( request_source = &#39;Your Name Here&#39;, years = c(&#39;2013&#39;), num_days_quality_filter = &#39;30&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39; ) In this example we’re able to see the date of the first observation of breaking leaf buds for cloned lilacs, averaged across individuals within sites. If any observation did not have a preceding no record within 30 days it was excluded from the calculations. We can now plot our cloned lilac site phenometric onset data by latitude. plot( mean_first_yes_doy~latitude, data=LilacLeafPoints2013, ylab=c(&quot;Day of Year&quot;), xlab=c(&quot;Latitude&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21, xlim=c(30,55), ylim=c(0,200) ) 3.1.6 Magnitude Phenometrics Magnitude Phenometrics are a suite of eight metrics derived from Status and Intensity data. This data type provides information on the extent to which a phenophase is expressed across multiple individuals or sites, for a given set of sequential time intervals. The data user may select a weekly, bi-weekly, monthly, or custom time interval to summarize the metrics. Two metrics are available for both plants and animals, one metric is available for plants alone and five metrics are available for animals alone (table 1). Three of the five animal metrics correct animal abundance values for observer effort in time and space. Here’s an example of how to query for Magnitude Phenometrics, for the active individuals phenophase for black-capped chickadee data, in 2018. Requirements are similar to other data types. You must additionally specify the time interval by which the data should be summarized. Typically this is weekly, biweekly or monthly, as in the POP and Visualization Tool. The interval chosen in this example is 7 days. npn_download_magnitude_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2018&#39;, period_frequency = &quot;7&quot;, species_ids = &#39;245&#39;, phenophase_ids = &#39;292&#39;, download_path = &#39;MPM_BCC_ActInd_2018.csv&#39; ) In this example we’re able to see all of the magnitude phenometric fields, including proportion_yes_records, and mean_num_animals_in-phase. See the https://pubs.usgs.gov/of/2018/1060/ofr20181060.pdf for full field descriptions. From this dataset we can view the Proportion_Yes_Records (of all the records submitted on this species, what proportion are positive/yes records) by weekly interval: BCC_AI&lt;-read.csv( &#39;MPM_BCC_ActInd_2018.csv&#39;, header = TRUE, na=-9999, stringsAsFactors = FALSE ) plot( BCC_AI$proportion_yes_record~as.Date(BCC_AI$start_date,&quot;%Y-%m-%d&quot;), ylab=c(&quot;Proportion Yes Records&quot;), xlab=c(&quot;Date&quot;), cex=2, cex.axis=1.5, cex.lab=1.5, pch=21, xlim=as.Date(c(&quot;2018-01-01&quot;, &quot;2018-08-01&quot;)), ylim=c(0,1) ) 3.1.7 USA-NPN Geospatial Data USA-NPN provides phenology-relevant climate data in raster format. There are two main suites of products in this category: Accumulated Growing Degree Days and Extended Spring Indices. Accumulated Growing Degree Days and the Extended Spring Indices are both representations of accumulated temperature. As accumulated winter and spring heat drives many spring season phenological events in much of the country, these products can be used to better understand patterns in the current and historical timing of these events across the landscape. For a complete description of the USA-NPN approach and notes for working with each data type see the Open File Report on USA-NPN Gridded Data. Both suites are available as: Current year value, with a 6-day forecast Current year anomaly, with a 6-day forecast Long-term (30 year) average Historical years AGDD - 2016-Prior Year Extended Spring Index - 1880-Prior Year All of these products can be downloaded using the npn_download_geospatial call. There is a number of other products and permutations of the above listed AGDD and Spring Index products, so you can get a complete list of available layers and additional details about them including resolution, extent and the abstract/layer description. layers &lt;- npn_get_layer_details() The following sections describe how to parameterize calls for both AGDD and Spring Index layers. These calls result in raster data sets for the contiguous United States. If you are interested in how many GDDs had accumulated when the red maple in your backyard leafed out, or what day the Spring Index requirements for leaf out were met for your location, you may wish to query the layers for these values, based on location and date. There are two ways to accomplish this, using the npn_get_point_data function which works for all layers and the npn_get_AGDD_point_data function, which only works for AGDD layers and provides a more precise result. npn_get_agdd_point_data( &#39;gdd:agdd_50f&#39;, &#39;38&#39;, &#39;-90&#39;, &#39;2019-02-25&#39; ) This returns a value of 7.64098 GDD, base 50F, for the coordinates 38 north, -90 west on February 25th, 2019. npn_get_point_data( &#39;si-x:lilac_bloom_ncep&#39;, &#39;30&#39;, &#39;-90&#39;, &#39;2019-02-25&#39; ) ## Downloading: 980 B Downloading: 980 B Downloading: 990 B Downloading: 990 B Downloading: 990 B Downloading: 990 B ## No encoding supplied: defaulting to UTF-8. ## [1] 48 This returns a value for lilac bloom of day 48, for the coordinates 30 north, -90 west, as of February 25th, 2019. The above mentioned AGDD products use base temperatures of 32F or 50F and are managed through WCS services. There is also a function to get dynamic AGDD calculations based on a user defined base temperature and a number of other parameters. custom_agdd_raster &lt;- npn_get_custom_agdd_raster( method = &#39;double-sine&#39;, climate_data_source = &#39;NCEP&#39;, temp_unit = &#39;fahrenheit&#39;, start_date = &#39;2019-01-01&#39;, end_date = &#39;2019-05-10&#39;, base_temp = 20, upper_threshold = 90 ) 3.2 Accumulated Growing Degree Day Products Heat accumulation is commonly used as a way of predicting the timing of phenological transitions in plants and animals, including when plants exhibit leaf out, flowering, or fruit ripening, or when insects emerge from dormancy. This is typically expressed as accumulated heat units, either Growing Degree Hours or Growing Degree Days. Growing degree day thresholds have been established for many species, and are commonly used in agriculture, horticulture, and pest management to schedule activities such as harvesting, pesticide treatment, and flower collection. The USA-NPN is currently generating Accumulated Growing Degree Days (AGDD) rasters using a January 1 start date, calculated using simple averaging. These are available calculated using two base temperatures, 32 degrees Fahrenheit (F) and 50 F. When querying certain layers, the underlying data is agnostic about the specific year, and in these cases it makes sense to use the day of year to request data, since that will provide a standardized result, (i.e., April 1st is day 91 in some years and day 92 in others). npn_download_geospatial( &#39;gdd:30yr_avg_agdd_50f&#39;, 95 ) But if you’re looking at a specific year, such as a current year layer, it makes sense to use a specific calendar date (formatted YYYY-MM-DD). It’s also possible to save the raster directly to file instead of loading it into memory. npn_download_geospatial( &#39;gdd:agdd&#39;, &#39;2018-05-05&#39;, output_path=&#39;20180505-agdd-value.tiff&#39; ) In the case of the historic Spring Index layers, however, the product represents the overall outcome for the entire year, so while the year component of the date matters, the month and day do not. In this case, specify January 1 as the month and date. npn_download_geospatial( &quot;si-x:average_bloom_prism&quot;, &quot;1995-01-01&quot; ) The dimension.range value, returned in the npn_get_layer_details() function, clarifies the full set of applicable dates for each layer. Of course, it’s also easy to grab raster data and load it into a visual plot as in this example, showing a map of AGDD base 50 on 2019-06-25: AGDDJun2019&lt;-npn_download_geospatial( &#39;gdd:agdd_50f&#39;, &#39;2019-06-25&#39; ) ## [1] &quot;https://geoserver.usanpn.org/geoserver/wcs?service=WCS&amp;version=2.0.1&amp;request=GetCoverage&amp;format=geotiff&amp;coverageId=gdd:agdd_50f&amp;SUBSET=time(\\&quot;2019-06-25T00:00:00.000Z\\&quot;)&quot; plot( AGDDJun2019, main = &quot;AGDD base 50 on June 25th, 2019&quot; ) An important layer to know of is the 30 year average for AGDD products. This is useful for many comparative analyses. This layer takes DOY as the date input, since it’s the average AGDD value for each day of year for 1981 - 2010. average_30yr &lt;- npn_download_geospatial( &quot;gdd:30yr_avg_agdd&quot;, 45 ) 3.3 Extended Spring Indices The Extended Spring Indices are mathematical models that predict the “start of spring” (timing of first leaf or first bloom) at a particular location. These models were constructed using historical observations of the timing of first leaf and first bloom in a cloned lilac cultivar (Syringa X chinensis ‘Red Rothomagensis’) and two cloned honeysuckle cultivars (Lonicera tatarica L. ‘Arnold Red’ and Lonicera korolkowii Stapf, also known as ‘Zabelii’), which were selected based on the availability of historical observations from across a wide geographic area. Primary inputs to the model are temperature and weather events, beginning January 1 of each year. The model outputs are first leaf and first bloom date for a given location. Data for the Spring Index is available through an enumeration of layers that represents each of the three sub-models as well as an ‘average’ model which represents the aggregation of the three sub-models. These layers are further enumerated by both of the represented phenophases, leaf and bloom. In the example below, first the layer representing only the Arnold Red model for 1987 is retrieved, while the second function call gets the model averaging all three of the models for the same year. npn_download_geospatial( &quot;si-x:arnoldred_bloom_prism&quot;, &quot;1987-01-01&quot; ) average_model &lt;- npn_download_geospatial( &quot;si-x:average_bloom_prism&quot;, &quot;1987-01-01&quot; ) The Spring Indices are also unique in that the algorithm has been run against the BEST climate data set, so historic data going back to 1880 is available. BESTSIxData1905 &lt;- npn_download_geospatial( &#39;si-x:average_bloom_best&#39;, &#39;1905-01-01&#39; ) NAvalue(BESTSIxData1905) &lt;- -9999 plot( BESTSIxData1905, main = &quot;Spring Index, 1905&quot; ) 3.3.1 Other Layers Besides the AGDD and Spring Index layers there are a number of other useful layers available through these services, including daily temperature minimum and maximums and aggregated MODISv6 phenometrics. The daily temperature minimum and maximum values are the underlying climate data used to generate current year AGDD and Spring Index maps. These data are generated by NOAA’s National Centers for Environmental Prediction (NCEP) and are reserved through NPN’s geospatial services. daily_max_20190505 &lt;- npn_download_geospatial( &#39;climate:tmax&#39;, &#39;2019-05-05&#39; ) plot( daily_max_20190505, main = &quot;Daily Temperature Max (C), May 5th, 2019&quot; ) The MODISv6 layers are aggregate values for remote sensing values from the MODISv6 data set, representing a subset of the following phenometrics, aggregated across 2001 - 2017: EVI Area, Mid-Greenup, Mid-Greendown. The available aggregate values for each layer are: median, TSslope, and mean absolute deviation. This example shows the median green up value, as DOY. Note that because this layer has a fixed date, the date parameter is input as a blank string. median_greenup &lt;- npn_download_geospatial( &#39;inca:midgup_median_nad83_02deg&#39;, &#39;&#39; ) plot( median_greenup, main = &quot;MODIS Median Mid-Greenup, 2001 - 2017&quot; ) 3.4 Putting it all together: 3.5 Combine Point and Raster Data Observational and gridded data can be visualized or analyzed together for a variety of purposes. Users may want to identify spatial patterns in the alignment of dogwood bloom and the Spring Index bloom model. The current year’s lilac leaf out observations may be compared to the 30 year average lilac sub-model of the spring index to see how well the model predicts the observations. This example shows several data access calls to assemble observational and gridded data. Option 1: You can add a parameter to an observational data call to additionally get a gridded layer value for each observation location/date. Note that if you don’t specify which sub model of the Spring Index you want, you will get the SI-x Average layers. npn_download_site_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2013&#39;, num_days_quality_filter = &#39;30&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39;, download_path = &#39;cl_lilac_data_2013_SIxLeaf.csv&#39;, six_leaf_layer = TRUE, six_sub_model = &#39;lilac&#39; ) If you want to append raster data other than Spring Index, Leaf values, there’s alternative boolean flags that can be set, including six_bloom_layer for Spring Index, Bloom data, and agdd_layer. Instead of TRUE or FALSE agdd_layer takes 32 or 50 and will correlate each data point with the corresponding AGDD value for the given date using either 32 or 50 base temperature. Option 2: You can create a combined plot of observational data with modeled/raster data. Building on the approach for accessing point data from earlier vignettes describing Individual Phenometrics and getting raster data, we can access and plot these products together. In this example, we will look at how well cloned lilac leaf out observations in 2018 are predicted by the lilac leaf sub model of the Spring Index. 3.5.1 Step 1: Get the data LilacLeaf2018&lt;-npn_download_geospatial( &#39;si-x:lilac_leaf_ncep&#39;, &#39;2018-12-31&#39;, ) LilacLeaf2018Obs &lt;-npn_download_individual_phenometrics( request_source = &#39;Your Name Here&#39;, years = &#39;2018&#39;, species_ids = &#39;35&#39;, phenophase_ids = &#39;373&#39; ) 3.5.2 Step 2: Preparing the data coords &lt;- LilacLeaf2018Obs[ , c(&quot;longitude&quot;, &quot;latitude&quot;)] data &lt;- as.data.frame(LilacLeaf2018Obs$first_yes_doy) crs &lt;- CRS(&quot;+proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) LL_spdf &lt;- SpatialPointsDataFrame( coords = coords, data = data, proj4string = crs ) 3.5.3 Step 3: Define style options and create graph my.palette &lt;- brewer.pal(n=9,name=&quot;OrRd&quot;) plot( LilacLeaf2018, col = my.palette, main=&quot;2018 Observed and Predicted Lilac Leaf Out&quot; ) plot( LL_spdf, main=&quot;Lilac Obs&quot;, pch = 21, bg = my.palette, col = &#39;black&#39;, xlim=c(-125.0208,-66.47917), ylim=c(24.0625 ,49.9375), add = TRUE ) legend( &quot;bottomright&quot;, legend=c(&quot;Cloned Lilac Leaf Out Observations&quot;), pch = 21, bg = &#39;white&#39;, col = &#39;black&#39;, bty=&quot;n&quot;, cex=.8 ) 3.6 Live Demo Code with Lee Marsh of USA-NPN 3.6.1 Basic Utility Functions species &lt;- npn_species() phenophases &lt;- npn_phenophases() layer_details &lt;- npn_get_layer_details() quick_get_phenophase &lt;- function(species_id, date,phenophase_name){ phenophases&lt;-npn_phenophases_by_species(c(species_id),date=date) phenophases_species &lt;- phenophases[phenophases$species_id==species_id]$phenophases[[1]] phenophases_species[phenophases_species$phenophase_name==phenophase_name,]$phenophase_id } quick_get_species &lt;- function(species_name){ species[species$common_name==species_name,]$species_id } 3.6.2 Download Observational Data white_oak_id &lt;- quick_get_species(&quot;white oak&quot;) fruits_id&lt;- quick_get_phenophase(white_oak_id,&quot;2017-05-15&quot;,&quot;Fruits&quot;) # Raw data download s2017_white_oak_raw &lt;- npn_download_status_data( request_source = &quot;R Demo&quot;, years = c(2017), species_ids = c(white_oak_id), phenophase_ids = c(fruits_id) ) 3.6.3 Magnitude Data m2017_white_oak_magnitude &lt;- npn_download_magnitude_phenometrics( request_source = &quot;INF550&quot;, years = c(2017), species_ids = c(white_oak_id), phenophase_ids = c(fruits_id), period_frequency = &quot;14&quot; ) datasets &lt;- npn_datasets() # NEON data, file download, additional fields npn_download_status_data( request_source = &quot;R Demo&quot;, years = c(2018:2020), states = c(&quot;CO&quot;), dataset_ids = c(16), additional_fields = c(&quot;Site_Name&quot;), download_path = &quot;NEON_CO_Data_2018-2010.csv&quot; ) 3.6.4 Downloading Geospatial Data SIXBloom2018 &lt;- npn_download_geospatial( &#39;si-x:average_bloom_ncep&#39;, &#39;2018-12-31&#39; ) npn_download_geospatial( &#39;gdd:agdd&#39;, &#39;2018-04-15&#39;, output_path = &quot;20180415-32-agdd.tiff&quot; ) my_point &lt;- npn_get_point_data(&quot;gdd:agdd_50f&quot;, 33.649, -111.861, &quot;2017-05-15&quot;) 3.6.5 Putting it together dogwood_id &lt;- quick_get_species(&quot;flowering dogwood&quot;) dogwood_flowering_id &lt;- quick_get_phenophase(dogwood_id,&quot;2018-05-05&quot;,&quot;Flowers or flower buds&quot;) dogwood_data &lt;- npn_download_site_phenometrics( request_source = &#39;Demo&#39;, years = &#39;2018&#39;, species_ids = dogwood_id, phenophase_ids = dogwood_flowering_id, six_leaf_layer = TRUE, agdd=32 ) 3.6.6 Other Data Sources, e.g. Daymet, MODIS add_fields &lt;- npn_download_status_data( request_source = &quot;INF550&quot;, years = c(2014), species_id = c(4), additional_fields = c(&quot;tmaxf&quot;,&quot;Greenup_0&quot;,&quot;MidGreenup_0&quot;) ) 3.7 USA-NPN Coding Lab library(rnpn) library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.2 library(neonUtilities) ## Warning: package &#39;neonUtilities&#39; was built under R version 3.6.2 library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union source(&#39;/Users/kdw223/Research/katharynduffy.github.io/neon_token_source.R&#39;) For the purposes of this exercise we will be focusing on two NEON sites: HARV and CPER. Save these two sites into your workplace so that you can feed them into functions and packages. Define AGGD and write the equation using LaTeX. What is an appropriate time interval over which we should calculate AGGD? This will be relevant for following questions Use the neonUtilities package to pull plant phenology observations (DP1.10055.001). We will work with the statusintensity data: Hints: #TOS Phenology Data sitesOfInterest &lt;- c(&quot;HARV&quot;) dpid &lt;- as.character(&#39;DP1.10055.001&#39;) #phe data pheDat &lt;- loadByProduct(dpID=&quot;DP1.10055.001&quot;, site = sitesOfInterest, package = &quot;basic&quot;, check.size = FALSE, token=NEON_TOKEN) #NEON sends the data as a nested list, so I need to undo that # unlist all data frames list2env(pheDat ,.GlobalEnv) summary(phe_perindividualperyear) summary(phe_statusintensity) #remove duplicate records phe_statusintensity &lt;- select(phe_statusintensity, -uid) phe_statusintensity &lt;- distinct(phe_statusintensity) library(lubridate) ## Warning: package &#39;lubridate&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union #Format dates phe_statusintensity$date &lt;- as.Date(phe_statusintensity$date, &quot;%Y-%m-%d&quot;) ## Warning in as.POSIXlt.POSIXct(x, tz = tz): unknown timezone &#39;%Y-%m-%d&#39; phe_statusintensity$editedDate &lt;- as.Date(phe_statusintensity$editedDate, &quot;%Y-%m-%d&quot;) ## Warning in as.POSIXlt.POSIXct(x, tz = tz): unknown timezone &#39;%Y-%m-%d&#39; phe_statusintensity$year &lt;- as.numeric(substr(phe_statusintensity$date, 1, 4)) phe_statusintensity$month &lt;- as.numeric(format(phe_statusintensity$date, format=&quot;%m&quot;)) In your phe_statusintensity data.frame pick a phenophase name of interest: unique(phe_statusintensity$phenophaseName) ## [1] &quot;Falling leaves&quot; &quot;Colored leaves&quot; &quot;Leaves&quot; ## [4] &quot;Increasing leaf size&quot; &quot;Open flowers&quot; &quot;Breaking leaf buds&quot; ## [7] &quot;Initial growth&quot; And select a single taxon: unique(phe_perindividual$taxonID) ## [1] &quot;QURU&quot; &quot;ACRU&quot; &quot;ARNU2&quot; Now create a new, filtered dataframe only including those observations and print a summary. You’ll also want to filter for typical things like NA values, and think about how you’ll work with data that comes in factors or strings. Are there ways you could extract numerical values for plotting? Could you count data? Summarize your strategy. Using dpid DP1.00002.001 Single Aspirated Air Temperature calculate AGGD based on NEON tower data over the time period you decidided upon in question 1. To save you time and frustration I’ve placed some mostly complete example code for one height on the tower just for Harvard. You will need to determine which height you think it best and conmplete these calculations for both sites. You will also need to consder things like filtering your temperature data for quality flags, and converting from GMT (Greenwich Mean Time) to your location’s time: ##load libraries #library(tidyverse) library(neonUtilities) #install.packages(&#39;mgcv&#39;) library(mgcv) dpid &lt;- as.character(&#39;DP1.00002.001&#39;) ##single aspirated air temperature tempDat &lt;- loadByProduct(dpID=dpid, site = &quot;HARV&quot;, startdate = &quot;2017-01&quot;, enddate=&quot;2017-12&quot;, avg=30, package = &quot;basic&quot;, check.size = FALSE) df &lt;- tempDat$SAAT_30min # GDD typically reported in F # convert df temps df$meanTempF=df$tempSingleMean*1.8+32 #pull date value from dateTime df$date &lt;- substr(df$endDateTime, 1, 10) Group data and summarize values Here, I will group the 30-minute temperature averages by data (to get daily values) You will want to consider which vertical position is most appropriate to use for your analysis. You can view the sensor position data in the sensor_positions table downloaded above, where HOR.VER are the horizontal and vertical position indices (separated by a period),and zOffset is in meters above the ground: select(tempDat$sensor_positions_00002, c(HOR.VER, zOffset)) you can also view all of the sensor position info with the following line: head(tempDat$sensor_positions_00002) For example, the lowest position sensor (verticalPosition == 010) may be most appropriate for comparison with the phenological state of very short plants, while the highest verticalPosition may be better for comparison with canopy trees. Here I’ll select level 1 for demonstration day_temp &lt;- df%&gt;% filter(verticalPosition==&quot;010&quot;)%&gt;% group_by(siteID, date)%&gt;% mutate(dayMaxTemp=max(meanTempF), dayMinTemp=min(meanTempF), dayMeanTemp=mean(meanTempF))%&gt;% select(siteID, date, dayMaxTemp, dayMinTemp, dayMeanTemp)%&gt;% distinct() ##alternative, simplified mean, consistent with many GDD calculations ### does accumulation differ for true mean vs. simplified mean? day_temp$mean2 &lt;- (day_temp$dayMinTemp + day_temp$dayMaxTemp)/2 Caluculate daily GDD for a true mean 50 degrees F is a common base temperature used to calculate plant specific GDD. When might you select a different base temp? How might you want to deal with different ‘means’ of temperature? A couple of options below: day_temp$GDD1 &lt;- ifelse(day_temp$dayMeanTemp-50 &lt; 0, 0, round(day_temp$dayMeanTemp-50, 0)) day_temp$GDD2 &lt;- ifelse(day_temp$mean2-50 &lt; 0, 0, round(day_temp$mean2-50, 0)) day_temp$GDD3 &lt;- ifelse(day_temp$dayMeanTemp-50 &lt; 0, 0, round(day_temp$mean2-50, 0)) # define year day_temp$year &lt;- substr(day_temp$date, 1, 4) #function to add daily GDD values sumr.2 &lt;- function(x) { sapply(1:length(x), function(i) sum(x[1:i])) } #calculate Accumlated GDD day_temp$AGDD3 &lt;- sumr.2(x=day_temp$GDD3) day_temp$AGDD2 &lt;- sumr.2(x=day_temp$GDD2) day_temp$AGDD1 &lt;- sumr.2(x=day_temp$GDD1) day_temp &lt;- ungroup(day_temp) library(plotly) p = plot_ly() %&gt;% add_trace( x= ~day_temp$date, y = ~ day_temp$AGDD1, type= &#39;scatter&#39;, mode = &quot;lines&quot;, line = list(width = 1, color = &quot;rgb(120,120,120)&quot;), name = &quot;Calculated Mean Temp&quot;, showlegend = TRUE, opacity=.5 )%&gt;% add_trace( data = day_temp, x = ~ date, y = ~ AGDD2, name= &#39;Simplified Mean Temp&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;lines&#39;, line = list(width = 1), opacity=.5)%&gt;% add_trace( data = day_temp, x = ~ date, y = ~ AGDD3, name= &#39;Filtered Using Both&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;lines&#39;, line = list(width = 1), opacity=.2) p Plot your calculated AGGD and comment on your calculations. Do you need to revise your time horizon or sensor height? Now we’re going to build a model to see how AGGD impacts phenological status. But Wait. Is phenology all driven by temperature? Should you consider any other variables? What about AGGD and just plain temperature? Also, we have one very temperate site, and another that is a semi-arid grassland. Should water availability of any sort be considered? Any other variables or data? Create a GAM (Generalized Additive Model) for your phenological data including any variables you think might be relevant. One of the bonuses of a GAM is that it will tell you which variables are relevant and which aren’t so you can iterate a bit on your model and revise it. You might want to test a few positions on your asipirated air temperature, or a few other additional variables. Your selection is up to you, but you must document and justify your decision. Hints: library(mgcv) model &lt;- mgcv::gam(phenological_status_you_picked ~ AGGD + s(temp or maybe precip) + s(doy), data=your_data) mgcv::summary.gam(model) and plot your models for each site: Hint: mgcv::plot.gam(model, pages=1 ) Now that we have a model for NEON data, let’s use the rnpn package to see how adding additional data could improve our fit. Use the taxonID that you selected at each NEON tower, and feed that to the rnpn package to grab observational data and increase your number of observations. Hints: Feeding a state or other region will make the data more congruent You’ll likely need to either request the phenophase that you selected from NEON, or filter again. It might make your life easier to request the NEON data from rnpn as they host it as well. Pull AGGD from USA-NPN based on the observations you just pulled. Combine your NEON and USA-NPN data into the same data.frame and re-fit your GAM. Summarize your new model Plot your new model Comment on your new model: was it improved? If so how? 3.8 NEON TOS Phenology Data Lecture Please watch the recorded lecture with Dr. Katie Jones, lead plant ecologist with NEON Battelle 3.9 Understanding Observation Biases and Censoring in Citizen Science Data The following information was adapted from Zachmann et al. (in prep) and is not to be distributed or used beyond this course. One significant barrier to advancing our understanding and utilization of phenology information is wrapped up in the ability of existing models to represent drivers of phenology and phenological shifts appropriately. The specific timing of phenological events, such as first leaf or first flower in plants, is affected by multiple, potentially interacting climatic and geophysical factors. For example, soil moisture supply, atmospheric demand for water, frost events, day length, and terrain have all been shown to influence species’ phenology. Many of these factors vary over time as well as space. Additionally, some of their effects may themselves vary over time as a result of interactions among controls on phenology. For instance: Precipitation and temperature likely interact to initiate phenological events; Precipitation triggers a state change only after enough heat has accumulated at a given site over the course of the growing season. Phenological predictions, specifically, consist of estimates of the anticipated onset dates of key events: bud break and the arrival of spring, for example, or plant senescence in the fall. In order to track anomalies in real-time, we need forecasts of phenology. Further, we need forecasts of long-term shifts in phenology as a result of climate change. However, making any such predictions requires both reasonable and skillful models of phenology. By “reasonable” we refer to models that are appropriate for the data – including any “nonignorable” aspects of the sampling design, Gelman et al. (2013) – and the underlying ecological process, while by “skillful” we mean that the model has adequate predictive performance. 3.9.1 Many existing modeling approaches fail in one (or both) regards. Much of the existing work relies on models that implicitly or explicitly ignore key characteristics of phenological data and the ecological process of phenology. For example, many studies ignore censoring and other aspects of the sampling design underlying observations, including the location and nestedness of sampling units, or incompletely account for the dynamic nature of phenological drivers. Efforts that fail to account for these features of the data are liable to misrepresent the ecological process, thereby making predictions that are demonstrably wrong and potentially biased. Though it can be argued that some information is better than no information, scientific understanding and consequential management actions require the right information. In other words, it is better to be ignorant than misled. 3.9.2 Censoring Time-to-event data are rarely recorded accurately, in which case observations are said to be censored. There are several types of censoring we must consider. In the context of plant phenology, perhaps the most common is interval censoring, which occurs when a phenological event transpires between visits to a site. Illustration of the types of censoring that are relevant to analyses of phenological data. Although shown extending from zero to positive infinity, the x-axis for most phenological events spans the length of a growing season and starts on Jan. 1. The timing, ti, of the event (e.g., first leaf), in relation to interval boundaries li and ri, is indicated by the position of the leaf along the x- axis, while actual site visits are represented by the presence of the person. Note that - in the case of right-censoring – the event need not occur, hence the question mark. Phenological observations are almost never observed exactly, as depicted on the bottom-most timeline for uncensored observations. (Zachmann et al, in prep) Taking first leaf as an example, a deciduous shrub at site i might be completely leafless at the time of the first visit, li, but could have several emerging leaves at time of the second visit ri. The size of the interval depends on how frequently the site is visited, and may span days or weeks. All that is known is that the event of interest happened in (li,ri), rather than an exact time. Censoring is often ignored in analysis, or it is addressed, potentially inappropriately, using imputation techniques. When the intervals are small relative to the full timespan of interest, the bias introduced by interval censored observations may be small enough to be safely ignored. Either such assumptions must be tested or statistical methods that account for this feature of the sampling design must be used. 3.10 So then how can we model censored data? An extension of the ontology developed above, this figure shows a simulated set of observations for eight sites chosen at random from the N = 100 sites used to develop the concept figures. Three types of censoring can be seen in this figure. Not all phenological observations are created equally, nor systematically! (Zachmann et al, in prep) For logistical or other reasons, record-keeping at some sites starts earlier than others. Additionally, some of the observers make visits to sites more frequently than others. Analyses that do not account for this sampling process explicitly are somewhat impaired — they must filter the observations for known events (times represented by visits shown in purple) and must discard others (e.g., the data for sites 30 and 77), or must impute times using single-point imputation techniques, which have been shown to lead to invalid inference. The histogram seen in the upper panel is constructed on the basis of the true, unobserved leaf out dates. Metaphorically speaking, it is the pile of leaves that would be formed if each leaf (in addition to the other 92 sites not shown) were to fall straight to the bottom of the plot. The histogram on the bottom (in purple) is created using actual first leaf observations (the event dates corresponding to the purple “botanists” above). (Zachmann et al, in prep) Here the authors seek to model the latent distribution of event times (the green line overlaying the histogram in the upper panel) that gives rise to the observations seen in the lower panel. As they begin to introduce both greater complexity, realism, and utility, they can model the moments (or parameters) of the latent distribution using well-established deterministic models involving climatic and other phenological forcings. 3.11 Intro to USA-NPN Culmination Activity Note: I fully realize that phenology data may not be relevant to all of you. Two suggestions: 1. Be creative, example: Say you work with ground water hydrology, how could leaves on trees perhaps be relevant to groundwater recharge rate? Might there be a lag? Etc etc. 2. I will fully accept alternate citizen-science-based datasets and project proposals based on those. The challenge of taking this option is that we have not covered that data. Write up a 1-page derived data product or research pipeline proposal summary of a project that you might want to explore using USA-NPN data. Include the types of USA-NPN (and other data) that you will need to implement this project. Save this summary as you will be refining and adding to your ideas over the course of the semester. Sugestions: Tables or bullet lists of specific data products An overarching high-quality figure to show how these data align One paragaph summarizing how this data or analysis is useful to you and/or the infrastructure. "],
["phenocam-digital-repeat-photography-networks-methods.html", "Chapter 4 PhenoCam: Digital Repeat Photography Networks &amp; Methods 4.1 Digital Repeat Photography Networks Learning Objectives 4.2 The PhenoCam Network Mission &amp; Design 4.3 PhenoCam’s Spatial design: 4.4 Digital Repeat Photography Written Questions 4.5 Introduction to Digital Repeat Photography Methods 4.6 Pulling Data via the phenocamapi Package 4.7 Exploring PhenoCam metadata 4.8 Download midday images 4.9 Detecting Foggy Images using the ‘hazer’ R Package 4.10 Extracting Timeseries from Images using the xROI R Package 4.11 Documentation and Citation 4.12 Hands on: Digital Repeat Photography Computational 4.13 Digital Repeat Photography Coding Lab 4.14 Intro to PhenoCam Culmination Activity", " Chapter 4 PhenoCam: Digital Repeat Photography Networks &amp; Methods Estimated Time: 4 hours Course participants: As you review this information, please consider the final course project that you will work on at the over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 4.1 Digital Repeat Photography Networks Learning Objectives At the end of this activity, you will be able to: Understand how phenology is a large driver of biosphere-atmosphere interactions, and is a sensitive indicator of climate change. Summarize data which can be pulled out of digital repeat imagery Describe basic processing of digital repeat photography Perform basic image processing. Estimate image haziness as an indication of fog, cloud or other natural or artificial factors using the hazerR package. Define and use a Region of Interest, or ROI, for digitial repeat photography methods. Handle Field-of-View (FOV) shifts in digital repeat photography. Extract timeseries data from a stack of images using color-based metrics. 4.2 The PhenoCam Network Mission &amp; Design Since its inception, the objective of the PhenoCam network has been to serve as a repository for phenologically-relevant, digital, repeat (time-lapse) imagery, and to make that imagery, and derived data products, freely available to a wide array of third-party data end-users, including researchers, educators, and the general public. “Thus, imagery from the PhenoCam archive is made publicly available, without restriction, and we encourage you to download imagery and datasets for use in your own research and teaching. In return, we ask that you acknowledge the source of the data and imagery, and abide by the terms of the Creative Commons CC BY Attribution License.” 4.2.1 Relevant documents &amp; background information: The PhenoCam Gallery A map of PhenoCam locations A site table of all PhenoCams PhenoCam metadata 4.3 PhenoCam’s Spatial design: The PhenoCam Network is: A voluntary ‘opt in’ network with collaborators who are varied, including: Individual research labs or field sites in North America, Europe, Asia, Africa, South America Individuals who think it would be cool to be part of a network like this The project is largely run by the Richardson Lab at NAU, with support from key collaborators at the University of New Hampshire who provide server and website management. Anyone can buy a relatively inexpensive camera, run some simple scripts to correct for things like auto-white balance (which we will cover later), and patch in to the netowrk. PhenoCam then retrieves, archieves and processess imagery for distribution. 4.3.1 PhenoCam as a Near Surface Remote Sensing Technique 6 years of PhenoCam imagery at Harvard Forest PhenoCam uses imagery from networked digital cameras for continuous monitoring of plant canopies Images are recorded approximately every 30 minutes (every 15 minutes for NEON), sunrise to sunset, 365 days a year The scale of observations is comparable to that of tower-based land-atmosphere flux measurements PhenoCams provide a direct link between what is happening on the ground and what is seen by satellites PhenoCams cover a wide array of: Plant Funtional Types (PFTs) Ecoregions Spatial distribution of PhenoCam data across ecological regions of North America. Background map illustrates USA Environmental Protection Agency Level I Ecoregions. Data counts have been aggregated to a spatial resolution of 4°, and the size of each circle corresponds to the number of site-years of data in the 4×4° grid cell. Sites in Hawaii, Puerto Rico, Central and South America, Europe, Asia and Africa (total of 88 site years) are not shown. (Seyednasrollah et al., 2019) Co-Located Networks Flux towers NEON LTER/LTAR Ambient vs. Experimental set ups SEGA sites SPRUCE Experiment 4.3.2 How PhenoCams Pull Data 4.3.3 Leveraging camera near-infrared (NIR) capabilities Petach et al., Agricultural &amp; Forest Meteorology 2014 CMOS sensor is sensitive to &gt; 700 nm A software-controlled filter enables sequential VIS (RGB color) and VIS+NIR (monochrome) images Potential applications: false color images “camera NDVI” as alternative to GCC 4.4 Digital Repeat Photography Written Questions Suggested completion: Before Digital Repeat Photography methods (day 2 PhenoCam) Question 1: What do you see as the value of the images themselves? The 1 or 3-day products, the transition dates? How could they be used for different applications? Question 2: Why does PhenoCam take photos every 15-30 minutes, but summarize to 1 or 3-day products? Question 3: Why does canopy color coordiante with photosynthesis in many ecosystems? Name an example of a situation where it wouldn’t and explain why. Question 4: Why are there sometimes multiple Regions of Interest (ROIs) for a PhenoCam? Question 5: How might or does the PhenoCam project intersect with your current research or future career goals? (1 paragraph) Question 6: Use the map on the PhenoCam website to answer the following questions. Consider the research question that you may explore as your final semester project or a current project that you are working on and answer each of the following questions: Are there PhenoCams that are in study regions of interest to you? Which PhenoCam sites does your current research or final project ideas coincide with? Are they connected to other networks (e.g. LTAR, NEON, Fluxnet)? What is the data record length for the sites you’re interested in? Question 7: Consider either your current or future research, or a question you’d like to address durring this course: Which types of PhenoCam data may be more useful to address these questions? What non-PhenoCam data resources could be combined to help address your question? What challenges, if any, could you foresee when beginning to work with these data? 4.5 Introduction to Digital Repeat Photography Methods The concept of repeat photography for studying environmental has been introduced to scientists long time ago (See Stephens et al., 1987). But in the past decade the idea has gained much popularity for monitoring environmental change (e.g., Sonnentag et al., 2012). One of the main applications of digital repeat photography is studying vegetation phenology for a diverse range of ecosystems and biomes (Richardson et al., 2019). The methods has also shown great applicability in other fields such as: assessing the seasonality of gross primary production, salt marsh restoration, monitoring tidal wetlands, investigating growth in croplands, and evaluating phenological data products derived from satellite remote sensing. Obtaining quantitative data from digital repeat photography images is usually performed by defining appropriate region of interest, also know as ROI’s, and for the red (R), green (G) and blue (B) color channels, calculating pixel value (intensity) statistics across the pixels within each ROI. ROI boundaries are delineated by mask files which define which pixels are included and which are excluded from these calculations. The masks are then used to extract color-based time series from a stack of images. Following the time-series, statistical metrics are used to obtain 1-day and 3-day summary time series. From the summary product time series, phenological transition dates corresponding to the start and the end of green-up and green-down phenological phases are calculated. In this chapter we explain this process by starting from general image processing tools and then to phenocam-based software applications. For more details about digital repeat photogrpahy you can check out the following publications: - Seyednarollah, et al. 2019, “Tracking vegetation phenology across diverse biomes using Version 2.0 of the PhenoCam Dataset”. - Seyednarollah, et al. 2019, “Data extraction from digital repeat photography using xROI: An interactive framework to facilitate the process”. 4.6 Pulling Data via the phenocamapi Package The phenocamapi R package is developed to simplify interacting with the PhenoCam network dataset and perform data wrangling steps on PhenoCam sites’ data and metadata. This tutorial will show you the basic commands for accessing PhenoCam data through the PhenoCam API. The phenocampapi R package is developed and maintained by Bijan Seyednarollah. The most recent release is available on GitHub (PhenocamAPI). Additional vignettes can be found on how to merge external time-series (e.g. Flux data) with the PhenoCam time-series. We begin with several useful skills and tools for extracting PhenoCam data directly from the server: Exploring the PhenoCam metadata Filtering the dataset by site attributes Extracting the list of midday images Downloading midday images for a given time range 4.7 Exploring PhenoCam metadata Each PhenoCam site has specific metadata including but not limited to how a site is set up and where it is located, what vegetation type is visible from the camera, and its climate regime. Each PhenoCam may have zero to several Regions of Interest (ROIs) per vegetation type. The phenocamapi package is an interface to interact with the PhenoCam server to extract those data and process them in an R environment. To explore the PhenoCam data, we’ll use several packages for this tutorial. library(data.table) library(phenocamapi) ## Loading required package: rjson ## Loading required package: RCurl ## Warning: package &#39;RCurl&#39; was built under R version 3.6.2 library(lubridate) ## Warning: package &#39;lubridate&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(jpeg) We can obtain an up-to-date data.frame of the metadata of the entire PhenoCam network using the get_phenos() function. The returning value would be a data.table in order to simplify further data exploration. # obtaining the phenocam site metadata from the server as data.table phenos &lt;- get_phenos() # checking out the first few sites head(phenos$site) ## [1] &quot;aafcottawacfiaf14e&quot; &quot;aafcottawacfiaf14w&quot; &quot;acadia&quot; ## [4] &quot;aguatibiaeast&quot; &quot;aguatibianorth&quot; &quot;ahwahnee&quot; # checking out the columns colnames(phenos) ## [1] &quot;site&quot; &quot;lat&quot; ## [3] &quot;lon&quot; &quot;elev&quot; ## [5] &quot;active&quot; &quot;utc_offset&quot; ## [7] &quot;date_first&quot; &quot;date_last&quot; ## [9] &quot;infrared&quot; &quot;contact1&quot; ## [11] &quot;contact2&quot; &quot;site_description&quot; ## [13] &quot;site_type&quot; &quot;group&quot; ## [15] &quot;camera_description&quot; &quot;camera_orientation&quot; ## [17] &quot;flux_data&quot; &quot;flux_networks&quot; ## [19] &quot;flux_sitenames&quot; &quot;dominant_species&quot; ## [21] &quot;primary_veg_type&quot; &quot;secondary_veg_type&quot; ## [23] &quot;site_meteorology&quot; &quot;MAT_site&quot; ## [25] &quot;MAP_site&quot; &quot;MAT_daymet&quot; ## [27] &quot;MAP_daymet&quot; &quot;MAT_worldclim&quot; ## [29] &quot;MAP_worldclim&quot; &quot;koeppen_geiger&quot; ## [31] &quot;ecoregion&quot; &quot;landcover_igbp&quot; ## [33] &quot;dataset_version1&quot; &quot;site_acknowledgements&quot; ## [35] &quot;modified&quot; &quot;flux_networks_name&quot; ## [37] &quot;flux_networks_url&quot; &quot;flux_networks_description&quot; Now we have a better idea of the types of metadata that are available for the Phenocams. 4.7.1 Remove null values We may want to explore some of the patterns in the metadata before we jump into specific locations. Let’s look at Mean Annual Precipitation (MAP) and Mean Annual Temperature (MAT) across the different field site and classify those by the primary vegetation type (primary_veg_type) for each site. We can find out what the abbreviations for the vegetation types mean from the following table: Abbreviation Description AG agriculture | DB deciduous broadleaf | DN deciduous needleleaf | EB evergreen broadleaf | EN evergreen needleleaf | GR grassland | MX mixed vegetation (generally EN/DN, DB/EN, or DB/EB) | SH shrubs | TN tundra (includes sedges, lichens, mosses, etc.) | WT wetland | NV non-vegetated | RF reference panel | XX unspecified | To do this we’d first want to remove the sites where there is not data and then plot the data. # removing the sites with unkown MAT and MAP values phenos &lt;- phenos[!((MAT_worldclim == -9999)|(MAP_worldclim == -9999))] # extracting the PhenoCam climate space based on the WorldClim dataset # and plotting the sites across the climate space different vegetation type as different symbols and colors phenos[primary_veg_type==&#39;DB&#39;, plot(MAT_worldclim, MAP_worldclim, pch = 19, col = &#39;green&#39;, xlim = c(-5, 27), ylim = c(0, 4000))] ## NULL phenos[primary_veg_type==&#39;DN&#39;, points(MAT_worldclim, MAP_worldclim, pch = 1, col = &#39;darkgreen&#39;)] ## NULL phenos[primary_veg_type==&#39;EN&#39;, points(MAT_worldclim, MAP_worldclim, pch = 17, col = &#39;brown&#39;)] ## NULL phenos[primary_veg_type==&#39;EB&#39;, points(MAT_worldclim, MAP_worldclim, pch = 25, col = &#39;orange&#39;)] ## NULL phenos[primary_veg_type==&#39;AG&#39;, points(MAT_worldclim, MAP_worldclim, pch = 12, col = &#39;yellow&#39;)] ## NULL phenos[primary_veg_type==&#39;SH&#39;, points(MAT_worldclim, MAP_worldclim, pch = 23, col = &#39;red&#39;)] ## NULL legend(&#39;topleft&#39;, legend = c(&#39;DB&#39;,&#39;DN&#39;, &#39;EN&#39;,&#39;EB&#39;,&#39;AG&#39;, &#39;SH&#39;), pch = c(19, 1, 17, 25, 12, 23), col = c(&#39;green&#39;, &#39;darkgreen&#39;, &#39;brown&#39;, &#39;orange&#39;, &#39;yellow&#39;, &#39;red&#39; )) 4.7.2 Filtering using attributes Alternatively, we may want to only include Phenocams with certain attributes in our datasets. For example, we may be interested only in sites with a co-located flux tower. For this, we’d want to filter for those with a flux tower using the flux_sitenames attribute in the metadata. # store sites with flux_data available and the FLUX site name is specified phenofluxsites &lt;- phenos[flux_data==TRUE&amp;!is.na(flux_sitenames)&amp;flux_sitenames!=&#39;&#39;, .(PhenoCam=site, Flux=flux_sitenames)] # return as table #and specify which variables to retain phenofluxsites &lt;- phenofluxsites[Flux!=&#39;&#39;] # see the first few rows head(phenofluxsites) ## PhenoCam Flux ## 1: alligatorriver US-NC4 ## 2: arsbrooks10 US-Br1: Brooks Field Site 10- Ames ## 3: arsbrooks11 US-Br3: Brooks Field Site 11- Ames ## 4: arscolesnorth LTAR ## 5: arscolessouth LTAR ## 6: arsgreatbasinltar098 US-Rws We could further identify which of those Phenocams with a flux tower and in deciduous broadleaf forests (primary_veg_type=='DB'). #list deciduous broadleaf sites with flux tower DB.flux &lt;- phenos[flux_data==TRUE&amp;primary_veg_type==&#39;DB&#39;, site] # return just the site names as a list # see the first few rows head(DB.flux) ## [1] &quot;alligatorriver&quot; &quot;bartlett&quot; &quot;bartlettir&quot; &quot;bbc1&quot; ## [5] &quot;bbc2&quot; &quot;bbc3&quot; 4.8 Download midday images While PhenoCam sites may have many images in a given day, many simple analyses can use just the midday image when the sun is most directly overhead the canopy. Therefore, extracting a list of midday images (only one image a day) can be useful. # obtaining midday_images for dukehw duke_middays &lt;- get_midday_list(&#39;dukehw&#39;) # see the first few rows head(duke_middays) ## [1] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/05/dukehw_2013_05_31_150331.jpg&quot; ## [2] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_01_120111.jpg&quot; ## [3] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_02_120109.jpg&quot; ## [4] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_03_120110.jpg&quot; ## [5] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_04_120119.jpg&quot; ## [6] &quot;http://phenocam.sr.unh.edu/data/archive/dukehw/2013/06/dukehw_2013_06_05_120110.jpg&quot; Now we have a list of all the midday images from this Phenocam. Let’s download them and plot # download a file destfile &lt;- tempfile(fileext = &#39;.jpg&#39;) # download only the first available file # modify the `[1]` to download other images download.file(duke_middays[1], destfile = destfile, mode = &#39;wb&#39;) # plot the image img &lt;- try(readJPEG(destfile)) if(class(img)!=&#39;try-error&#39;){ par(mar= c(0,0,0,0)) plot(0:1,0:1, type=&#39;n&#39;, axes= FALSE, xlab= &#39;&#39;, ylab = &#39;&#39;) rasterImage(img, 0, 0, 1, 1) } 4.8.1 Download midday images for a given time range Now we can access all the midday images and download them one at a time. However, we frequently want all the images within a specific time range of interest. We’ll learn how to do that next. # open a temporary directory tmp_dir &lt;- tempdir() # download a subset. Example dukehw 2017 download_midday_images(site = &#39;dukehw&#39;, # which site y = 2017, # which year(s) months = 1:12, # which month(s) days = 15, # which days on month(s) download_dir = tmp_dir) # where on your computer ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% ## [1] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpMRDKKL&quot; # list of downloaded files duke_middays_path &lt;- dir(tmp_dir, pattern = &#39;dukehw*&#39;, full.names = TRUE) head(duke_middays_path) ## [1] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpMRDKKL/dukehw_2017_01_15_120109.jpg&quot; ## [2] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpMRDKKL/dukehw_2017_02_15_120108.jpg&quot; ## [3] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpMRDKKL/dukehw_2017_03_15_120151.jpg&quot; ## [4] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpMRDKKL/dukehw_2017_04_15_120110.jpg&quot; ## [5] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpMRDKKL/dukehw_2017_05_15_120108.jpg&quot; ## [6] &quot;/var/folders/5n/vxrdb72140b43_jhsmhfy514pqgq1n/T//RtmpMRDKKL/dukehw_2017_06_15_120120.jpg&quot; We can demonstrate the seasonality of Duke forest observed from the camera. (Note this code may take a while to run through the loop). n &lt;- length(duke_middays_path) par(mar= c(0,0,0,0), mfrow=c(4,3), oma=c(0,0,3,0)) for(i in 1:n){ img &lt;- readJPEG(duke_middays_path[i]) plot(0:1,0:1, type=&#39;n&#39;, axes= FALSE, xlab= &#39;&#39;, ylab = &#39;&#39;) rasterImage(img, 0, 0, 1, 1) mtext(month.name[i], line = -2) } mtext(&#39;Seasonal variation of forest at Duke Hardwood Forest&#39;, font = 2, outer = TRUE) The goal of this section was to show how to download a limited number of midday images from the PhenoCam server. However, more extensive datasets should be downloaded from the PhenoCam . The phenocamapi R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/phenocamapi. 4.9 Detecting Foggy Images using the ‘hazer’ R Package 4.9.0.1 Read &amp; Plot an Image We will use several packages in this tutorial. All are available from CRAN. # load packages library(hazer) library(jpeg) library(data.table) library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## between, first, last ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Before we start the image processing steps, let’s read in and plot an image. This image is an example image that comes with the hazer package. # read the path to the example image jpeg_file &lt;- system.file(package = &#39;hazer&#39;, &#39;pointreyes.jpg&#39;) # read the image as an array rgb_array &lt;- jpeg::readJPEG(jpeg_file) # plot the RGB array on the active device panel # first set the margin in this order:(bottom, left, top, right) par(mar=c(0,0,3,0)) plotRGBArray(rgb_array, bty = &#39;n&#39;, main = &#39;Point Reyes National Seashore&#39;) When we work with images, all data we work with is generally on the scale of each individual pixel in the image. Therefore, for large images we will be working with large matrices that hold the value for each pixel. Keep this in mind before opening some of the matrices we’ll be creating this tutorial as it can take a while for them to load. 4.9.0.2 Histogram of RGB channels A histogram of the colors can be useful to understanding what our image is made up of. Using the density() function from the base stats package, we can extract density distribution of each color channel. # color channels can be extracted from the matrix red_vector &lt;- rgb_array[,,1] green_vector &lt;- rgb_array[,,2] blue_vector &lt;- rgb_array[,,3] # plotting par(mar=c(5,4,4,2)) plot(density(red_vector), col = &#39;red&#39;, lwd = 2, main = &#39;Density function of the RGB channels&#39;, ylim = c(0,5)) lines(density(green_vector), col = &#39;green&#39;, lwd = 2) lines(density(blue_vector), col = &#39;blue&#39;, lwd = 2) In hazer we can also extract three basic elements of an RGB image : Brightness Darkness Contrast 4.9.0.3 Brightness The brightness matrix comes from the maximum value of the R, G, or B channel. We can extract and show the brightness matrix using the getBrightness() function. # extracting the brightness matrix brightness_mat &lt;- getBrightness(rgb_array) # unlike the RGB array which has 3 dimensions, the brightness matrix has only two # dimensions and can be shown as a grayscale image, # we can do this using the same plotRGBArray function par(mar=c(0,0,3,0)) plotRGBArray(brightness_mat, bty = &#39;n&#39;, main = &#39;Brightness matrix&#39;) Here the grayscale is used to show the value of each pixel’s maximum brightness of the R, G or B color channel. To extract a single brightness value for the image, depending on our needs we can perform some statistics or we can just use the mean of this matrix. # the main quantiles quantile(brightness_mat) ## 0% 25% 50% 75% 100% ## 0.09019608 0.43529412 0.62745098 0.80000000 0.91764706 # create histogram par(mar=c(5,4,4,2)) hist(brightness_mat) Question for the class: Why are we getting so many images up in the high range of the brightness? Where does this correlate to on the RGB image? 4.9.0.4 Darkness Darkness is determined by the minimum of the R, G or B color channel. In the Similarly, we can extract and show the darkness matrix using the getDarkness() function. # extracting the darkness matrix darkness_mat &lt;- getDarkness(rgb_array) # the darkness matrix has also two dimensions and can be shown as a grayscale image par(mar=c(0,0,3,0)) plotRGBArray(darkness_mat, bty = &#39;n&#39;, main = &#39;Darkness matrix&#39;) # main quantiles quantile(darkness_mat) ## 0% 25% 50% 75% 100% ## 0.03529412 0.23137255 0.36470588 0.47843137 0.83529412 # histogram par(mar=c(5,4,4,2)) hist(darkness_mat) 4.9.0.5 Contrast The contrast of an image is the difference between the darkness and brightness of the image. The contrast matrix is calculated by difference between the darkness and brightness matrices. The contrast of the image can quickly be extracted using the getContrast() function. # extracting the contrast matrix contrast_mat &lt;- getContrast(rgb_array) # the contrast matrix has also 2D and can be shown as a grayscale image par(mar=c(0,0,3,0)) plotRGBArray(contrast_mat, bty = &#39;n&#39;, main = &#39;Contrast matrix&#39;) # main quantiles quantile(contrast_mat) ## 0% 25% 50% 75% 100% ## 0.0000000 0.1450980 0.2470588 0.3333333 0.4509804 # histogram par(mar=c(5,4,4,2)) hist(contrast_mat) 4.9.0.6 Image fogginess &amp; haziness Haziness of an image can be estimated using the getHazeFactor() function. This function is based on the method described in Mao et al. (2014). The technique was originally developed to for “detecting foggy images and estimating the haze degree factor” for a wide range of outdoor conditions. The function returns a vector of two numeric values: haze as the haze degree and A0 as the global atmospheric light, as it is explained in the original paper. The PhenoCam standards classify any image with the haze degree greater than 0.4 as a significantly foggy image. # extracting the haze matrix haze_degree &lt;- getHazeFactor(rgb_array) print(haze_degree) ## $haze ## [1] 0.2251633 ## ## $A0 ## [1] 0.7105258 Here we have the haze values for our image. Note that the values might be slightly different due to rounding errors on different platforms. 4.9.0.7 Process sets of images We can use for loops or the lapply functions to extract the haze values for a stack of images. You can download the related datasets from here (direct download). Download and extract the zip file to be used as input data for the following step. #pointreyes_url &lt;- &#39;http://bit.ly/2F8w2Ia&#39; # set up the input image directory data_dir &lt;- &#39;data/&#39; #dir.create(data_dir, showWarnings = F) #pointreyes_zip &lt;- paste0(data_dir, &#39;pointreyes.zip&#39;) pointreyes_dir &lt;- paste0(data_dir, &#39;pointreyes&#39;) #download zip file #download.file(pointreyes_url, destfile = pointreyes_zip) #unzip(pointreyes_zip, exdir = data_dir) # get a list of all .jpg files in the directory pointreyes_images &lt;- dir(path = &#39;data/pointreyes&#39;, pattern = &#39;*.jpg&#39;, ignore.case = TRUE, full.names = TRUE) Now we can use a for loop to process all of the images to get the haze and A0 values. # number of images n &lt;- length(pointreyes_images) # create an empty matrix to fill with haze and A0 values haze_mat &lt;- data.frame() # the process takes a bit, a progress bar lets us know it is working. pb &lt;- txtProgressBar(0, n, style = 3) ## | | | 0% for(i in 1:n) { image_path &lt;- pointreyes_images[i] img &lt;- jpeg::readJPEG(image_path) hz &lt;- getHazeFactor(img) haze_mat &lt;- rbind(haze_mat, data.frame(file = as.character(image_path), haze = hz[1], A0 = hz[2])) setTxtProgressBar(pb, i) } ## | |= | 1% | |== | 3% | |=== | 4% | |==== | 6% | |===== | 7% | |====== | 8% | |======= | 10% | |======== | 11% | |========= | 13% | |========== | 14% | |=========== | 15% | |============ | 17% | |============= | 18% | |============== | 20% | |=============== | 21% | |================ | 23% | |================= | 24% | |================== | 25% | |=================== | 27% | |==================== | 28% | |===================== | 30% | |====================== | 31% | |======================= | 32% | |======================== | 34% | |========================= | 35% | |========================== | 37% | |=========================== | 38% | |============================ | 39% | |============================= | 41% | |============================== | 42% | |=============================== | 44% | |================================ | 45% | |================================= | 46% | |================================== | 48% | |=================================== | 49% | |=================================== | 51% | |==================================== | 52% | |===================================== | 54% | |====================================== | 55% | |======================================= | 56% | |======================================== | 58% | |========================================= | 59% | |========================================== | 61% | |=========================================== | 62% | |============================================ | 63% | |============================================= | 65% | |============================================== | 66% | |=============================================== | 68% | |================================================ | 69% | |================================================= | 70% | |================================================== | 72% | |=================================================== | 73% | |==================================================== | 75% | |===================================================== | 76% | |====================================================== | 77% | |======================================================= | 79% | |======================================================== | 80% | |========================================================= | 82% | |========================================================== | 83% | |=========================================================== | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 89% | |=============================================================== | 90% | |================================================================ | 92% | |================================================================= | 93% | |================================================================== | 94% | |=================================================================== | 96% | |==================================================================== | 97% | |===================================================================== | 99% | |======================================================================| 100% Now we have a matrix with haze and A0 values for all our images. Let’s compare top three images with low and high haze values. top10_high_haze &lt;- haze_mat %&gt;% dplyr::arrange(desc(haze)) %&gt;% slice(1:3) top10_low_haze &lt;- haze_mat %&gt;% arrange(haze)%&gt;% slice(1:3) par(mar= c(0,0,0,0), mfrow=c(3,2), oma=c(0,0,3,0)) for(i in 1:3){ img &lt;- readJPEG(as.character(top10_low_haze$file[i])) plot.new() rasterImage(img, par()$usr[1], par()$usr[3], par()$usr[2], par()$usr[4]) img &lt;- readJPEG(as.character(top10_high_haze$file[i])) plot.new() rasterImage(img, par()$usr[1], par()$usr[3], par()$usr[2], par()$usr[4]) } mtext(&#39;Top images with low (left) and high (right) haze values at Point Reyes&#39;, font = 2, outer = TRUE) Let’s classify those into hazy and non-hazy as per the PhenoCam standard of 0.4. # classify image as hazy: T/F haze_mat=haze_mat%&gt;% mutate(haze_mat, foggy=ifelse(haze&gt;.4, TRUE, FALSE)) head(haze_mat) ## file haze A0 foggy ## 1 data/pointreyes/pointreyes_2017_01_01_120056.jpg 0.2249810 0.6970257 FALSE ## 2 data/pointreyes/pointreyes_2017_01_06_120210.jpg 0.2339372 0.6826148 FALSE ## 3 data/pointreyes/pointreyes_2017_01_16_120105.jpg 0.2312940 0.7009978 FALSE ## 4 data/pointreyes/pointreyes_2017_01_21_120105.jpg 0.4536108 0.6209055 TRUE ## 5 data/pointreyes/pointreyes_2017_01_26_120106.jpg 0.2297961 0.6813884 FALSE ## 6 data/pointreyes/pointreyes_2017_01_31_120125.jpg 0.4206842 0.6315728 TRUE Now we can save all the foggy images to a new folder that will retain the foggy images but keep them separate from the non-foggy ones that we want to analyze. # identify directory to move the foggy images to foggy_dir &lt;- paste0(pointreyes_dir, &#39;foggy&#39;) clear_dir &lt;- paste0(pointreyes_dir, &#39;clear&#39;) # if a new directory, create new directory at this file path dir.create(foggy_dir, showWarnings = FALSE) dir.create(clear_dir, showWarnings = FALSE) # copy the files to the new directories #file.copy(haze_mat[foggy==TRUE,file], to = foggy_dir) #file.copy(haze_mat[foggy==FALSE,file], to = clear_dir) Now that we have our images separated, we can get the full list of haze values only for those images that are not classified as “foggy”. # this is an alternative approach instead of a for loop # loading all the images as a list of arrays pointreyes_clear_images &lt;- dir(path = clear_dir, pattern = &#39;*.jpg&#39;, ignore.case = TRUE, full.names = TRUE) img_list &lt;- lapply(pointreyes_clear_images, FUN = jpeg::readJPEG) # getting the haze value for the list # patience - this takes a bit of time haze_list &lt;- t(sapply(img_list, FUN = getHazeFactor)) # view first few entries head(haze_list) ## haze A0 ## [1,] 0.224981 0.6970257 ## [2,] 0.2339372 0.6826148 ## [3,] 0.231294 0.7009978 ## [4,] 0.2297961 0.6813884 ## [5,] 0.2152078 0.6949932 ## [6,] 0.345584 0.6789334 We can then use these values for further analyses and data correction. The hazer R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/hazer. 4.10 Extracting Timeseries from Images using the xROI R Package In this section, we’ll learn how to use an interactive open-source toolkit, the xROI R package that facilitates the process of time series extraction and improves the quality of the final data. The xROI package provides a responsive environment for scientists to interactively: delineate regions of interest (ROIs), handle field of view (FOV) shifts, and extract and export time series data characterizing color-based metrics. Using the xROI R package, the user can detect FOV shifts with minimal difficulty. The software gives user the opportunity to re-adjust mask files or redraw new ones every time an FOV shift occurs. 4.10.1 xROI Design The R language and Shiny package were used as the main development tool for xROI, while Markdown, HTML, CSS and JavaScript languages were used to improve the interactivity. While Shiny apps are primarily used for web-based applications to be used online, the package authors used Shiny for its graphical user interface capabilities. In other words, both the User Interface (UI) and server modules are run locally from the same machine and hence no internet connection is required (after installation). The xROI’s UI element presents a side-panel for data entry and three main tab-pages, each responsible for a specific task. The server-side element consists of R and bash scripts. Image processing and geospatial features were performed using the Geospatial Data Abstraction Library (GDAL) and the rgdal and raster R packages. 4.10.2 Install xROI The xROI R package has been published on The Comprehensive R Archive Network (CRAN). The latest tested xROI package can be installed from the CRAN packages repository by running the following command in an R environment. utils::install.packages(&#39;xROI&#39;, repos = &quot;http://cran.us.r-project.org&quot; ) Alternatively, the latest beta release of xROI can be directly downloaded and installed from the development GitHub repository. # install devtools first utils::install.packages(&#39;devtools&#39;, repos = &quot;http://cran.us.r-project.org&quot; ) # use devtools to install from GitHub devtools::install_github(&quot;bnasr/xROI&quot;) xROI depends on many R packages including: raster, rgdal, sp, jpeg, tiff, shiny, shinyjs, shinyBS, shinyAce, shinyTime, shinyFiles, shinydashboard, shinythemes, colourpicker, rjson, stringr, data.table, lubridate, plotly, moments, and RCurl. All the required libraries and packages will be automatically installed with installation of xROI. The package offers a fully interactive high-level interface as well as a set of low-level functions for ROI processing. 4.10.3 Launch xROI A comprehensive user manual for low-level image processing using xROI is available from CRAN xROI.pdf. While the user manual includes a set of examples for each function; here we will learn to use the graphical interactive mode. Calling the Launch() function, as we’ll do below, opens up the interactive mode in your operating system’s default web browser. The landing page offers an example dataset to explore different modules or upload a new dataset of images. You can launch the interactive mode can be launched from an interactive R environment. # load xROI library(xROI) # launch xROI Launch() Or from the command line (e.g. bash in Linux, Terminal in macOS and Command Prompt in Windows machines) where an R engine is already installed. Rscript -e “xROI::Launch(Interactive = TRUE)” 4.10.4 End xROI When you are done with the xROI interface you can close the tab in your browser and end the session in R by using one of the following options In RStudio: Press the key on your keyboard. In R Terminal: Press &lt;Ctrl + C&gt; on your keyboard. 4.10.5 Use xROI To get some hands-on experience with xROI, we can analyze images from the dukehw of the PhenoCam network. You can download the data set from this link (direct download). Follow the steps below: First,save and extract (unzip) the file on your computer. Second, open the data set in xROI by setting the file path to your data # launch data in ROI # first edit the path below to the dowloaded directory you just extracted xROI::Launch(&#39;/path/to/extracted/directory&#39;) # alternatively, you can run without specifying a path and use the interface to browse Now, draw an ROI and the metadata. Then, save the metadata and explore its content. Now we can explore if there is any FOV shift in the dataset using the CLI processer tab. Finally, we can go to the Time series extraction tab. Extract the time-series. Save the output and explore the dataset in R. 4.11 Documentation and Citation More documentation about xROI can be found from: Seyednarollah, et al. 2019. knitr::include_graphics(&#39;docs/images/xROI-ms2019.png&#39;) &gt;xROI published in ISPRS Journal of Photogrammetry and Remote Sensing, 2019 4.11.1 Challenge: Use xROI Let’s use xROI on a little more challenging site with field of view shifts. Download and extract the data set from this link (direct download, 218 MB) and follow the above steps to extract the time-series. The xROI R package is developed and maintained by Bijan Seyednarollah. The most recent release is available from https://github.com/bnasr/xROI. 4.12 Hands on: Digital Repeat Photography Computational First let’s load some packages: library(jsonlite) ## ## Attaching package: &#39;jsonlite&#39; ## The following objects are masked from &#39;package:rjson&#39;: ## ## fromJSON, toJSON library(phenocamapi) library(plotly) ## Loading required package: ggplot2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(phenocamr) library(dplyr) As a refresher, there are two main ways to pull in PhenoCam data. First, directly via the API: c = jsonlite::fromJSON(&#39;https://phenocam.sr.unh.edu/api/cameras/?format=json&amp;limit=2000&#39;) c = c$results c_m=c$sitemetadata c$sitemetadata=NULL cams_=cbind(c, c_m) cams_[is.na(cams_)] = &#39;N&#39; cams_[, 2:4] &lt;- sapply(cams_[, 2:4], as.numeric) #changing lat/lon/elev from string values into numeric ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion head(cams_) ## Sitename Lat Lon Elev active utc_offset date_first ## 1 aafcottawacfiaf14e 45.29210 -75.76640 90 TRUE -5 2020-04-27 ## 2 aafcottawacfiaf14w 45.29210 -75.76640 90 TRUE -5 2020-05-01 ## 3 acadia 44.37694 -68.26083 158 TRUE -5 2007-03-15 ## 4 aguatibiaeast 33.62200 -116.86700 1086 FALSE -8 2007-08-16 ## 5 aguatibianorth 33.60222 -117.34368 1090 FALSE -8 2003-10-01 ## 6 ahwahnee 37.74670 -119.58160 1199 TRUE -8 2008-08-28 ## date_last infrared contact1 ## 1 2020-09-28 N Elizabeth Pattey &lt;elizabeth DOT pattey AT canada DOT ca&gt; ## 2 2020-09-28 N Elizabeth Pattey &lt;elizabeth DOT pattey AT canada DOT ca&gt; ## 3 2020-10-06 N Dee Morse &lt;dee_morse AT nps DOT gov&gt; ## 4 2019-01-25 N Ann E Mebane &lt;amebane AT fs DOT fed DOT us&gt; ## 5 2006-10-25 N ## 6 2020-10-06 N Dee Morse &lt;dee_morse AT nps DOT gov&gt; ## contact2 ## 1 Luc Pelletier &lt;luc DOT pelletier3 AT canada DOT ca&gt; ## 2 Luc Pelletier &lt;luc DOT pelletier3 AT canada DOT ca&gt; ## 3 John Gross &lt;John_Gross AT nps DOT gov&gt; ## 4 Kristi Savig &lt;KSavig AT air-resource DOT com&gt; ## 5 ## 6 John Gross &lt;John_Gross AT nps DOT gov&gt; ## site_description site_type ## 1 AAFC Site - Ottawa (On) - CFIA - Field F14 -East Flux Tower II ## 2 AAFC Site - Ottawa (On) - CFIA - Field F14 -West Flux Tower II ## 3 Acadia National Park, McFarland Hill, near Bar Harbor, Maine III ## 4 Agua Tibia Wilderness, California III ## 5 Agua Tibia Wilderness, California III ## 6 Ahwahnee Meadow, Yosemite National Park, California III ## group camera_description camera_orientation flux_data ## 1 N Campbell Scientific CCFC NE TRUE ## 2 N Campbell Scientific CCFC WNW TRUE ## 3 National Park Service unknown NE FALSE ## 4 USFS unknown SW FALSE ## 5 USFS unknown NE FALSE ## 6 National Park Service unknown E FALSE ## flux_networks flux_sitenames ## 1 NULL N ## 2 OTHER, , Other/Unaffiliated N ## 3 NULL ## 4 NULL ## 5 NULL ## 6 NULL ## dominant_species primary_veg_type ## 1 Zea mays, Triticum aestivum, Brassica napus, Glycine max AG ## 2 Zea mays, Triticum aestivum, Brassica napus, Glycine max AG ## 3 DB ## 4 SH ## 5 SH ## 6 EN ## secondary_veg_type site_meteorology MAT_site MAP_site MAT_daymet MAP_daymet ## 1 AG TRUE 6.4 943 6.3 952 ## 2 AG TRUE 6.4 943 6.3 952 ## 3 EN FALSE N N 7.05 1439 ## 4 FALSE N N 15.75 483 ## 5 FALSE N N 16 489 ## 6 GR FALSE N N 12.25 871 ## MAT_worldclim MAP_worldclim koeppen_geiger ecoregion landcover_igbp ## 1 6 863 Dfb 8 12 ## 2 6 863 Dfb 8 12 ## 3 6.5 1303 Dfb 8 5 ## 4 14.9 504 Csa 11 7 ## 5 13.8 729 Csa 11 7 ## 6 11.8 886 Csb 6 8 ## site_acknowledgements ## 1 Camera funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors’ impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 2 Cameras funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors’ impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 3 Camera images from Acadia National Park are provided courtesy of the National Park Service Air Resources Program. ## 4 Camera images from Agua Tibia Wilderness are provided courtesy of the USDA Forest Service Air Resources Management Program. ## 5 Camera images from Agua Tibia Wilderness are provided courtesy of the USDA Forest Service Air Resources Management Program. ## 6 Camera images from Yosemite National Park are provided courtesy of the National Park Service Air Resources Program. ## modified ## 1 2020-05-04T10:46:30.065790-04:00 ## 2 2020-05-04T10:46:32.523976-04:00 ## 3 2016-11-01T15:42:15.016778-04:00 ## 4 2016-11-01T15:42:15.086984-04:00 ## 5 2016-11-01T15:42:15.095277-04:00 ## 6 2016-11-01T15:42:15.111916-04:00 And second, via the phenocamapi package: phenos=get_phenos() head(phenos) ## site lat lon elev active utc_offset date_first ## 1: aafcottawacfiaf14e 45.29210 -75.76640 90 TRUE -5 2020-04-27 ## 2: aafcottawacfiaf14w 45.29210 -75.76640 90 TRUE -5 2020-05-01 ## 3: acadia 44.37694 -68.26083 158 TRUE -5 2007-03-15 ## 4: aguatibiaeast 33.62200 -116.86700 1086 FALSE -8 2007-08-16 ## 5: aguatibianorth 33.60222 -117.34368 1090 FALSE -8 2003-10-01 ## 6: ahwahnee 37.74670 -119.58160 1199 TRUE -8 2008-08-28 ## date_last infrared contact1 ## 1: 2020-09-28 N Elizabeth Pattey &lt;elizabeth.pattey@canada.ca&gt; ## 2: 2020-09-28 N Elizabeth Pattey &lt;elizabeth.pattey@canada.ca&gt; ## 3: 2020-10-06 N Dee Morse &lt;dee_morse@nps.gov&gt; ## 4: 2019-01-25 N Ann E Mebane &lt;amebane@fs.fed.us&gt; ## 5: 2006-10-25 N ## 6: 2020-10-06 N Dee Morse &lt;dee_morse@nps.gov&gt; ## contact2 ## 1: Luc Pelletier &lt;luc.pelletier3@canada.ca&gt; ## 2: Luc Pelletier &lt;luc.pelletier3@canada.ca&gt; ## 3: John Gross &lt;John_Gross@nps.gov&gt; ## 4: Kristi Savig &lt;KSavig@air-resource.com&gt; ## 5: ## 6: John Gross &lt;John_Gross@nps.gov&gt; ## site_description site_type ## 1: AAFC Site - Ottawa (On) - CFIA - Field F14 -East Flux Tower II ## 2: AAFC Site - Ottawa (On) - CFIA - Field F14 -West Flux Tower II ## 3: Acadia National Park, McFarland Hill, near Bar Harbor, Maine III ## 4: Agua Tibia Wilderness, California III ## 5: Agua Tibia Wilderness, California III ## 6: Ahwahnee Meadow, Yosemite National Park, California III ## group camera_description camera_orientation flux_data ## 1: &lt;NA&gt; Campbell Scientific CCFC NE TRUE ## 2: &lt;NA&gt; Campbell Scientific CCFC WNW TRUE ## 3: National Park Service unknown NE FALSE ## 4: USFS unknown SW FALSE ## 5: USFS unknown NE FALSE ## 6: National Park Service unknown E FALSE ## flux_networks flux_sitenames ## 1: &lt;list[0]&gt; &lt;NA&gt; ## 2: &lt;list[1]&gt; &lt;NA&gt; ## 3: &lt;list[0]&gt; ## 4: &lt;list[0]&gt; ## 5: &lt;list[0]&gt; ## 6: &lt;list[0]&gt; ## dominant_species primary_veg_type ## 1: Zea mays, Triticum aestivum, Brassica napus, Glycine max AG ## 2: Zea mays, Triticum aestivum, Brassica napus, Glycine max AG ## 3: DB ## 4: SH ## 5: SH ## 6: EN ## secondary_veg_type site_meteorology MAT_site MAP_site MAT_daymet MAP_daymet ## 1: AG TRUE 6.4 943 6.30 952 ## 2: AG TRUE 6.4 943 6.30 952 ## 3: EN FALSE NA NA 7.05 1439 ## 4: FALSE NA NA 15.75 483 ## 5: FALSE NA NA 16.00 489 ## 6: GR FALSE NA NA 12.25 871 ## MAT_worldclim MAP_worldclim koeppen_geiger ecoregion landcover_igbp ## 1: 6.0 863 Dfb 8 12 ## 2: 6.0 863 Dfb 8 12 ## 3: 6.5 1303 Dfb 8 5 ## 4: 14.9 504 Csa 11 7 ## 5: 13.8 729 Csa 11 7 ## 6: 11.8 886 Csb 6 8 ## dataset_version1 ## 1: NA ## 2: NA ## 3: NA ## 4: NA ## 5: NA ## 6: NA ## site_acknowledgements ## 1: Camera funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors’ impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 2: Cameras funded by Agriculture and Agri-Food Canada (AAFC) Project J-001735 - Commercial inhibitors’ impact on crop productivity and emissions of nitrous oxide led by Dr. Elizabeth Pattey; Support provided by Drs, Luc Pelletier and Elizabeth Pattey, Micrometeorologiccal Laboratory of AAFC - Ottawa Research and Development Centre. ## 3: Camera images from Acadia National Park are provided courtesy of the National Park Service Air Resources Program. ## 4: Camera images from Agua Tibia Wilderness are provided courtesy of the USDA Forest Service Air Resources Management Program. ## 5: Camera images from Agua Tibia Wilderness are provided courtesy of the USDA Forest Service Air Resources Management Program. ## 6: Camera images from Yosemite National Park are provided courtesy of the National Park Service Air Resources Program. ## modified flux_networks_name flux_networks_url ## 1: 2020-05-04T10:46:30.065790-04:00 &lt;NA&gt; &lt;NA&gt; ## 2: 2020-05-04T10:46:32.523976-04:00 OTHER ## 3: 2016-11-01T15:42:15.016778-04:00 &lt;NA&gt; &lt;NA&gt; ## 4: 2016-11-01T15:42:15.086984-04:00 &lt;NA&gt; &lt;NA&gt; ## 5: 2016-11-01T15:42:15.095277-04:00 &lt;NA&gt; &lt;NA&gt; ## 6: 2016-11-01T15:42:15.111916-04:00 &lt;NA&gt; &lt;NA&gt; ## flux_networks_description ## 1: &lt;NA&gt; ## 2: Other/Unaffiliated ## 3: &lt;NA&gt; ## 4: &lt;NA&gt; ## 5: &lt;NA&gt; ## 6: &lt;NA&gt; To familiarize yourself with the phenocam API, let’s explore the structure: https://phenocam.sr.unh.edu/api/ Explore the options for filtering, file type and so forth. 4.12.1 PhenoCam time series PhenoCam time series are extracted time series data obtained from ROI’s for a given site. 4.12.2 Obtain ROIs To download the phenological time series from the PhenoCam, we need to know the site name, vegetation type and ROI ID. This information can be obtained from each specific PhenoCam page on the PhenoCam website or by using the get_rois() function. # obtaining the list of all the available ROI&#39;s on the PhenoCam server rois &lt;- get_rois() # view what information is returned colnames(rois) ## [1] &quot;roi_name&quot; &quot;site&quot; &quot;lat&quot; ## [4] &quot;lon&quot; &quot;roitype&quot; &quot;active&quot; ## [7] &quot;show_link&quot; &quot;show_data_link&quot; &quot;sequence_number&quot; ## [10] &quot;description&quot; &quot;first_date&quot; &quot;last_date&quot; ## [13] &quot;site_years&quot; &quot;missing_data_pct&quot; &quot;roi_page&quot; ## [16] &quot;roi_stats_file&quot; &quot;one_day_summary&quot; &quot;three_day_summary&quot; ## [19] &quot;data_release&quot; # view first few locations head(rois$roi_name) ## [1] &quot;alligatorriver_DB_1000&quot; &quot;arbutuslake_DB_1000&quot; ## [3] &quot;arbutuslakeinlet_DB_1000&quot; &quot;arbutuslakeinlet_EN_1000&quot; ## [5] &quot;arbutuslakeinlet_EN_2000&quot; &quot;archboldavir_AG_1000&quot; 4.12.3 Download time series The get_pheno_ts() function can download a time series and return the result as a data.table. Let’s work with the Duke Forest Hardwood Stand (dukehw) PhenoCam and specifically the ROI DB_1000 we can run the following code. # list ROIs for dukehw rois[site==&#39;dukehw&#39;,] ## roi_name site lat lon roitype active show_link ## 1: dukehw_DB_1000 dukehw 35.97358 -79.10037 DB TRUE TRUE ## show_data_link sequence_number description ## 1: TRUE 1000 canopy level DB forest at awesome Duke forest ## first_date last_date site_years missing_data_pct ## 1: 2013-06-01 2020-10-07 7.1 3.0 ## roi_page ## 1: https://phenocam.sr.unh.edu/webcam/roi/dukehw/DB_1000/ ## roi_stats_file ## 1: https://phenocam.sr.unh.edu/data/archive/dukehw/ROI/dukehw_DB_1000_roistats.csv ## one_day_summary ## 1: https://phenocam.sr.unh.edu/data/archive/dukehw/ROI/dukehw_DB_1000_1day.csv ## three_day_summary ## 1: https://phenocam.sr.unh.edu/data/archive/dukehw/ROI/dukehw_DB_1000_3day.csv ## data_release ## 1: NA # to obtain the DB 1000 from dukehw dukehw_DB_1000 &lt;- get_pheno_ts(site = &#39;dukehw&#39;, vegType = &#39;DB&#39;, roiID = 1000, type = &#39;3day&#39;) # what data are available str(dukehw_DB_1000) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 898 obs. of 35 variables: ## $ date : Factor w/ 898 levels &quot;2013-06-01&quot;,&quot;2013-06-04&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ doy : int 152 155 158 161 164 167 170 173 176 179 ... ## $ image_count : int 57 76 77 77 77 78 21 0 0 0 ... ## $ midday_filename : Factor w/ 869 levels &quot;dukehw_2013_06_01_120111.jpg&quot;,..: 1 2 3 4 5 6 7 869 869 869 ... ## $ midday_r : num 91.3 76.4 60.6 76.5 88.9 ... ## $ midday_g : num 97.9 85 73.2 82.2 95.7 ... ## $ midday_b : num 47.4 33.6 35.6 37.1 51.4 ... ## $ midday_gcc : num 0.414 0.436 0.432 0.42 0.406 ... ## $ midday_rcc : num 0.386 0.392 0.358 0.391 0.377 ... ## $ r_mean : num 87.6 79.9 72.7 80.9 83.8 ... ## $ r_std : num 5.9 6 9.5 8.23 5.89 ... ## $ g_mean : num 92.1 86.9 84 88 89.7 ... ## $ g_std : num 6.34 5.26 7.71 7.77 6.47 ... ## $ b_mean : num 46.1 38 39.6 43.1 46.7 ... ## $ b_std : num 4.48 3.42 5.29 4.73 4.01 ... ## $ gcc_mean : num 0.408 0.425 0.429 0.415 0.407 ... ## $ gcc_std : num 0.00859 0.0089 0.01318 0.01243 0.01072 ... ## $ gcc_50 : num 0.408 0.427 0.431 0.416 0.407 ... ## $ gcc_75 : num 0.414 0.431 0.435 0.424 0.415 ... ## $ gcc_90 : num 0.417 0.434 0.44 0.428 0.421 ... ## $ rcc_mean : num 0.388 0.39 0.37 0.381 0.38 ... ## $ rcc_std : num 0.01176 0.01032 0.01326 0.00881 0.00995 ... ## $ rcc_50 : num 0.387 0.391 0.373 0.383 0.382 ... ## $ rcc_75 : num 0.391 0.396 0.378 0.388 0.385 ... ## $ rcc_90 : num 0.397 0.399 0.382 0.391 0.389 ... ## $ max_solar_elev : num 76 76.3 76.6 76.8 76.9 ... ## $ snow_flag : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_mean: logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_50 : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_75 : logi NA NA NA NA NA NA ... ## $ outlierflag_gcc_90 : logi NA NA NA NA NA NA ... ## $ YEAR : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ DOY : int 152 155 158 161 164 167 170 173 176 179 ... ## $ YYYYMMDD : chr &quot;2013-06-01&quot; &quot;2013-06-04&quot; &quot;2013-06-07&quot; &quot;2013-06-10&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; We now have a variety of data related to this ROI from the Hardwood Stand at Duke Forest. Green Chromatic Coordinate (GCC) is a measure of “greenness” of an area and is widely used in Phenocam images as an indicator of the green pigment in vegetation. Let’s use this measure to look at changes in GCC over time at this site. Looking back at the available data, we have several options for GCC. gcc90 is the 90th quantile of GCC in the pixels across the ROI (for more details, PhenoCam v1 description). We’ll use this as it tracks the upper greenness values while not including many outliners. Before we can plot gcc-90 we do need to fix our dates and convert them from Factors to Date to correctly plot. # date variable into date format dukehw_DB_1000[,date:=as.Date(date)] # plot gcc_90 dukehw_DB_1000[,plot(date, gcc_90, col = &#39;green&#39;, type = &#39;b&#39;)] ## NULL mtext(&#39;Duke Forest, Hardwood&#39;, font = 2) Now, based on either direct API access or via the phenocamapi package, generate a dataframe of phenocam sites. Select two phenocam sites from different plant functional types to explore (e.g. one grassland site and one evergreen needleleaf site) #example GrassSites=cams_%&gt;% filter(cams_$primary_veg_type==&#39;GR&#39;) head(GrassSites) ## Sitename Lat Lon Elev active utc_offset date_first ## 1 archboldbahia 27.16560 -81.21611 8 TRUE -5 2017-03-21 ## 2 arizonagrass 31.59070 -110.50920 1469 FALSE -7 2008-01-01 ## 3 arsgacp2 31.43950 -83.59146 101 FALSE 2 2016-04-27 ## 4 bozeman 45.78306 -110.77778 2332 FALSE -7 2015-08-16 ## 5 butte 45.95304 -112.47964 1682 TRUE -7 2008-04-01 ## 6 coaloilpoint 34.41369 -119.88023 6 FALSE -8 2008-05-11 ## date_last infrared contact1 ## 1 2020-10-06 Y Amartya Saha &lt;asaha AT archbold-station DOT org&gt; ## 2 2010-04-25 N Mark Heuer &lt;Mark DOT Heuer AT noaa DOT gov&gt; ## 3 2018-01-23 Y David Bosch &lt;David DOT Bosch AT ars DOT usda DOT gov&gt; ## 4 2019-12-18 Y Paul Stoy &lt;paul DOT stoy AT gmail DOT com&gt; ## 5 2020-10-06 N James Gallagher &lt;jgallagher AT opendap DOT org&gt; ## 6 2012-12-05 N Roberts &lt;dar AT geog DOT ucsb DOT edu&gt; ## contact2 ## 1 Elizabeth Boughton &lt;eboughton AT archbold-station DOT org&gt; ## 2 Tilden Meyers &lt;tilden DOT meyers AT noaa DOT gov&gt; ## 3 ## 4 ## 5 Martha Apple &lt;MApple AT mtech DOT edu&gt; ## 6 ## site_description site_type ## 1 Archbold Biological Station, Florida, USA I ## 2 Sierra Vista, Arizona III ## 3 Southeast Watershed Research Laboratory EC2 Tifton, Georgia I ## 4 Bangtail Study Area, Montana State University, Montana I ## 5 Continental Divide, Butte, Montana I ## 6 Coal Oil Point Natural Reserve, Santa Barbara, California II ## group camera_description camera_orientation flux_data ## 1 LTAR StarDot NetCam SC N FALSE ## 2 GEWEX Olympus D-360L N FALSE ## 3 LTAR N N FALSE ## 4 AmericaView AMERIFLUX StarDot NetCam SC N TRUE ## 5 PhenoCam StarDot NetCam SC E FALSE ## 6 unknown FALSE ## flux_networks flux_sitenames ## 1 NULL N ## 2 NULL N ## 3 NULL N ## 4 AMERIFLUX, http://ameriflux.lbl.gov, AmeriFlux Network US-MTB (forthcoming) ## 5 NULL ## 6 NULL ## dominant_species ## 1 ## 2 ## 3 ## 4 Festuca idahoensis ## 5 Agropyron cristatum, Poa pratensis, Phalaris arundinaceae, Carex. sp., Geum triflorum, Ericameria nauseousa, Centaurea macula, Achillea millefolium, Senecio sp., Lupinus sp., Penstemon sp., Linaria vulgaris, Cirsium arvense; Alnus incana, Salix sp., Populus tremuloides ## 6 Lolium multiflorum, Bromus horadaceous, Avena fatua, Plantago lanceolata, Sisyrinchium bellum ## primary_veg_type secondary_veg_type site_meteorology MAT_site MAP_site ## 1 GR N FALSE N N ## 2 GR N FALSE N N ## 3 GR N FALSE N N ## 4 GR EN FALSE 5 850 ## 5 GR SH FALSE N N ## 6 GR SH FALSE 14.4 424 ## MAT_daymet MAP_daymet MAT_worldclim MAP_worldclim koeppen_geiger ecoregion ## 1 22.85 1302 22.5 1208 Cfa 8 ## 2 16.25 468 15.3 446 BSk 12 ## 3 19 1251 18.7 1213 Cfa 8 ## 4 2.3 981 0.9 728 Dfb 6 ## 5 5.05 365 4.3 311 BSk 6 ## 6 16.15 623 15.4 405 Csb 11 ## landcover_igbp ## 1 14 ## 2 7 ## 3 14 ## 4 10 ## 5 10 ## 6 7 ## site_acknowledgements ## 1 ## 2 ## 3 ## 4 Research at the Bozeman site is supported by Colorado State University and the AmericaView program (grants G13AC00393, G11AC20461, G15AC00056) with phenocam equipment and deployment sponsored by the Department of Interior North Central Climate Science Center. ## 5 Research at the Continental Divide PhenoCam Site in Butte, Montana is supported by the National Science Foundation-EPSCoR (grant NSF-0701906), OpenDap, Inc., and Montana Tech of the University of Montana ## 6 ## modified ## 1 2019-01-07T18:36:07.631244-05:00 ## 2 2018-04-13T10:46:23.462958-04:00 ## 3 2018-06-18T20:27:17.280524-04:00 ## 4 2016-11-01T15:42:19.771057-04:00 ## 5 2016-11-01T15:42:19.846100-04:00 ## 6 2016-11-01T15:42:19.929455-04:00 FirstSite=GrassSites[5, ] #randomly chose the fifth site in the table FirstSite ## Sitename Lat Lon Elev active utc_offset date_first date_last ## 5 butte 45.95304 -112.4796 1682 TRUE -7 2008-04-01 2020-10-06 ## infrared contact1 ## 5 N James Gallagher &lt;jgallagher AT opendap DOT org&gt; ## contact2 site_description ## 5 Martha Apple &lt;MApple AT mtech DOT edu&gt; Continental Divide, Butte, Montana ## site_type group camera_description camera_orientation flux_data ## 5 I PhenoCam StarDot NetCam SC E FALSE ## flux_networks flux_sitenames ## 5 NULL ## dominant_species ## 5 Agropyron cristatum, Poa pratensis, Phalaris arundinaceae, Carex. sp., Geum triflorum, Ericameria nauseousa, Centaurea macula, Achillea millefolium, Senecio sp., Lupinus sp., Penstemon sp., Linaria vulgaris, Cirsium arvense; Alnus incana, Salix sp., Populus tremuloides ## primary_veg_type secondary_veg_type site_meteorology MAT_site MAP_site ## 5 GR SH FALSE N N ## MAT_daymet MAP_daymet MAT_worldclim MAP_worldclim koeppen_geiger ecoregion ## 5 5.05 365 4.3 311 BSk 6 ## landcover_igbp ## 5 10 ## site_acknowledgements ## 5 Research at the Continental Divide PhenoCam Site in Butte, Montana is supported by the National Science Foundation-EPSCoR (grant NSF-0701906), OpenDap, Inc., and Montana Tech of the University of Montana ## modified ## 5 2016-11-01T15:42:19.846100-04:00 Chose your own sites and build out your code chunk here: print(&#39;build your code here&#39;) ## [1] &quot;build your code here&quot; Koen Huffkens developed the phenocamr package, which streamlines access to quality controlled data. We will now use this package to download and process site based data according to a standardized methodology. A full description of the methodology is provided in Scientific Data: Tracking vegetation phenology across diverse North American biomes using PhenoCam imagery (Richardson et al. 2018). #uncomment if you need to install via devtools #if(!require(devtools)){install.package(devtools)} #devtools::install_github(&quot;khufkens/phenocamr&quot;) library(phenocamr) Use the dataframe of sites that you want to analyze to feed the phenocamr package. Note: you can choose either a 1 or 3 day product dir.create(&#39;data/&#39;, showWarnings = F) phenocamr::download_phenocam( frequency = 3, veg_type = &#39;DB&#39;, roi_id = 1000, site = &#39;harvard&#39;, phenophase = TRUE, out_dir = &quot;data/&quot; ) ## Downloading: harvard_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardbarn_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardbarn2_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardhemlock_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! ## Downloading: harvardlph_DB_1000_3day.csv ## -- Flagging outliers! ## -- Smoothing time series! ## -- Estimating transition dates! #&gt; Downloading: harvard_DB_1000_3day.csv #&gt; -- Flagging outliers! #&gt; -- Smoothing time series! #&gt; -- Estimating transition dates! Now look in your working directory. You have data! Read it in: # load the time series data but replace the csv filename with whatever you downloaded df &lt;- read.table(&quot;data/harvard_DB_1000_3day.csv&quot;, header = TRUE, sep = &quot;,&quot;) # read in the transidation date file td &lt;- read.table(&quot;data/harvard_DB_1000_3day_transition_dates.csv&quot;, header = TRUE, sep = &quot;,&quot;) Let’s take a look at the data: p = plot_ly() %&gt;% add_trace( data = df, x = ~ as.Date(date), y = ~ smooth_gcc_90, name = &#39;Smoothed GCC&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_markers( data=df, x ~ as.Date(date), y = ~gcc_90, name = &#39;GCC&#39;, type = &#39;scatter&#39;, color =&#39;#07A4B5&#39;, opacity=.5 ) p ## Warning: `arrange_()` is deprecated as of dplyr 0.7.0. ## Please use `arrange()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: Ignoring 3235 observations ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels What patterns do you notice? How would we go about determining say the start of spring? (SOS) 4.12.4 Threshold values Let’s subset the transition date (td) for each year when 25% of the greenness amplitude (of the 90^th) percentile is reached (threshold_25). # select the rising (spring dates) for 25% threshold of Gcc 90 spring &lt;- td[td$direction == &quot;rising&quot; &amp; td$gcc_value == &quot;gcc_90&quot;,] Let’s create a simple plot_ly line graph of the smooth Green Chromatic Coordinate (Gcc) and add points for transition dates: p = plot_ly() %&gt;% add_trace( data = df, x = ~ as.Date(date), y = ~ smooth_gcc_90, name = &#39;PhenoCam GCC&#39;, showlegend = TRUE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_markers( data= spring, x = ~ as.Date(spring$transition_25, origin = &quot;1970-01-01&quot;), y = ~ spring$threshold_25, type = &#39;scatter&#39;, mode = &#39;marker&#39;, name = &#39;Spring Dates&#39;) p Now we can see the transition date for each year of interest and the annual patterns of Gcc. However, if you want more control over the parameters used during processing, you can run through the three default processing steps as implemented in download_phenocam() and set parameters manually. Of particular interest is the option to specify your own threshold used in determining transition dates. What would be a reasonable threshold for peak greenness? Or autumn onset? Look at the ts dataset and phenocamr package and come up with a threshold. Use the same code to plot it here: # #some hint code # #what does &#39;rising&#39; versus &#39;falling&#39; denote? # #what threshold should you choose? # #td &lt;- phenophases(&quot;butte_GR_1000_3day.csv&quot;, # # internal = TRUE, # # upper_thresh = 0.8) fall &lt;- td[td$direction == &quot;falling&quot; &amp; td$gcc_value == &quot;gcc_90&quot;,] #Now generate a fall dataframe, what metrics should you use? 4.12.5 Comparing phenology across vegetation types Let’s load in a function to make plotting smoother. I’ve dropped it here in the markdown so that you can edit it and re-run it as you see fit: gcc_plot = function(gcc, spring, fall){ unix = &quot;1970-01-01&quot; p = plot_ly( data = gcc, x = ~ date, y = ~ gcc_90, showlegend = FALSE, type = &#39;scatter&#39;, mode = &#39;line&#39; ) %&gt;% add_trace( y = ~ smooth_gcc_90, mode = &quot;lines&quot;, line = list(width = 2, color = &quot;rgb(120,120,120)&quot;), name = &quot;Gcc loess fit&quot;, showlegend = TRUE ) %&gt;% # SOS spring # 10% add_trace( data = spring, x = ~ as.Date(transition_10), y = ~ threshold_10, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#7FFF00&quot;, symbol = &quot;circle&quot;), name = &quot;SOS (10%)&quot;, showlegend = TRUE ) %&gt;% add_segments(x = ~ as.Date(transition_10_lower_ci), xend = ~ as.Date(transition_10_upper_ci), # y = ~ 0, # yend = ~ 1, y = ~ threshold_10, yend = ~ threshold_10, line = list(color = &quot;#7FFF00&quot;), name = &quot;SOS (10%) - CI&quot; ) %&gt;% # 25 % add_trace( x = ~ as.Date(transition_25), y = ~ threshold_25, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#66CD00&quot;, symbol = &quot;square&quot;), showlegend = TRUE, name = &quot;SOS (25%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_25_lower_ci), xend = ~ as.Date(transition_25_upper_ci), y = ~ threshold_25, yend = ~ threshold_25, line = list(color = &quot;#66CD00&quot;), name = &quot;SOS (25%) - CI&quot; ) %&gt;% # 50 % add_trace( x = ~ as.Date(transition_50), y = ~ threshold_50, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#458B00&quot;, symbol = &quot;diamond&quot;), showlegend = TRUE, name = &quot;SOS (50%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_50_lower_ci), xend = ~ as.Date(transition_50_upper_ci), y = ~ threshold_50, yend = ~ threshold_50, line = list(color = &quot;#458B00&quot;), name = &quot;SOS (50%) - CI&quot; ) %&gt;% # EOS fall # 50% add_trace( data = fall, x = ~ as.Date(transition_50), y = ~ threshold_50, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#FFB90F&quot;, symbol = &quot;diamond&quot;), showlegend = TRUE, name = &quot;EOS (50%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_50_lower_ci), xend = ~ as.Date(transition_50_upper_ci), y = ~ threshold_50, yend = ~ threshold_50, line = list(color = &quot;#FFB90F&quot;), name = &quot;EOS (50%) - CI&quot; ) %&gt;% # 25 % add_trace( x = ~ as.Date(transition_25), y = ~ threshold_25, mode = &quot;markers&quot;, type = &quot;scatter&quot;, marker = list(color = &quot;#CD950C&quot;, symbol = &quot;square&quot;), showlegend = TRUE, name = &quot;EOS (25%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_25_lower_ci), xend = ~ as.Date(transition_25_upper_ci), y = ~ threshold_25, yend = ~ threshold_25, line = list(color = &quot;#CD950C&quot;), name = &quot;EOS (25%) - CI&quot; ) %&gt;% # 10 % add_trace( x = ~ as.Date(transition_10), y = ~ threshold_10, mode = &quot;markers&quot;, marker = list(color = &quot;#8B6508&quot;, symbol = &quot;circle&quot;), showlegend = TRUE, name = &quot;EOS (10%)&quot; ) %&gt;% add_segments(x = ~ as.Date(transition_10_lower_ci), xend = ~ as.Date(transition_10_upper_ci), y = ~ threshold_10, yend = ~ threshold_10, line = list(color = &quot;#8B6508&quot;), name = &quot;EOS (10%) - CI&quot; ) return (p) } gcc_p = gcc_plot(df, spring, fall) gcc_p &lt;- gcc_p %&gt;% layout( legend = list(x = 0.9, y = 0.9), xaxis = list( type = &#39;date&#39;, tickformat = &quot; %B&lt;br&gt;%Y&quot;, title=&#39;Year&#39;), yaxis = list( title = &#39;PhenoCam GCC&#39; )) gcc_p ## Warning: Ignoring 3235 observations ## Warning: `group_by_()` is deprecated as of dplyr 0.7.0. ## Please use `group_by()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: Can&#39;t display both discrete &amp; non-discrete data on same axis What is the difference in 25% greenness onset for your first site? #hint, look at the spring dataframe you just generated #some hints to get you started # d=spring$transition_25 # d=as.Date(d) # d #more code hints # dates_split &lt;- data.frame(date = d, # year = as.numeric(format(d, format = &quot;%Y&quot;)), # month = as.numeric(format(d, format = &quot;%m&quot;)), # day = as.numeric(format(d, format = &quot;%d&quot;))) Generate a plot of smoothed gcc and transition dates for your two sites and subplot them. What do you notice? #some hint code for subplotting in plot_ly: #p &lt;- subplot(p1, p2, nrows=2) #p 4.12.6 In Class Hands-on Coding: Comparing phenology of the same plant function type (PFT) across climate space As Dr. Richardson mentioned in his introduction lecture, the same plant functional types (e.g. grasslands) can have very different phenologogical cycles. Let’s pick two phenocam grassland sites: one from a tropical climate (kamuela), and one from an arid climate (konza): GrassSites=GrassSites[&#39;filter for your sites&#39;] Now pull data for those sites via phenocamr or the phenocamapi print(&#39;code here&#39;) ## [1] &quot;code here&quot; Now let’s create a subplot of your grasslands to compare phenology, some hint code below: #some hint code for subplotting in plot_ly: #p &lt;- subplot(p1, p2, nrows=2) #p Once you have a subplot of grassland phenology across 2 climates answer the following questions in your markdown: 1. What seasonal patterns do you see? 2. Do you think you set your thresholds correctly for transition dates/phenophases? How might that very as a function of climate? 3. What are the challenges of forecasting or modeling tropical versus arid grasslands? 4.13 Digital Repeat Photography Coding Lab 4.13.1 Quantifying haze and redness to evaluate California wildfires Pull mid-day imagery for September 1-7th, 2019 and 2020 for the canopy-level camera NEON.D17.SOAP.DP1.00033. Create a 2-panel plot showing those images in 2019 (left) and 2020 (right). Use the hazeR package to quantify the haze in each of those images. Print a summary of your results. Generate a density function RGB plot for your haziest image in 2020, and one for the same date in 2019. Create a 2-panel plot showing 2019 on the left and 2020 on the right. Pull timesseries data via the phenocamapi package. Calculate the difference in the rcc90 between 2019 and 2020 over the same time period as your images. Create a summary plot showing haze as a bar and the differenece in rcc90 from question 4 as a timersies. Answer the following questions: Does the hazeR package pick up smokey images? If you were to use color coordinates, which color band would be most useful to highlight smoke and why? Optional Bonus Points: Repeat the above calculations for the understory camera on the same tower NEON.D17.SOAP.DP1.00042 and produce the same plots. Which camera is ‘better’ and capturing wildfire? Why do you think that is so? 4.14 Intro to PhenoCam Culmination Activity Write up a 1-page summary of a project that you might want to explore using PhenoCam data over the duration of this course. Include the types of PhenoCam (and other data) that you will need to implement this project. Save this summary as you will be refining and adding to your ideas over the course of the semester. "],
["flux-measurements-inter-operability.html", "Chapter 5 Flux Measurements &amp; Inter-Operability 5.1 Learning Objectives 5.2 Eddy Co_variance Data: What does it actually measure? 5.3 QA/QC Flags 5.4 Examples of Other Flux Networks: AMERIFLUX &amp; FLUXNET 5.5 The Power of Networked Ecology: Bridging to AMERIFLUX and Beyond 5.6 Hands On: Introduction to working with NEON eddy flux data 5.7 Exercises", " Chapter 5 Flux Measurements &amp; Inter-Operability Estimated Time: 3 hours Course participants: As you review this information, please consider the final course project that you will work on at the over this semester. At the end of this section, you will document an initial research question or idea and associated data needed to address that question, that you may want to explore while pursuing this course. 5.1 Learning Objectives At the end of this activity, you will be able to: Define the eddy covariance method Articulate various carbon storage and flux terms Understand the structure of bundled eddy covariance data Be able to process NEON flux data such that it is comparable with AmeriFlux 5.2 Eddy Co_variance Data: What does it actually measure? 5.2.1 Example Eddy Site 5.3 QA/QC Flags 5.4 Examples of Other Flux Networks: AMERIFLUX &amp; FLUXNET AmeriFlux is a network of PI-managed sites measuring ecosystem CO2, water, and energy fluxes in North, Central and South America. It was established to connect research on field sites representing major climate and ecological biomes, including tundra, grasslands, savanna, crops, and conifer, deciduous, and tropical forests. As a grassroots, investigator-driven network, the AmeriFlux community has tailored instrumentation to suit each unique ecosystem. This “coalition of the willing” is diverse in its interests, use of technologies and collaborative approaches. As a result, the AmeriFlux Network continually pioneers new ground. The network was launched in 1996, after an international workshop on flux measurements in La Thuile, Italy, in 1995, where some of the first year-long flux measurements were presented. Early support for the network came from many sources, including the U.S. Department of Energy’s Terrestrial Carbon Program, the DOE’s National Institute of Global Environmental Change (NIGEC), NASA, NOAA and the US Forest Service. The network grew from about 15 sites in 1997 to more than 110 active sites registered today. Sixty-one other sites, now inactive, have flux data stored in the network’s database. In 2012, the U.S. DOE established the AmeriFlux Management Project (AMP) at Lawrence Berkeley National Laboratory (LBNL) to support the broad AmeriFlux community and the AmeriFlux sites. View the AMERIFLUX Network-at-a-Glance AmeriFlux is now one of the DOE Office of Biological and Environmental Research’s (BER) best-known and most highly regarded brands in climate and ecological research. AmeriFlux datasets, and the understanding derived from them, provide crucial linkages between terrestrial ecosystem processes and climate-relevant responses at landscape, regional, and continental scales. 5.5 The Power of Networked Ecology: Bridging to AMERIFLUX and Beyond Given that AmeriFlux has been collecting and coordinating eddy covariance data across the Americas since 1996. The network provides a common platform for data sharing and collaboration for organizations and individual private investigators collecting flux tower data. There are now &gt;470 registered flux tower sites in North, Central, and South America in the AmeriFlux network, many operated by individual researchers or universities. The towers collect eddy covariance data across a broad range of climate zones and ecosystem types, from Chile to Alaska and everywhere in between. Now, data from the NEON project is available through the AmeriFlux data portal. The NEON team has formatted data from the NEON flux towers to make it fully compatible with AmeriFlux data. This allows researchers to view, download and analyze data from the NEON flux towers alongside data from all of the other flux towers in the AmeriFlux network. With 47 flux towers at terrestrial field sites across the U.S., the NEON program is now the largest single contributor of flux tower data to the AmeriFlux network. NEON field sites are located in 20 ecoclimatic zones across the U.S., representing many distinct ecosystems. Eddy covariance data will be served using the same methods at each site for the entire 30-year life of the Observatory, allowing for unprecedented comparability across both time and space. 5.6 Hands On: Introduction to working with NEON eddy flux data 5.6.1 Setup Start by installing and loading packages and setting options. To work with the NEON flux data, we need the rhdf5 package, which is hosted on Bioconductor, and requires a different installation process than CRAN packages: install.packages(&#39;BiocManager&#39;) BiocManager::install(&#39;rhdf5&#39;) options(stringsAsFactors=F) library(neonUtilities) Use the zipsByProduct() function from the neonUtilities package to download flux data from two sites and two months. The transformations and functions below will work on any time range and site(s), but two sites and two months allows us to see all the available functionality while minimizing download size. Inputs to the zipsByProduct() function: dpID: DP4.00200.001, the bundled eddy covariance product package: basic (the expanded package is not covered in this tutorial) site: NIWO = Niwot Ridge and HARV = Harvard Forest startdate: 2018-06 (both dates are inclusive) enddate: 2018-07 (both dates are inclusive) savepath: modify this to something logical on your machine check.size: T if you want to see file size before downloading, otherwise F The download may take a while, especially if you’re on a slow network. zipsByProduct(dpID=&quot;DP4.00200.001&quot;, package=&quot;basic&quot;, site=c(&quot;NIWO&quot;, &quot;HARV&quot;), startdate=&quot;2018-06&quot;, enddate=&quot;2018-07&quot;, savepath=&quot;./data&quot;, check.size=F) ## Finding available files ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## ## Downloading files totaling approximately 298.8 MiB ## Warning in dir.create(filepath): &#39;./data/filesToStack00200&#39; already exists ## Downloading 4 files ## | | | 0% | |======================= | 33% | |=============================================== | 67% | |======================================================================| 100% ## 4 files successfully downloaded to ./data/filesToStack00200 5.6.2 Data Levels There are five levels of data contained in the eddy flux bundle. For full details, refer to the NEON algorithm document. Briefly, the data levels are: Level 0’ (dp0p): Calibrated raw observations Level 1 (dp01): Time-aggregated observations, e.g. 30-minute mean gas concentrations Level 2 (dp02): Time-interpolated data, e.g. rate of change of a gas concentration Level 3 (dp03): Spatially interpolated data, i.e. vertical profiles Level 4 (dp04): Fluxes The dp0p data are available in the expanded data package and are beyond the scope of this tutorial. The dp02 and dp03 data are used in storage calculations, and the dp04 data include both the storage and turbulent components. Since many users will want to focus on the net flux data, we’ll start there. 5.6.3 Extract Level 4 data (Fluxes!) To extract the Level 4 data from the HDF5 files and merge them into a single table, we’ll use the stackEddy() function from the neonUtilities package. stackEddy() requires two inputs: filepath: Path to a file or folder, which can be any one of: A zip file of eddy flux data downloaded from the NEON data portal A folder of eddy flux data downloaded by the zipsByProduct() function The folder of files resulting from unzipping either of 1 or 2 A single HDF5 file of NEON eddy flux data level: dp01-4 Input the filepath you downloaded to using zipsByProduct() earlier, including the filestoStack00200 folder created by the function, and dp04: flux &lt;- stackEddy(filepath=&quot;./data/filesToStack00200&quot;, level=&quot;dp04&quot;) ## Extracting data ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## Stacking data tables by month ## | | | 0% | |== | 4% | |===== | 7% | |======== | 11% | |========== | 14% | |============ | 18% | |=============== | 21% | |================== | 25% | |==================== | 29% | |====================== | 32% | |========================= | 36% | |============================ | 39% | |============================== | 43% | |================================ | 46% | |=================================== | 50% | |====================================== | 54% | |======================================== | 57% | |========================================== | 61% | |============================================= | 64% | |================================================ | 68% | |================================================== | 71% | |==================================================== | 75% | |======================================================= | 79% | |========================================================== | 82% | |============================================================ | 86% | |============================================================== | 89% | |================================================================= | 93% | |==================================================================== | 96% | |======================================================================| 100% ## Joining data variables ## | | | 0% | |== | 4% | |===== | 7% | |======== | 11% | |========== | 14% | |============ | 18% | |=============== | 21% | |================== | 25% | |==================== | 29% | |====================== | 32% | |========================= | 36% | |============================ | 39% | |============================== | 43% | |================================ | 46% | |=================================== | 50% | |====================================== | 54% | |======================================== | 57% | |========================================== | 61% | |============================================= | 64% | |================================================ | 68% | |================================================== | 71% | |==================================================== | 75% | |======================================================= | 79% | |========================================================== | 82% | |============================================================ | 86% | |============================================================== | 89% | |================================================================= | 93% | |==================================================================== | 96% | |======================================================================| 100% We now have an object called flux. It’s a named list containing four tables: one table for each site’s data, and variables and objDesc tables. names(flux) ## [1] &quot;HARV&quot; &quot;NIWO&quot; &quot;variables&quot; &quot;objDesc&quot; Let’s look at the contents of one of the site data files: knitr::kable(head(flux$NIWO)) timeBgn timeEnd data.fluxCo2.nsae.flux data.fluxCo2.stor.flux data.fluxCo2.turb.flux data.fluxH2o.nsae.flux data.fluxH2o.stor.flux data.fluxH2o.turb.flux data.fluxMome.turb.veloFric data.fluxTemp.nsae.flux data.fluxTemp.stor.flux data.fluxTemp.turb.flux data.foot.stat.angZaxsErth data.foot.stat.distReso data.foot.stat.veloYaxsHorSd data.foot.stat.veloZaxsHorSd data.foot.stat.veloFric data.foot.stat.distZaxsMeasDisp data.foot.stat.distZaxsRgh data.foot.stat.distZaxsAbl data.foot.stat.distXaxs90 data.foot.stat.distXaxsMax data.foot.stat.distYaxs90 qfqm.fluxCo2.stor.qfFinl qfqm.fluxH2o.stor.qfFinl qfqm.fluxTemp.stor.qfFinl 2018-06-01T00:00:00.000Z 2018-06-01T00:29:59.000Z 0.1111935 -0.0619119 0.1731053 19.401823 3.2511265 16.150697 0.1970704 4.1712006 -1.4227119 5.593913 94.90147 8.34 0.7734536 0.2708072 0.2 8.34 0.0322148 1000 333.60 133.44 25.02 1 1 0 2018-06-01T00:30:00.000Z 2018-06-01T00:59:59.000Z 0.9328922 0.0853412 0.8475510 10.444936 -1.1768333 11.621770 0.1969972 -0.9163691 0.3331562 -1.249525 354.70503 8.34 0.8450318 0.2300000 0.2 8.34 0.3300708 1000 258.54 108.42 50.04 1 1 0 2018-06-01T01:00:00.000Z 2018-06-01T01:29:59.000Z 0.4673682 0.0217722 0.4455960 5.140617 -4.3112673 9.451884 0.0651821 -2.9814957 0.1825849 -3.164081 358.86732 8.34 1.2219162 0.2300000 0.2 8.34 0.1287607 1000 308.58 125.10 58.38 1 1 0 2018-06-01T01:30:00.000Z 2018-06-01T01:59:59.000Z 0.7263614 0.2494437 0.4769178 9.017467 0.1980776 8.819389 0.1296400 -13.3556222 -2.4317615 -10.923861 137.68858 8.34 0.7325131 0.2300000 0.2 8.34 0.8340000 1000 208.50 83.40 75.06 1 1 0 2018-06-01T02:00:00.000Z 2018-06-01T02:29:59.000Z 0.4740572 0.2252436 0.2488136 3.180385 0.1316297 3.048756 0.1746071 -5.3406503 -0.7324937 -4.608157 188.33767 8.34 0.7093743 0.2300000 0.2 8.34 0.8340000 1000 208.50 83.40 66.72 1 1 0 2018-06-01T02:30:00.000Z 2018-06-01T02:59:59.000Z 0.8807022 0.0707801 0.8099221 4.398761 -0.2989443 4.697706 0.1047797 -7.2739206 -1.8616349 -5.412286 183.15582 8.34 0.3791676 0.2300000 0.2 8.34 0.8340000 1000 208.50 83.40 41.70 1 1 0 The variables and objDesc tables can help you interpret the column headers in the data table. The objDesc table contains definitions for many of the terms used in the eddy flux data product, but it isn’t complete. To get the terms of interest, we’ll break up the column headers into individual terms and look for them in the objDesc table: term &lt;- unlist(strsplit(names(flux$NIWO), split=&quot;.&quot;, fixed=T)) flux$objDesc[which(flux$objDesc$Object %in% term),] ## Object ## 138 angZaxsErth ## 171 data ## 343 qfFinl ## 420 qfqm ## 604 timeBgn ## 605 timeEnd ## Description ## 138 Wind direction ## 171 Represents data fields ## 343 The final quality flag indicating if the data are valid for the given aggregation period (1=fail, 0=pass) ## 420 Quality flag and quality metrics, represents quality flags and quality metrics that accompany the provided data ## 604 The beginning time of the aggregation period ## 605 The end time of the aggregation period knitr::kable(term) x timeBgn timeEnd data fluxCo2 nsae flux data fluxCo2 stor flux data fluxCo2 turb flux data fluxH2o nsae flux data fluxH2o stor flux data fluxH2o turb flux data fluxMome turb veloFric data fluxTemp nsae flux data fluxTemp stor flux data fluxTemp turb flux data foot stat angZaxsErth data foot stat distReso data foot stat veloYaxsHorSd data foot stat veloZaxsHorSd data foot stat veloFric data foot stat distZaxsMeasDisp data foot stat distZaxsRgh data foot stat distZaxsAbl data foot stat distXaxs90 data foot stat distXaxsMax data foot stat distYaxs90 qfqm fluxCo2 stor qfFinl qfqm fluxH2o stor qfFinl qfqm fluxTemp stor qfFinl For the terms that aren’t captured here, fluxCo2, fluxH2o, and fluxTemp are self-explanatory. The flux components are turb: Turbulent flux stor: Storage nsae: Net surface-atmosphere exchange The variables table contains the units for each field: knitr::kable(flux$variables) category system variable stat units data fluxCo2 nsae umolCo2 m-2 s-1 data fluxCo2 stor umolCo2 m-2 s-1 data fluxCo2 turb umolCo2 m-2 s-1 data fluxH2o nsae W m-2 data fluxH2o stor W m-2 data fluxH2o turb W m-2 data fluxMome turb m s-1 data fluxTemp nsae W m-2 data fluxTemp stor W m-2 data fluxTemp turb W m-2 data foot stat angZaxsErth deg data foot stat distReso m data foot stat veloYaxsHorSd m s-1 data foot stat veloZaxsHorSd m s-1 data foot stat veloFric m s-1 data foot stat distZaxsMeasDisp m data foot stat distZaxsRgh m data foot stat distZaxsAbl m data foot stat distXaxs90 m data foot stat distXaxsMax m data foot stat distYaxs90 m qfqm fluxCo2 stor NA qfqm fluxH2o stor NA qfqm fluxTemp stor NA Let’s plot some data! First, we’ll need to convert the time stamps to an R date-time format (right now they’re just character fields). 5.6.4 Time stamps NEON sensor data come with time stamps for both the start and end of the averaging period. Depending on the analysis you’re doing, you may want to use one or the other; for general plotting, re-formatting, and transformations, I prefer to use the start time, because there are some small inconsistencies between data products in a few of the end time stamps. Note that all NEON data use UTC time, noted as tz=&quot;GMT&quot; in the code below. This is true across NEON’s instrumented, observational, and airborne measurements. When working with NEON data, it’s best to keep everything in UTC as much as possible, otherwise it’s very easy to end up with data in mismatched times, which can cause insidious and hard-to-detect problems. Be sure to include the tz argument in all the lines of code below - if there is no time zone specified, R will default to the local time zone it detects on your operating system. timeB &lt;- as.POSIXct(flux$NIWO$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%S&quot;, tz=&quot;GMT&quot;) flux$NIWO &lt;- cbind(timeB, flux$NIWO) plot(flux$NIWO$data.fluxCo2.nsae.flux~timeB, pch=&quot;.&quot;, xlab=&quot;Date&quot;, ylab=&quot;CO2 flux&quot;, xaxt=&quot;n&quot;) axis.POSIXct(1, x=timeB, format=&quot;%Y-%m-%d&quot;) Like a lot of flux data, these data have some stray spikes, but there is a clear diurnal pattern going into the growing season. Let’s trim down to just two days of data to see a few other details. plot(flux$NIWO$data.fluxCo2.nsae.flux~timeB, pch=20, xlab=&quot;Date&quot;, ylab=&quot;CO2 flux&quot;, xlim=c(as.POSIXct(&quot;2018-07-07&quot;, tz=&quot;GMT&quot;), as.POSIXct(&quot;2018-07-09&quot;, tz=&quot;GMT&quot;)), ylim=c(-20,20), xaxt=&quot;n&quot;) axis.POSIXct(1, x=timeB, format=&quot;%Y-%m-%d %H:%M:%S&quot;) Note the timing of C uptake; the UTC time zone is clear here, where uptake occurs at times that appear to be during the night. 5.6.5 Merge flux data with other sensor data Many of the data sets we would use to interpret and model flux data are measured as part of the NEON project, but are not present in the eddy flux data product bundle. In this section, we’ll download PAR data and merge them with the flux data; the steps taken here can be applied to any of the NEON instrumented (IS) data products. 5.6.5.1 Download PAR data To get NEON PAR data, use the loadByProduct() function from the neonUtilities package. loadByProduct() takes the same inputs as zipsByProduct(), but it loads the downloaded data directly into the current R environment. Let’s download PAR data matching the Niwot Ridge flux data. The inputs needed are: dpID: DP1.00024.001 site: NIWO startdate: 2018-06 enddate: 2018-07 package: basic avg: 30 The new input here is avg=30, which downloads only the 30-minute data. Since the flux data are at a 30-minute resolution, we can save on download time by disregarding the 1-minute data files (which are of course 30 times larger). The avg input can be left off if you want to download all available averaging intervals. pr &lt;- loadByProduct(&quot;DP1.00024.001&quot;, site=&quot;NIWO&quot;, avg=30, startdate=&quot;2018-06&quot;, enddate=&quot;2018-07&quot;, package=&quot;basic&quot;, check.size=F) ## Finding available files ## | | | 0% | |=================================== | 50% | |======================================================================| 100% ## ## Downloading files totaling approximately 1.2 MiB ## Downloading 11 files ## | | | 0% | |======= | 10% | |============== | 20% | |===================== | 30% | |============================ | 40% | |=================================== | 50% | |========================================== | 60% | |================================================= | 70% | |======================================================== | 80% | |=============================================================== | 90% | |======================================================================| 100% ## ## Stacking operation across a single core. ## Stacking table PARPAR_30min ## Merged the most recent publication of sensor position files for each site and saved to /stackedFiles ## Copied the most recent publication of variable definition file to /stackedFiles ## Finished: Stacked 1 data tables and 2 metadata tables! ## Stacking took 0.682564 secs ## All unzipped monthly data folders have been removed. pr is another named list, and again, metadata and units can be found in the variables table. The PARPAR_30min table contains a verticalPosition field. This field indicates the position on the tower, with 10 being the first tower level, and 20, 30, etc going up the tower. 5.6.5.2 Join PAR to flux data We’ll connect PAR data from the tower top to the flux data. pr.top &lt;- pr$PARPAR_30min[which(pr$PARPAR_30min$verticalPosition== max(pr$PARPAR_30min$verticalPosition)),] loadByProduct() automatically converts time stamps when it reads the data, so here we just need to indicate which time field to use to merge the flux and PAR data. timeB &lt;- pr.top$startDateTime pr.top &lt;- cbind(timeB, pr.top) And merge the two datasets: fx.pr &lt;- merge(pr.top, flux$NIWO, by=&quot;timeB&quot;) plot(fx.pr$data.fluxCo2.nsae.flux~fx.pr$PARMean, pch=&quot;.&quot;, ylim=c(-20,20), xlab=&quot;PAR&quot;, ylab=&quot;CO2 flux&quot;) If you’re interested in data in the eddy covariance bundle besides the net flux data, the rest of this tutorial will guide you through how to get those data out of the bundle. 5.6.6 Vertical profile data (Level 3) The Level 3 (dp03) data are the spatially interpolated profiles of the rates of change of CO2, H2O, and temperature. Extract the Level 3 data from the HDF5 file using stackEddy() with the same syntax as for the Level 4 data. prof &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp03&quot;) ## Extracting data ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## Stacking data tables by month ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% ## Joining data variables ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% knitr::kable(head(prof$NIWO)) timeBgn timeEnd data.co2Stor.rateRtioMoleDryCo2.X0.1.m data.co2Stor.rateRtioMoleDryCo2.X0.2.m data.co2Stor.rateRtioMoleDryCo2.X0.3.m data.co2Stor.rateRtioMoleDryCo2.X0.4.m data.co2Stor.rateRtioMoleDryCo2.X0.5.m data.co2Stor.rateRtioMoleDryCo2.X0.6.m data.co2Stor.rateRtioMoleDryCo2.X0.7.m data.co2Stor.rateRtioMoleDryCo2.X0.8.m data.co2Stor.rateRtioMoleDryCo2.X0.9.m data.co2Stor.rateRtioMoleDryCo2.X1.m data.co2Stor.rateRtioMoleDryCo2.X1.1.m data.co2Stor.rateRtioMoleDryCo2.X1.2.m data.co2Stor.rateRtioMoleDryCo2.X1.3.m data.co2Stor.rateRtioMoleDryCo2.X1.4.m data.co2Stor.rateRtioMoleDryCo2.X1.5.m data.co2Stor.rateRtioMoleDryCo2.X1.6.m data.co2Stor.rateRtioMoleDryCo2.X1.7.m data.co2Stor.rateRtioMoleDryCo2.X1.8.m data.co2Stor.rateRtioMoleDryCo2.X1.9.m data.co2Stor.rateRtioMoleDryCo2.X2.m data.co2Stor.rateRtioMoleDryCo2.X2.1.m data.co2Stor.rateRtioMoleDryCo2.X2.2.m data.co2Stor.rateRtioMoleDryCo2.X2.3.m data.co2Stor.rateRtioMoleDryCo2.X2.4.m data.co2Stor.rateRtioMoleDryCo2.X2.5.m data.co2Stor.rateRtioMoleDryCo2.X2.6.m data.co2Stor.rateRtioMoleDryCo2.X2.7.m data.co2Stor.rateRtioMoleDryCo2.X2.8.m data.co2Stor.rateRtioMoleDryCo2.X2.9.m data.co2Stor.rateRtioMoleDryCo2.X3.m data.co2Stor.rateRtioMoleDryCo2.X3.1.m data.co2Stor.rateRtioMoleDryCo2.X3.2.m data.co2Stor.rateRtioMoleDryCo2.X3.3.m data.co2Stor.rateRtioMoleDryCo2.X3.4.m data.co2Stor.rateRtioMoleDryCo2.X3.5.m data.co2Stor.rateRtioMoleDryCo2.X3.6.m data.co2Stor.rateRtioMoleDryCo2.X3.7.m data.co2Stor.rateRtioMoleDryCo2.X3.8.m data.co2Stor.rateRtioMoleDryCo2.X3.9.m data.co2Stor.rateRtioMoleDryCo2.X4.m data.co2Stor.rateRtioMoleDryCo2.X4.1.m data.co2Stor.rateRtioMoleDryCo2.X4.2.m data.co2Stor.rateRtioMoleDryCo2.X4.3.m data.co2Stor.rateRtioMoleDryCo2.X4.4.m data.co2Stor.rateRtioMoleDryCo2.X4.5.m data.co2Stor.rateRtioMoleDryCo2.X4.6.m data.co2Stor.rateRtioMoleDryCo2.X4.7.m data.co2Stor.rateRtioMoleDryCo2.X4.8.m data.co2Stor.rateRtioMoleDryCo2.X4.9.m data.co2Stor.rateRtioMoleDryCo2.X5.m data.co2Stor.rateRtioMoleDryCo2.X5.1.m data.co2Stor.rateRtioMoleDryCo2.X5.2.m data.co2Stor.rateRtioMoleDryCo2.X5.3.m data.co2Stor.rateRtioMoleDryCo2.X5.4.m data.co2Stor.rateRtioMoleDryCo2.X5.5.m data.co2Stor.rateRtioMoleDryCo2.X5.6.m data.co2Stor.rateRtioMoleDryCo2.X5.7.m data.co2Stor.rateRtioMoleDryCo2.X5.8.m data.co2Stor.rateRtioMoleDryCo2.X5.9.m data.co2Stor.rateRtioMoleDryCo2.X6.m data.co2Stor.rateRtioMoleDryCo2.X6.1.m data.co2Stor.rateRtioMoleDryCo2.X6.2.m data.co2Stor.rateRtioMoleDryCo2.X6.3.m data.co2Stor.rateRtioMoleDryCo2.X6.4.m data.co2Stor.rateRtioMoleDryCo2.X6.5.m data.co2Stor.rateRtioMoleDryCo2.X6.6.m data.co2Stor.rateRtioMoleDryCo2.X6.7.m data.co2Stor.rateRtioMoleDryCo2.X6.8.m data.co2Stor.rateRtioMoleDryCo2.X6.9.m data.co2Stor.rateRtioMoleDryCo2.X7.m data.co2Stor.rateRtioMoleDryCo2.X7.1.m data.co2Stor.rateRtioMoleDryCo2.X7.2.m data.co2Stor.rateRtioMoleDryCo2.X7.3.m data.co2Stor.rateRtioMoleDryCo2.X7.4.m data.co2Stor.rateRtioMoleDryCo2.X7.5.m data.co2Stor.rateRtioMoleDryCo2.X7.6.m data.co2Stor.rateRtioMoleDryCo2.X7.7.m data.co2Stor.rateRtioMoleDryCo2.X7.8.m data.co2Stor.rateRtioMoleDryCo2.X7.9.m data.co2Stor.rateRtioMoleDryCo2.X8.m data.co2Stor.rateRtioMoleDryCo2.X8.1.m data.co2Stor.rateRtioMoleDryCo2.X8.2.m data.co2Stor.rateRtioMoleDryCo2.X8.3.m data.co2Stor.rateRtioMoleDryCo2.X8.4.m data.h2oStor.rateRtioMoleDryH2o.X0.1.m data.h2oStor.rateRtioMoleDryH2o.X0.2.m data.h2oStor.rateRtioMoleDryH2o.X0.3.m data.h2oStor.rateRtioMoleDryH2o.X0.4.m data.h2oStor.rateRtioMoleDryH2o.X0.5.m data.h2oStor.rateRtioMoleDryH2o.X0.6.m data.h2oStor.rateRtioMoleDryH2o.X0.7.m data.h2oStor.rateRtioMoleDryH2o.X0.8.m data.h2oStor.rateRtioMoleDryH2o.X0.9.m data.h2oStor.rateRtioMoleDryH2o.X1.m data.h2oStor.rateRtioMoleDryH2o.X1.1.m data.h2oStor.rateRtioMoleDryH2o.X1.2.m data.h2oStor.rateRtioMoleDryH2o.X1.3.m data.h2oStor.rateRtioMoleDryH2o.X1.4.m data.h2oStor.rateRtioMoleDryH2o.X1.5.m data.h2oStor.rateRtioMoleDryH2o.X1.6.m data.h2oStor.rateRtioMoleDryH2o.X1.7.m data.h2oStor.rateRtioMoleDryH2o.X1.8.m data.h2oStor.rateRtioMoleDryH2o.X1.9.m data.h2oStor.rateRtioMoleDryH2o.X2.m data.h2oStor.rateRtioMoleDryH2o.X2.1.m data.h2oStor.rateRtioMoleDryH2o.X2.2.m data.h2oStor.rateRtioMoleDryH2o.X2.3.m data.h2oStor.rateRtioMoleDryH2o.X2.4.m data.h2oStor.rateRtioMoleDryH2o.X2.5.m data.h2oStor.rateRtioMoleDryH2o.X2.6.m data.h2oStor.rateRtioMoleDryH2o.X2.7.m data.h2oStor.rateRtioMoleDryH2o.X2.8.m data.h2oStor.rateRtioMoleDryH2o.X2.9.m data.h2oStor.rateRtioMoleDryH2o.X3.m data.h2oStor.rateRtioMoleDryH2o.X3.1.m data.h2oStor.rateRtioMoleDryH2o.X3.2.m data.h2oStor.rateRtioMoleDryH2o.X3.3.m data.h2oStor.rateRtioMoleDryH2o.X3.4.m data.h2oStor.rateRtioMoleDryH2o.X3.5.m data.h2oStor.rateRtioMoleDryH2o.X3.6.m data.h2oStor.rateRtioMoleDryH2o.X3.7.m data.h2oStor.rateRtioMoleDryH2o.X3.8.m data.h2oStor.rateRtioMoleDryH2o.X3.9.m data.h2oStor.rateRtioMoleDryH2o.X4.m data.h2oStor.rateRtioMoleDryH2o.X4.1.m data.h2oStor.rateRtioMoleDryH2o.X4.2.m data.h2oStor.rateRtioMoleDryH2o.X4.3.m data.h2oStor.rateRtioMoleDryH2o.X4.4.m data.h2oStor.rateRtioMoleDryH2o.X4.5.m data.h2oStor.rateRtioMoleDryH2o.X4.6.m data.h2oStor.rateRtioMoleDryH2o.X4.7.m data.h2oStor.rateRtioMoleDryH2o.X4.8.m data.h2oStor.rateRtioMoleDryH2o.X4.9.m data.h2oStor.rateRtioMoleDryH2o.X5.m data.h2oStor.rateRtioMoleDryH2o.X5.1.m data.h2oStor.rateRtioMoleDryH2o.X5.2.m data.h2oStor.rateRtioMoleDryH2o.X5.3.m data.h2oStor.rateRtioMoleDryH2o.X5.4.m data.h2oStor.rateRtioMoleDryH2o.X5.5.m data.h2oStor.rateRtioMoleDryH2o.X5.6.m data.h2oStor.rateRtioMoleDryH2o.X5.7.m data.h2oStor.rateRtioMoleDryH2o.X5.8.m data.h2oStor.rateRtioMoleDryH2o.X5.9.m data.h2oStor.rateRtioMoleDryH2o.X6.m data.h2oStor.rateRtioMoleDryH2o.X6.1.m data.h2oStor.rateRtioMoleDryH2o.X6.2.m data.h2oStor.rateRtioMoleDryH2o.X6.3.m data.h2oStor.rateRtioMoleDryH2o.X6.4.m data.h2oStor.rateRtioMoleDryH2o.X6.5.m data.h2oStor.rateRtioMoleDryH2o.X6.6.m data.h2oStor.rateRtioMoleDryH2o.X6.7.m data.h2oStor.rateRtioMoleDryH2o.X6.8.m data.h2oStor.rateRtioMoleDryH2o.X6.9.m data.h2oStor.rateRtioMoleDryH2o.X7.m data.h2oStor.rateRtioMoleDryH2o.X7.1.m data.h2oStor.rateRtioMoleDryH2o.X7.2.m data.h2oStor.rateRtioMoleDryH2o.X7.3.m data.h2oStor.rateRtioMoleDryH2o.X7.4.m data.h2oStor.rateRtioMoleDryH2o.X7.5.m data.h2oStor.rateRtioMoleDryH2o.X7.6.m data.h2oStor.rateRtioMoleDryH2o.X7.7.m data.h2oStor.rateRtioMoleDryH2o.X7.8.m data.h2oStor.rateRtioMoleDryH2o.X7.9.m data.h2oStor.rateRtioMoleDryH2o.X8.m data.h2oStor.rateRtioMoleDryH2o.X8.1.m data.h2oStor.rateRtioMoleDryH2o.X8.2.m data.h2oStor.rateRtioMoleDryH2o.X8.3.m data.h2oStor.rateRtioMoleDryH2o.X8.4.m data.tempStor.rateTemp.X0.1.m data.tempStor.rateTemp.X0.2.m data.tempStor.rateTemp.X0.3.m data.tempStor.rateTemp.X0.4.m data.tempStor.rateTemp.X0.5.m data.tempStor.rateTemp.X0.6.m data.tempStor.rateTemp.X0.7.m data.tempStor.rateTemp.X0.8.m data.tempStor.rateTemp.X0.9.m data.tempStor.rateTemp.X1.m data.tempStor.rateTemp.X1.1.m data.tempStor.rateTemp.X1.2.m data.tempStor.rateTemp.X1.3.m data.tempStor.rateTemp.X1.4.m data.tempStor.rateTemp.X1.5.m data.tempStor.rateTemp.X1.6.m data.tempStor.rateTemp.X1.7.m data.tempStor.rateTemp.X1.8.m data.tempStor.rateTemp.X1.9.m data.tempStor.rateTemp.X2.m data.tempStor.rateTemp.X2.1.m data.tempStor.rateTemp.X2.2.m data.tempStor.rateTemp.X2.3.m data.tempStor.rateTemp.X2.4.m data.tempStor.rateTemp.X2.5.m data.tempStor.rateTemp.X2.6.m data.tempStor.rateTemp.X2.7.m data.tempStor.rateTemp.X2.8.m data.tempStor.rateTemp.X2.9.m data.tempStor.rateTemp.X3.m data.tempStor.rateTemp.X3.1.m data.tempStor.rateTemp.X3.2.m data.tempStor.rateTemp.X3.3.m data.tempStor.rateTemp.X3.4.m data.tempStor.rateTemp.X3.5.m data.tempStor.rateTemp.X3.6.m data.tempStor.rateTemp.X3.7.m data.tempStor.rateTemp.X3.8.m data.tempStor.rateTemp.X3.9.m data.tempStor.rateTemp.X4.m data.tempStor.rateTemp.X4.1.m data.tempStor.rateTemp.X4.2.m data.tempStor.rateTemp.X4.3.m data.tempStor.rateTemp.X4.4.m data.tempStor.rateTemp.X4.5.m data.tempStor.rateTemp.X4.6.m data.tempStor.rateTemp.X4.7.m data.tempStor.rateTemp.X4.8.m data.tempStor.rateTemp.X4.9.m data.tempStor.rateTemp.X5.m data.tempStor.rateTemp.X5.1.m data.tempStor.rateTemp.X5.2.m data.tempStor.rateTemp.X5.3.m data.tempStor.rateTemp.X5.4.m data.tempStor.rateTemp.X5.5.m data.tempStor.rateTemp.X5.6.m data.tempStor.rateTemp.X5.7.m data.tempStor.rateTemp.X5.8.m data.tempStor.rateTemp.X5.9.m data.tempStor.rateTemp.X6.m data.tempStor.rateTemp.X6.1.m data.tempStor.rateTemp.X6.2.m data.tempStor.rateTemp.X6.3.m data.tempStor.rateTemp.X6.4.m data.tempStor.rateTemp.X6.5.m data.tempStor.rateTemp.X6.6.m data.tempStor.rateTemp.X6.7.m data.tempStor.rateTemp.X6.8.m data.tempStor.rateTemp.X6.9.m data.tempStor.rateTemp.X7.m data.tempStor.rateTemp.X7.1.m data.tempStor.rateTemp.X7.2.m data.tempStor.rateTemp.X7.3.m data.tempStor.rateTemp.X7.4.m data.tempStor.rateTemp.X7.5.m data.tempStor.rateTemp.X7.6.m data.tempStor.rateTemp.X7.7.m data.tempStor.rateTemp.X7.8.m data.tempStor.rateTemp.X7.9.m data.tempStor.rateTemp.X8.m data.tempStor.rateTemp.X8.1.m data.tempStor.rateTemp.X8.2.m data.tempStor.rateTemp.X8.3.m data.tempStor.rateTemp.X8.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X0.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X1.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X2.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X3.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X4.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X5.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X6.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.4.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.5.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.6.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.7.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.8.m qfqm.co2Stor.rateRtioMoleDryCo2.X7.9.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.1.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.2.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.3.m qfqm.co2Stor.rateRtioMoleDryCo2.X8.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X0.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X1.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X2.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X3.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X4.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X5.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X6.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.4.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.5.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.6.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.7.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.8.m qfqm.h2oStor.rateRtioMoleDryH2o.X7.9.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.1.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.2.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.3.m qfqm.h2oStor.rateRtioMoleDryH2o.X8.4.m qfqm.tempStor.rateTemp.X0.1.m qfqm.tempStor.rateTemp.X0.2.m qfqm.tempStor.rateTemp.X0.3.m qfqm.tempStor.rateTemp.X0.4.m qfqm.tempStor.rateTemp.X0.5.m qfqm.tempStor.rateTemp.X0.6.m qfqm.tempStor.rateTemp.X0.7.m qfqm.tempStor.rateTemp.X0.8.m qfqm.tempStor.rateTemp.X0.9.m qfqm.tempStor.rateTemp.X1.m qfqm.tempStor.rateTemp.X1.1.m qfqm.tempStor.rateTemp.X1.2.m qfqm.tempStor.rateTemp.X1.3.m qfqm.tempStor.rateTemp.X1.4.m qfqm.tempStor.rateTemp.X1.5.m qfqm.tempStor.rateTemp.X1.6.m qfqm.tempStor.rateTemp.X1.7.m qfqm.tempStor.rateTemp.X1.8.m qfqm.tempStor.rateTemp.X1.9.m qfqm.tempStor.rateTemp.X2.m qfqm.tempStor.rateTemp.X2.1.m qfqm.tempStor.rateTemp.X2.2.m qfqm.tempStor.rateTemp.X2.3.m qfqm.tempStor.rateTemp.X2.4.m qfqm.tempStor.rateTemp.X2.5.m qfqm.tempStor.rateTemp.X2.6.m qfqm.tempStor.rateTemp.X2.7.m qfqm.tempStor.rateTemp.X2.8.m qfqm.tempStor.rateTemp.X2.9.m qfqm.tempStor.rateTemp.X3.m qfqm.tempStor.rateTemp.X3.1.m qfqm.tempStor.rateTemp.X3.2.m qfqm.tempStor.rateTemp.X3.3.m qfqm.tempStor.rateTemp.X3.4.m qfqm.tempStor.rateTemp.X3.5.m qfqm.tempStor.rateTemp.X3.6.m qfqm.tempStor.rateTemp.X3.7.m qfqm.tempStor.rateTemp.X3.8.m qfqm.tempStor.rateTemp.X3.9.m qfqm.tempStor.rateTemp.X4.m qfqm.tempStor.rateTemp.X4.1.m qfqm.tempStor.rateTemp.X4.2.m qfqm.tempStor.rateTemp.X4.3.m qfqm.tempStor.rateTemp.X4.4.m qfqm.tempStor.rateTemp.X4.5.m qfqm.tempStor.rateTemp.X4.6.m qfqm.tempStor.rateTemp.X4.7.m qfqm.tempStor.rateTemp.X4.8.m qfqm.tempStor.rateTemp.X4.9.m qfqm.tempStor.rateTemp.X5.m qfqm.tempStor.rateTemp.X5.1.m qfqm.tempStor.rateTemp.X5.2.m qfqm.tempStor.rateTemp.X5.3.m qfqm.tempStor.rateTemp.X5.4.m qfqm.tempStor.rateTemp.X5.5.m qfqm.tempStor.rateTemp.X5.6.m qfqm.tempStor.rateTemp.X5.7.m qfqm.tempStor.rateTemp.X5.8.m qfqm.tempStor.rateTemp.X5.9.m qfqm.tempStor.rateTemp.X6.m qfqm.tempStor.rateTemp.X6.1.m qfqm.tempStor.rateTemp.X6.2.m qfqm.tempStor.rateTemp.X6.3.m qfqm.tempStor.rateTemp.X6.4.m qfqm.tempStor.rateTemp.X6.5.m qfqm.tempStor.rateTemp.X6.6.m qfqm.tempStor.rateTemp.X6.7.m qfqm.tempStor.rateTemp.X6.8.m qfqm.tempStor.rateTemp.X6.9.m qfqm.tempStor.rateTemp.X7.m qfqm.tempStor.rateTemp.X7.1.m qfqm.tempStor.rateTemp.X7.2.m qfqm.tempStor.rateTemp.X7.3.m qfqm.tempStor.rateTemp.X7.4.m qfqm.tempStor.rateTemp.X7.5.m qfqm.tempStor.rateTemp.X7.6.m qfqm.tempStor.rateTemp.X7.7.m qfqm.tempStor.rateTemp.X7.8.m qfqm.tempStor.rateTemp.X7.9.m qfqm.tempStor.rateTemp.X8.m qfqm.tempStor.rateTemp.X8.1.m qfqm.tempStor.rateTemp.X8.2.m qfqm.tempStor.rateTemp.X8.3.m qfqm.tempStor.rateTemp.X8.4.m 2018-06-01T00:00:00.000Z 2018-06-01T00:29:59.000Z -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 -0.0002682 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 0.0003159 -0.0001014 -0.0001014 -0.0001014 -0.0001014 -0.0001014 -0.0001051 -0.0001112 -0.0001172 -0.0001233 -0.0001294 -0.0001354 -0.0001415 -0.0001476 -0.0001537 -0.0001597 -0.0001658 -0.0001719 -0.0001779 -0.0001840 -0.0001857 -0.0001870 -0.0001882 -0.0001895 -0.0001907 -0.0001919 -0.0001932 -0.0001944 -0.0001956 -0.0001969 -0.0001981 -0.0001994 -0.0002006 -0.0002018 -0.0002031 -0.0002043 -0.0002055 -0.0002068 -0.0002080 -0.0002093 -0.0002105 -0.0002117 -0.0002130 -0.0002142 -0.0002154 -0.0002172 -0.0002190 -0.0002208 -0.0002226 -0.0002244 -0.0002262 -0.0002279 -0.0002297 -0.0002315 -0.0002333 -0.0002351 -0.0002369 -0.0002387 -0.0002404 -0.0002422 -0.0002440 -0.0002458 -0.0002476 -0.0002494 -0.0002512 -0.0002529 -0.0002547 -0.0002565 -0.0002583 -0.0002601 -0.0002619 -0.0002637 -0.0002654 -0.0002672 -0.0002690 -0.0002708 -0.0002726 -0.0002744 -0.0002762 -0.0002779 -0.0002797 -0.0002815 -0.0002833 -0.0002851 -0.0002869 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01T00:30:00.000Z 2018-06-01T00:59:59.000Z 0.0004879 0.0004879 0.0004879 0.0004879 0.0004879 0.0004674 0.0004331 0.0003989 0.0003647 0.0003305 0.0002963 0.0002621 0.0002278 0.0001936 0.0001594 0.0001252 0.0000910 0.0000568 0.0000225 0.0000381 0.0000592 0.0000803 0.0001013 0.0001224 0.0001435 0.0001646 0.0001857 0.0002068 0.0002278 0.0002489 0.0002700 0.0002911 0.0003122 0.0003333 0.0003543 0.0003754 0.0003965 0.0004176 0.0004387 0.0004598 0.0004808 0.0005019 0.0005230 0.0005441 0.0005393 0.0005346 0.0005298 0.0005251 0.0005203 0.0005156 0.0005108 0.0005061 0.0005013 0.0004966 0.0004918 0.0004871 0.0004823 0.0004776 0.0004728 0.0004681 0.0004633 0.0004586 0.0004538 0.0004491 0.0004443 0.0004396 0.0004348 0.0004301 0.0004253 0.0004206 0.0004158 0.0004111 0.0004063 0.0004016 0.0003968 0.0003921 0.0003873 0.0003826 0.0003778 0.0003731 0.0003683 0.0003636 0.0003588 0.0003541 -0.0003565 -0.0003565 -0.0003565 -0.0003565 -0.0003565 -0.0003426 -0.0003192 -0.0002959 -0.0002726 -0.0002493 -0.0002260 -0.0002027 -0.0001793 -0.0001560 -0.0001327 -0.0001094 -0.0000861 -0.0000628 -0.0000394 -0.0000366 -0.0000361 -0.0000356 -0.0000350 -0.0000345 -0.0000339 -0.0000334 -0.0000329 -0.0000323 -0.0000318 -0.0000313 -0.0000307 -0.0000302 -0.0000296 -0.0000291 -0.0000286 -0.0000280 -0.0000275 -0.0000270 -0.0000264 -0.0000259 -0.0000253 -0.0000248 -0.0000243 -0.0000237 -0.0000279 -0.0000321 -0.0000363 -0.0000405 -0.0000446 -0.0000488 -0.0000530 -0.0000572 -0.0000614 -0.0000656 -0.0000697 -0.0000739 -0.0000781 -0.0000823 -0.0000865 -0.0000906 -0.0000948 -0.0000990 -0.0001032 -0.0001074 -0.0001116 -0.0001157 -0.0001199 -0.0001241 -0.0001283 -0.0001325 -0.0001366 -0.0001408 -0.0001450 -0.0001492 -0.0001534 -0.0001576 -0.0001617 -0.0001659 -0.0001701 -0.0001743 -0.0001785 -0.0001827 -0.0001868 -0.0001910 -0.0001814 -0.0001814 -0.0001814 -0.0001814 -0.0001814 -0.0001725 -0.0001576 -0.0001427 -0.0001278 -0.0001129 -0.0000980 -0.0000831 -0.0000683 -0.0000534 -0.0000385 -0.0000236 -0.0000087 0.0000062 0.0000211 0.0000247 0.0000271 0.0000295 0.0000318 0.0000342 0.0000366 0.0000390 0.0000414 0.0000437 0.0000461 0.0000485 0.0000509 0.0000533 0.0000556 0.0000580 0.0000604 0.0000628 0.0000652 0.0000675 0.0000699 0.0000723 0.0000747 0.0000771 0.0000794 0.0000818 0.0000836 0.0000854 0.0000872 0.0000891 0.0000909 0.0000927 0.0000945 0.0000963 0.0000981 0.0000999 0.0001017 0.0001035 0.0001053 0.0001071 0.0001090 0.0001108 0.0001126 0.0001144 0.0001162 0.0001180 0.0001198 0.0001216 0.0001234 0.0001252 0.0001271 0.0001289 0.0001307 0.0001325 0.0001343 0.0001361 0.0001379 0.0001397 0.0001415 0.0001433 0.0001451 0.0001470 0.0001488 0.0001506 0.0001524 0.0001542 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01T01:00:00.000Z 2018-06-01T01:29:59.000Z 0.0005086 0.0005086 0.0005086 0.0005086 0.0005086 0.0005025 0.0004925 0.0004825 0.0004724 0.0004624 0.0004523 0.0004423 0.0004323 0.0004222 0.0004122 0.0004021 0.0003921 0.0003820 0.0003720 0.0003480 0.0003224 0.0002968 0.0002712 0.0002457 0.0002201 0.0001945 0.0001689 0.0001434 0.0001178 0.0000922 0.0000666 0.0000410 0.0000155 -0.0000101 -0.0000357 -0.0000613 -0.0000869 -0.0001124 -0.0001380 -0.0001636 -0.0001892 -0.0002147 -0.0002403 -0.0002659 -0.0002551 -0.0002443 -0.0002335 -0.0002227 -0.0002119 -0.0002011 -0.0001904 -0.0001796 -0.0001688 -0.0001580 -0.0001472 -0.0001364 -0.0001256 -0.0001148 -0.0001040 -0.0000932 -0.0000824 -0.0000716 -0.0000609 -0.0000501 -0.0000393 -0.0000285 -0.0000177 -0.0000069 0.0000039 0.0000147 0.0000255 0.0000363 0.0000471 0.0000579 0.0000686 0.0000794 0.0000902 0.0001010 0.0001118 0.0001226 0.0001334 0.0001442 0.0001550 0.0001658 -0.0002073 -0.0002073 -0.0002073 -0.0002073 -0.0002073 -0.0002152 -0.0002283 -0.0002414 -0.0002545 -0.0002676 -0.0002807 -0.0002938 -0.0003069 -0.0003200 -0.0003331 -0.0003462 -0.0003593 -0.0003724 -0.0003855 -0.0003947 -0.0004034 -0.0004122 -0.0004209 -0.0004297 -0.0004384 -0.0004472 -0.0004559 -0.0004647 -0.0004735 -0.0004822 -0.0004910 -0.0004997 -0.0005085 -0.0005172 -0.0005260 -0.0005347 -0.0005435 -0.0005523 -0.0005610 -0.0005698 -0.0005785 -0.0005873 -0.0005960 -0.0006048 -0.0005965 -0.0005881 -0.0005798 -0.0005714 -0.0005631 -0.0005548 -0.0005464 -0.0005381 -0.0005297 -0.0005214 -0.0005131 -0.0005047 -0.0004964 -0.0004880 -0.0004797 -0.0004713 -0.0004630 -0.0004547 -0.0004463 -0.0004380 -0.0004296 -0.0004213 -0.0004130 -0.0004046 -0.0003963 -0.0003879 -0.0003796 -0.0003713 -0.0003629 -0.0003546 -0.0003462 -0.0003379 -0.0003296 -0.0003212 -0.0003129 -0.0003045 -0.0002962 -0.0002879 -0.0002795 -0.0002712 -0.0002556 -0.0002556 -0.0002556 -0.0002556 -0.0002556 -0.0002458 -0.0002293 -0.0002129 -0.0001965 -0.0001801 -0.0001637 -0.0001473 -0.0001308 -0.0001144 -0.0000980 -0.0000816 -0.0000652 -0.0000487 -0.0000323 -0.0000270 -0.0000229 -0.0000188 -0.0000147 -0.0000106 -0.0000065 -0.0000024 0.0000017 0.0000058 0.0000099 0.0000140 0.0000181 0.0000222 0.0000263 0.0000304 0.0000345 0.0000386 0.0000427 0.0000468 0.0000509 0.0000550 0.0000591 0.0000632 0.0000673 0.0000714 0.0000739 0.0000765 0.0000790 0.0000815 0.0000840 0.0000866 0.0000891 0.0000916 0.0000941 0.0000967 0.0000992 0.0001017 0.0001042 0.0001068 0.0001093 0.0001118 0.0001143 0.0001169 0.0001194 0.0001219 0.0001244 0.0001270 0.0001295 0.0001320 0.0001345 0.0001371 0.0001396 0.0001421 0.0001446 0.0001472 0.0001497 0.0001522 0.0001547 0.0001573 0.0001598 0.0001623 0.0001649 0.0001674 0.0001699 0.0001724 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01T01:30:00.000Z 2018-06-01T01:59:59.000Z 0.0013277 0.0013277 0.0013277 0.0013277 0.0013277 0.0013735 0.0014499 0.0015263 0.0016027 0.0016790 0.0017554 0.0018318 0.0019082 0.0019845 0.0020609 0.0021373 0.0022137 0.0022900 0.0023664 0.0023013 0.0022204 0.0021396 0.0020587 0.0019778 0.0018970 0.0018161 0.0017353 0.0016544 0.0015735 0.0014927 0.0014118 0.0013310 0.0012501 0.0011692 0.0010884 0.0010075 0.0009267 0.0008458 0.0007649 0.0006841 0.0006032 0.0005224 0.0004415 0.0003606 0.0003730 0.0003853 0.0003976 0.0004099 0.0004222 0.0004345 0.0004468 0.0004591 0.0004714 0.0004837 0.0004960 0.0005083 0.0005206 0.0005329 0.0005452 0.0005575 0.0005698 0.0005821 0.0005944 0.0006067 0.0006190 0.0006313 0.0006436 0.0006559 0.0006682 0.0006805 0.0006928 0.0007051 0.0007174 0.0007297 0.0007420 0.0007543 0.0007666 0.0007789 0.0007912 0.0008035 0.0008158 0.0008281 0.0008404 0.0008527 0.0000313 0.0000313 0.0000313 0.0000313 0.0000313 0.0000337 0.0000378 0.0000418 0.0000459 0.0000499 0.0000540 0.0000580 0.0000620 0.0000661 0.0000701 0.0000742 0.0000782 0.0000823 0.0000863 0.0000849 0.0000830 0.0000810 0.0000791 0.0000771 0.0000751 0.0000732 0.0000712 0.0000692 0.0000673 0.0000653 0.0000634 0.0000614 0.0000594 0.0000575 0.0000555 0.0000536 0.0000516 0.0000496 0.0000477 0.0000457 0.0000437 0.0000418 0.0000398 0.0000379 0.0000349 0.0000319 0.0000290 0.0000260 0.0000230 0.0000201 0.0000171 0.0000141 0.0000112 0.0000082 0.0000052 0.0000023 -0.0000007 -0.0000037 -0.0000066 -0.0000096 -0.0000126 -0.0000155 -0.0000185 -0.0000215 -0.0000244 -0.0000274 -0.0000304 -0.0000333 -0.0000363 -0.0000393 -0.0000422 -0.0000452 -0.0000482 -0.0000511 -0.0000541 -0.0000571 -0.0000600 -0.0000630 -0.0000660 -0.0000689 -0.0000719 -0.0000749 -0.0000778 -0.0000808 -0.0010983 -0.0010983 -0.0010983 -0.0010983 -0.0010983 -0.0010630 -0.0010043 -0.0009456 -0.0008868 -0.0008281 -0.0007693 -0.0007106 -0.0006519 -0.0005931 -0.0005344 -0.0004756 -0.0004169 -0.0003582 -0.0002994 -0.0002907 -0.0002876 -0.0002845 -0.0002814 -0.0002783 -0.0002751 -0.0002720 -0.0002689 -0.0002658 -0.0002627 -0.0002595 -0.0002564 -0.0002533 -0.0002502 -0.0002471 -0.0002439 -0.0002408 -0.0002377 -0.0002346 -0.0002315 -0.0002283 -0.0002252 -0.0002221 -0.0002190 -0.0002159 -0.0002160 -0.0002161 -0.0002163 -0.0002164 -0.0002165 -0.0002167 -0.0002168 -0.0002169 -0.0002171 -0.0002172 -0.0002173 -0.0002175 -0.0002176 -0.0002177 -0.0002179 -0.0002180 -0.0002181 -0.0002183 -0.0002184 -0.0002185 -0.0002187 -0.0002188 -0.0002189 -0.0002191 -0.0002192 -0.0002193 -0.0002195 -0.0002196 -0.0002197 -0.0002199 -0.0002200 -0.0002201 -0.0002203 -0.0002204 -0.0002205 -0.0002207 -0.0002208 -0.0002209 -0.0002211 -0.0002212 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01T02:00:00.000Z 2018-06-01T02:29:59.000Z 0.0007344 0.0007344 0.0007344 0.0007344 0.0007344 0.0008510 0.0010454 0.0012397 0.0014341 0.0016284 0.0018228 0.0020171 0.0022115 0.0024058 0.0026002 0.0027946 0.0029889 0.0031833 0.0033776 0.0032919 0.0031751 0.0030583 0.0029415 0.0028247 0.0027079 0.0025911 0.0024743 0.0023575 0.0022407 0.0021239 0.0020071 0.0018903 0.0017735 0.0016567 0.0015399 0.0014231 0.0013063 0.0011895 0.0010727 0.0009558 0.0008390 0.0007222 0.0006054 0.0004886 0.0004664 0.0004442 0.0004219 0.0003997 0.0003775 0.0003553 0.0003330 0.0003108 0.0002886 0.0002664 0.0002441 0.0002219 0.0001997 0.0001774 0.0001552 0.0001330 0.0001108 0.0000885 0.0000663 0.0000441 0.0000218 -0.0000004 -0.0000226 -0.0000448 -0.0000671 -0.0000893 -0.0001115 -0.0001338 -0.0001560 -0.0001782 -0.0002004 -0.0002227 -0.0002449 -0.0002671 -0.0002894 -0.0003116 -0.0003338 -0.0003560 -0.0003783 -0.0004005 -0.0000316 -0.0000316 -0.0000316 -0.0000316 -0.0000316 -0.0000246 -0.0000130 -0.0000013 0.0000103 0.0000219 0.0000336 0.0000452 0.0000568 0.0000685 0.0000801 0.0000917 0.0001034 0.0001150 0.0001266 0.0001239 0.0001195 0.0001152 0.0001109 0.0001065 0.0001022 0.0000978 0.0000935 0.0000891 0.0000848 0.0000804 0.0000761 0.0000717 0.0000674 0.0000630 0.0000587 0.0000543 0.0000500 0.0000456 0.0000413 0.0000369 0.0000326 0.0000283 0.0000239 0.0000196 0.0000170 0.0000145 0.0000120 0.0000095 0.0000070 0.0000045 0.0000020 -0.0000005 -0.0000030 -0.0000056 -0.0000081 -0.0000106 -0.0000131 -0.0000156 -0.0000181 -0.0000206 -0.0000231 -0.0000256 -0.0000282 -0.0000307 -0.0000332 -0.0000357 -0.0000382 -0.0000407 -0.0000432 -0.0000457 -0.0000482 -0.0000508 -0.0000533 -0.0000558 -0.0000583 -0.0000608 -0.0000633 -0.0000658 -0.0000683 -0.0000708 -0.0000733 -0.0000759 -0.0000784 -0.0000809 -0.0006432 -0.0006432 -0.0006432 -0.0006432 -0.0006432 -0.0006189 -0.0005784 -0.0005379 -0.0004973 -0.0004568 -0.0004163 -0.0003758 -0.0003353 -0.0002947 -0.0002542 -0.0002137 -0.0001732 -0.0001326 -0.0000921 -0.0000846 -0.0000808 -0.0000770 -0.0000732 -0.0000694 -0.0000656 -0.0000618 -0.0000580 -0.0000541 -0.0000503 -0.0000465 -0.0000427 -0.0000389 -0.0000351 -0.0000313 -0.0000275 -0.0000236 -0.0000198 -0.0000160 -0.0000122 -0.0000084 -0.0000046 -0.0000008 0.0000030 0.0000069 0.0000066 0.0000064 0.0000062 0.0000060 0.0000058 0.0000055 0.0000053 0.0000051 0.0000049 0.0000047 0.0000044 0.0000042 0.0000040 0.0000038 0.0000035 0.0000033 0.0000031 0.0000029 0.0000027 0.0000024 0.0000022 0.0000020 0.0000018 0.0000016 0.0000013 0.0000011 0.0000009 0.0000007 0.0000005 0.0000002 0.0000000 -0.0000002 -0.0000004 -0.0000007 -0.0000009 -0.0000011 -0.0000013 -0.0000015 -0.0000018 -0.0000020 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2018-06-01T02:30:00.000Z 2018-06-01T02:59:59.000Z -0.0009450 -0.0009450 -0.0009450 -0.0009450 -0.0009450 -0.0007653 -0.0004659 -0.0001665 0.0001329 0.0004323 0.0007317 0.0010311 0.0013305 0.0016300 0.0019294 0.0022288 0.0025282 0.0028276 0.0031270 0.0030239 0.0028760 0.0027282 0.0025803 0.0024325 0.0022846 0.0021368 0.0019889 0.0018411 0.0016932 0.0015454 0.0013975 0.0012497 0.0011018 0.0009540 0.0008061 0.0006583 0.0005104 0.0003626 0.0002147 0.0000669 -0.0000810 -0.0002288 -0.0003767 -0.0005245 -0.0005201 -0.0005157 -0.0005112 -0.0005068 -0.0005024 -0.0004979 -0.0004935 -0.0004891 -0.0004846 -0.0004802 -0.0004757 -0.0004713 -0.0004669 -0.0004624 -0.0004580 -0.0004536 -0.0004491 -0.0004447 -0.0004403 -0.0004358 -0.0004314 -0.0004269 -0.0004225 -0.0004181 -0.0004136 -0.0004092 -0.0004048 -0.0004003 -0.0003959 -0.0003915 -0.0003870 -0.0003826 -0.0003782 -0.0003737 -0.0003693 -0.0003648 -0.0003604 -0.0003560 -0.0003515 -0.0003471 -0.0001513 -0.0001513 -0.0001513 -0.0001513 -0.0001513 -0.0001414 -0.0001250 -0.0001085 -0.0000921 -0.0000756 -0.0000591 -0.0000427 -0.0000262 -0.0000098 0.0000067 0.0000232 0.0000396 0.0000561 0.0000725 0.0000696 0.0000645 0.0000594 0.0000544 0.0000493 0.0000442 0.0000391 0.0000340 0.0000289 0.0000238 0.0000188 0.0000137 0.0000086 0.0000035 -0.0000016 -0.0000067 -0.0000118 -0.0000168 -0.0000219 -0.0000270 -0.0000321 -0.0000372 -0.0000423 -0.0000474 -0.0000524 -0.0000516 -0.0000508 -0.0000499 -0.0000491 -0.0000482 -0.0000474 -0.0000466 -0.0000457 -0.0000449 -0.0000441 -0.0000432 -0.0000424 -0.0000415 -0.0000407 -0.0000399 -0.0000390 -0.0000382 -0.0000373 -0.0000365 -0.0000357 -0.0000348 -0.0000340 -0.0000332 -0.0000323 -0.0000315 -0.0000306 -0.0000298 -0.0000290 -0.0000281 -0.0000273 -0.0000265 -0.0000256 -0.0000248 -0.0000239 -0.0000231 -0.0000223 -0.0000214 -0.0000206 -0.0000197 -0.0000189 -0.0010168 -0.0010168 -0.0010168 -0.0010168 -0.0010168 -0.0009788 -0.0009156 -0.0008524 -0.0007891 -0.0007259 -0.0006627 -0.0005994 -0.0005362 -0.0004729 -0.0004097 -0.0003465 -0.0002832 -0.0002200 -0.0001568 -0.0001510 -0.0001516 -0.0001523 -0.0001529 -0.0001535 -0.0001542 -0.0001548 -0.0001554 -0.0001561 -0.0001567 -0.0001574 -0.0001580 -0.0001586 -0.0001593 -0.0001599 -0.0001605 -0.0001612 -0.0001618 -0.0001624 -0.0001631 -0.0001637 -0.0001643 -0.0001650 -0.0001656 -0.0001662 -0.0001655 -0.0001648 -0.0001640 -0.0001633 -0.0001626 -0.0001618 -0.0001611 -0.0001603 -0.0001596 -0.0001589 -0.0001581 -0.0001574 -0.0001566 -0.0001559 -0.0001552 -0.0001544 -0.0001537 -0.0001530 -0.0001522 -0.0001515 -0.0001507 -0.0001500 -0.0001493 -0.0001485 -0.0001478 -0.0001470 -0.0001463 -0.0001456 -0.0001448 -0.0001441 -0.0001434 -0.0001426 -0.0001419 -0.0001411 -0.0001404 -0.0001397 -0.0001389 -0.0001382 -0.0001374 -0.0001367 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5.6.7 Un-interpolated vertical profile data (Level 2) The Level 2 data are interpolated in time but not in space. They contain the rates of change at the measurement heights. Again, they can be extracted from the HDF5 files using stackEddy() with the same syntax: prof.l2 &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp02&quot;) ## Extracting data ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## Stacking data tables by month ## | | | 0% | |= | 2% | |== | 3% | |==== | 5% | |===== | 7% | |====== | 8% | |======= | 10% | |======== | 12% | |========= | 13% | |========== | 15% | |============ | 17% | |============= | 18% | |============== | 20% | |=============== | 22% | |================ | 23% | |================== | 25% | |=================== | 27% | |==================== | 28% | |===================== | 30% | |====================== | 32% | |======================= | 33% | |======================== | 35% | |========================== | 37% | |=========================== | 38% | |============================ | 40% | |============================= | 42% | |============================== | 43% | |================================ | 45% | |================================= | 47% | |================================== | 48% | |=================================== | 50% | |==================================== | 52% | |===================================== | 53% | |====================================== | 55% | |======================================== | 57% | |========================================= | 58% | |========================================== | 60% | |=========================================== | 62% | |============================================ | 63% | |============================================== | 65% | |=============================================== | 67% | |================================================ | 68% | |================================================= | 70% | |================================================== | 72% | |=================================================== | 73% | |==================================================== | 75% | |====================================================== | 77% | |======================================================= | 78% | |======================================================== | 80% | |========================================================= | 82% | |========================================================== | 83% | |============================================================ | 85% | |============================================================= | 87% | |============================================================== | 88% | |=============================================================== | 90% | |================================================================ | 92% | |================================================================= | 93% | |================================================================== | 95% | |==================================================================== | 97% | |===================================================================== | 98% | |======================================================================| 100% ## Joining data variables ## | | | 0% | |====== | 8% | |============ | 17% | |================== | 25% | |======================= | 33% | |============================= | 42% | |=================================== | 50% | |========================================= | 58% | |=============================================== | 67% | |==================================================== | 75% | |========================================================== | 83% | |================================================================ | 92% | |======================================================================| 100% knitr::kable(head(prof.l2$HARV)) Note that here, as in the PAR data, there is a verticalPosition field. It has the same meaning as in the PAR data, indicating the tower level of the measurement. 5.6.8 Calibrated raw data (Level 1) Level 1 (dp01) data are calibrated, and aggregated in time, but otherwise untransformed. Use Level 1 data for raw gas concentrations and atmospheric stable isotopes. Using stackEddy() to extract Level 1 data requires additional inputs. The Level 1 files are too large to simply pull out all the variables by default, and they include mutiple averaging intervals, which can’t be merged. So two additional inputs are needed: avg: The averaging interval to extract var: One or more variables to extract What variables are available, at what averaging intervals? Another function in the neonUtilities package, getVarsEddy(), returns a list of HDF5 file contents. It requires only one input, a filepath to a single NEON HDF5 file: vars &lt;- getVarsEddy(&quot;./data/filesToStack00200/NEON.D01.HARV.DP4.00200.001.nsae.2018-07.basic.h5&quot;) knitr::kable(head(vars)) site level category system hor ver tmi name otype dclass dim oth 5 HARV dp01 data amrs 000 060 01m angNedXaxs H5I_DATASET COMPOUND 44640 NA 6 HARV dp01 data amrs 000 060 01m angNedYaxs H5I_DATASET COMPOUND 44640 NA 7 HARV dp01 data amrs 000 060 01m angNedZaxs H5I_DATASET COMPOUND 44640 NA 9 HARV dp01 data amrs 000 060 30m angNedXaxs H5I_DATASET COMPOUND 1488 NA 10 HARV dp01 data amrs 000 060 30m angNedYaxs H5I_DATASET COMPOUND 1488 NA 11 HARV dp01 data amrs 000 060 30m angNedZaxs H5I_DATASET COMPOUND 1488 NA Inputs to var can be any values from the name field in the table returned by getVarsEddy(). Let’s take a look at CO2 and H2O, 13C in CO2 and 18O in H2O, at 30-minute aggregation. Let’s look at Harvard Forest for these data, since deeper canopies generally have more interesting profiles: iso &lt;- stackEddy(filepath=&quot;./data/filesToStack00200/&quot;, level=&quot;dp01&quot;, var=c(&quot;rtioMoleDryCo2&quot;,&quot;rtioMoleDryH2o&quot;, &quot;dlta13CCo2&quot;,&quot;dlta18OH2o&quot;), avg=30) ## Extracting data ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% ## Stacking data tables by month ## | | | 0% | | | 1% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |=== | 4% | |=== | 5% | |==== | 5% | |==== | 6% | |===== | 7% | |===== | 8% | |====== | 8% | |====== | 9% | |======= | 9% | |======= | 10% | |======= | 11% | |======== | 11% | |======== | 12% | |========= | 12% | |========= | 13% | |========== | 14% | |========== | 15% | |=========== | 15% | |=========== | 16% | |============ | 17% | |============ | 18% | |============= | 18% | |============= | 19% | |============== | 19% | |============== | 20% | |============== | 21% | |=============== | 21% | |=============== | 22% | |================ | 22% | |================ | 23% | |================= | 24% | |================= | 25% | |================== | 25% | |================== | 26% | |=================== | 27% | |=================== | 28% | |==================== | 28% | |==================== | 29% | |===================== | 29% | |===================== | 30% | |===================== | 31% | |====================== | 31% | |====================== | 32% | |======================= | 32% | |======================= | 33% | |======================== | 34% | |======================== | 35% | |========================= | 35% | |========================= | 36% | |========================== | 37% | |========================== | 38% | |=========================== | 38% | |=========================== | 39% | |============================ | 39% | |============================ | 40% | |============================ | 41% | |============================= | 41% | |============================= | 42% | |============================== | 42% | |============================== | 43% | |=============================== | 44% | |=============================== | 45% | |================================ | 45% | |================================ | 46% | |================================= | 47% | |================================= | 48% | |================================== | 48% | |================================== | 49% | |=================================== | 49% | |=================================== | 50% | |=================================== | 51% | |==================================== | 51% | |==================================== | 52% | |===================================== | 52% | |===================================== | 53% | |====================================== | 54% | |====================================== | 55% | |======================================= | 55% | |======================================= | 56% | |======================================== | 57% | |======================================== | 58% | |========================================= | 58% | |========================================= | 59% | |========================================== | 59% | |========================================== | 60% | |========================================== | 61% | |=========================================== | 61% | |=========================================== | 62% | |============================================ | 62% | |============================================ | 63% | |============================================= | 64% | |============================================= | 65% | |============================================== | 65% | |============================================== | 66% | |=============================================== | 67% | |=============================================== | 68% | |================================================ | 68% | |================================================ | 69% | |================================================= | 69% | |================================================= | 70% | |================================================= | 71% | |================================================== | 71% | |================================================== | 72% | |=================================================== | 72% | |=================================================== | 73% | |==================================================== | 74% | |==================================================== | 75% | |===================================================== | 75% | |===================================================== | 76% | |====================================================== | 77% | |====================================================== | 78% | |======================================================= | 78% | |======================================================= | 79% | |======================================================== | 79% | |======================================================== | 80% | |======================================================== | 81% | |========================================================= | 81% | |========================================================= | 82% | |========================================================== | 82% | |========================================================== | 83% | |=========================================================== | 84% | |=========================================================== | 85% | |============================================================ | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================= | 88% | |============================================================== | 88% | |============================================================== | 89% | |=============================================================== | 89% | |=============================================================== | 90% | |=============================================================== | 91% | |================================================================ | 91% | |================================================================ | 92% | |================================================================= | 92% | |================================================================= | 93% | |================================================================== | 94% | |================================================================== | 95% | |=================================================================== | 95% | |=================================================================== | 96% | |==================================================================== | 97% | |==================================================================== | 98% | |===================================================================== | 98% | |===================================================================== | 99% | |======================================================================| 99% | |======================================================================| 100% ## Joining data variables ## | | | 0% | |= | 2% | |=== | 4% | |==== | 6% | |===== | 7% | |====== | 9% | |======== | 11% | |========= | 13% | |========== | 15% | |============ | 17% | |============= | 19% | |============== | 20% | |================ | 22% | |================= | 24% | |================== | 26% | |=================== | 28% | |===================== | 30% | |====================== | 31% | |======================= | 33% | |========================= | 35% | |========================== | 37% | |=========================== | 39% | |============================= | 41% | |============================== | 43% | |=============================== | 44% | |================================ | 46% | |================================== | 48% | |=================================== | 50% | |==================================== | 52% | |====================================== | 54% | |======================================= | 56% | |======================================== | 57% | |========================================= | 59% | |=========================================== | 61% | |============================================ | 63% | |============================================= | 65% | |=============================================== | 67% | |================================================ | 69% | |================================================= | 70% | |=================================================== | 72% | |==================================================== | 74% | |===================================================== | 76% | |====================================================== | 78% | |======================================================== | 80% | |========================================================= | 81% | |========================================================== | 83% | |============================================================ | 85% | |============================================================= | 87% | |============================================================== | 89% | |================================================================ | 91% | |================================================================= | 93% | |================================================================== | 94% | |=================================================================== | 96% | |===================================================================== | 98% | |======================================================================| 100% knitr::kable(head(iso$HARV)) Let’s plot vertical profiles of CO2 and 13C in CO2 on a single day. Here, for convenience, instead of converting the time stamps to a time format, it’s easy to use the character format to extract the ones we want using grep(). And discard the verticalPosition values that are string values - those are the calibration gases. iso.d &lt;- iso$HARV[grep(&quot;2018-06-25&quot;, iso$HARV$timeBgn, fixed=T),] iso.d &lt;- iso.d[-which(is.na(as.numeric(iso.d$verticalPosition))),] ## Warning in which(is.na(as.numeric(iso.d$verticalPosition))): NAs introduced by ## coercion ggplot is well suited to these types of data, let’s use it to plot the profiles. library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.2 g &lt;- ggplot(iso.d, aes(y=verticalPosition)) + geom_path(aes(x=data.co2Stor.rtioMoleDryCo2.mean, group=timeBgn, col=timeBgn)) + theme(legend.position=&quot;none&quot;) + xlab(&quot;CO2&quot;) + ylab(&quot;Tower level&quot;) g ## Warning: Removed 2 row(s) containing missing values (geom_path). g &lt;- ggplot(iso.d, aes(y=verticalPosition)) + geom_path(aes(x=data.isoCo2.dlta13CCo2.mean, group=timeBgn, col=timeBgn)) + theme(legend.position=&quot;none&quot;) + xlab(&quot;d13C&quot;) + ylab(&quot;Tower level&quot;) g Warning message: “Removed 55 rows containing missing values (geom_path).” The legends are omitted for space, see if you can work out the times of day the different colors represent. 5.6.9 Convert NEON flux data variables to AmeriFlux FP standard Install and load packages #Install NEONprocIS.base from GitHub, this package is a dependency of eddy4R.base devtools::install_github(repo=&quot;NEONScience/NEON-IS-data-processing&quot;, ref=&quot;master&quot;, subdir=&quot;pack/NEONprocIS.base&quot;, dependencies=c(NA, TRUE)[2], repos=c(BiocManager::repositories(), # for dependencies on Bioconductor packages &quot;https://cran.rstudio.com/&quot;) # for CRAN ) #Install eddy4R.base from GitHub devtools::install_github(repo=&quot;NEONScience/eddy4R&quot;, ref=&quot;master&quot;, subdir=&quot;pack/eddy4R.base&quot;, dependencies=c(NA, TRUE)[2], repos=c(BiocManager::repositories(), # for dependencies on Bioconductor packages &quot;https://cran.rstudio.com/&quot;) # for CRAN ) packReq &lt;- c(&quot;rhdf5&quot;, &quot;eddy4R.base&quot;, &quot;jsonlite&quot;, &quot;lubridate&quot;) lapply(packReq, function(x) { print(x) if(require(x, character.only = TRUE) == FALSE) { install.packages(x) library(x, character.only = TRUE) }}) Select your site of interest from the list of NEON sites below. site &lt;- &quot;KONZ&quot; #&quot;BARR&quot;,&quot;CLBJ&quot;,&quot;MLBS&quot;,&quot;DSNY&quot;,&quot;NIWO&quot;,&quot;ORNL&quot;,&quot;OSBS&quot;, #&quot;SCBI&quot;,&quot;LENO&quot;,&quot;TALL&quot;,&quot;CPER&quot;,&quot;BART&quot;,&quot;HARV&quot;,&quot;BLAN&quot;, #&quot;SERC&quot;,&quot;JERC&quot;,&quot;GUAN&quot;,&quot;LAJA&quot;,&quot;STEI&quot;,&quot;TREE&quot;,&quot;UNDE&quot;, #&quot;KONA&quot;,&quot;KONZ&quot;,&quot;UKFS&quot;,&quot;GRSM&quot;,&quot;DELA&quot;,&quot;DCFS&quot;,&quot;NOGP&quot;, #&quot;WOOD&quot;,&quot;RMNP&quot;,&quot;OAES&quot;,&quot;YELL&quot;,&quot;MOAB&quot;,&quot;STER&quot;,&quot;JORN&quot;, #&quot;SRER&quot;,&quot;ONAQ&quot;,&quot;ABBY&quot;,&quot;WREF&quot;,&quot;SJER&quot;,&quot;SOAP&quot;,&quot;TEAK&quot;, #&quot;TOOL&quot;,&quot;BONA&quot;,&quot;DEJU&quot;,&quot;HEAL&quot;,&quot;PUUM&quot; } If you would like to download a set range of dates, define the following paramemters. If these are not defined, it will default to the entire record at the site #define start and end dates, optional, defaults to entire period of site operation. Use %Y-%m-%d format. dateBgn &lt;- &quot;2020-03-01&quot; dateEnd &lt;- &quot;2020-05-31&quot; # Data package from the portal Pack &lt;- c(&#39;basic&#39;,&#39;expanded&#39;)[1] #The version data for the FP standard conversion processing ver = paste0(&quot;v&quot;,format(Sys.time(), &quot;%Y%m%dT%H%m&quot;)) Specify Download directory for HDF5 files from the NEON data portal and output directory to save the resulting csv files. Change save paths to where you want the files on your computer. #download directory DirDnld=tempdir() #Output directory, change this to where you want to save the output csv DirOutBase &lt;-paste0(&quot;~/eddy/data/Ameriflux/&quot;,ver) Specify Data Product number, for the Bundled Eddy-Covariance files, this is DP4.00200.001 #DP number dpID &lt;- &#39;DP4.00200.001&#39; Get metadata from Ameriflux Site Info BADM sheets for the site of interest #Grab a list of all Ameriflux sites, containing site ID and site description sites_web &lt;- jsonlite::fromJSON(&quot;http://ameriflux-data.lbl.gov/AmeriFlux/SiteSearch.svc/SiteList/AmeriFlux&quot;) #Grab only NEON sites sitesNeon &lt;- sites_web[grep(pattern = paste0(&quot;NEON.*&quot;,site), x = sites_web$SITE_NAME),] #For all NEON sites siteNeon &lt;- sites_web[grep(pattern = paste0(&quot;NEON.*&quot;,site), x = sites_web$SITE_NAME),] metaSite &lt;- lapply(siteNeon$SITE_ID, function(x) { pathSite &lt;- paste0(&quot;http://ameriflux-data.lbl.gov/BADM/Anc/SiteInfo/&quot;,x) tmp &lt;- fromJSON(pathSite) return(tmp) }) Use Ameriflux site IDs to name metadata lists #use NEON ID as list name names(metaSite) &lt;- site #Use Ameriflux site ID as list name #names(metaSite) &lt;- sitesNeon$SITE_ID Check if dateBgn is defined, if not make it the initial operations date “IOCR” of the site if(!exists(&quot;dateBgn&quot;) || is.na(dateBgn) || is.null(dateBgn)){ dateBgn &lt;- as.Date(metaSite[[site]]$values$GRP_FLUX_MEASUREMENTS[[1]]$FLUX_MEASUREMENTS_DATE_START, &quot;%Y%m%d&quot;) } else { dateBgn &lt;- dateBgn }#End of checks for missing dateBgn #Check if dateEnd is defined, if not make it the system date if(!exists(&quot;dateEnd&quot;) || is.na(dateEnd) || is.null(dateEnd)){ dateEnd &lt;- as.Date(Sys.Date()) } else { dateEnd &lt;- dateEnd }#End of checks for missing dateEnd Grab the UTC time offset from the Ameriflux API timeOfstUtc &lt;- as.integer(metaSite[[site]]$values$GRP_UTC_OFFSET[[1]]$UTC_OFFSET) Create the date sequence setDate &lt;- seq(from = as.Date(dateBgn), to = as.Date(dateEnd), by = &quot;month&quot;) Start processing the site time range specified, verify that the site and date range are specified as intended msg &lt;- paste0(&quot;Starting Ameriflux FP standard conversion processing workflow for &quot;, site, &quot; for &quot;, dateBgn, &quot; to &quot;, dateEnd) print(msg) Create output directory by checking if the download directory exists and create it if not if(dir.exists(DirDnld) == FALSE) dir.create(DirDnld, recursive = TRUE) #Append the site to the base output directory DirOut &lt;- paste0(DirOutBase, &quot;/&quot;, siteNeon$SITE_ID) #Check if directory exists and create if not if(!dir.exists(DirOut)) dir.create(DirOut, recursive = TRUE) Download and extract data #Initialize data List dataList &lt;- list() #Read data from the API dataList &lt;- lapply(setDate, function(x) { # year &lt;- lubridate::year(x) # mnth &lt;- lubridate::month(x) date &lt;- stringr::str_extract(x, pattern = paste0(&quot;[0-9]{4}&quot;, &quot;-&quot;, &quot;[0-9]{2}&quot;)) tryCatch(neonUtilities::zipsByProduct(dpID = dpID, site = site, startdate = date, enddate = date, package = &quot;basic&quot;, savepath = DirDnld, check.size = FALSE), error=function(e) NULL) files &lt;- list.files(paste0(DirDnld, &quot;/filesToStack00200&quot;)) utils::unzip(paste0(DirDnld, &quot;/filesToStack00200/&quot;, files[grep(pattern = paste0(site,&quot;.*.&quot;, date, &quot;.*.zip&quot;), x = files)]), exdir = paste0(DirDnld, &quot;/filesToStack00200&quot;)) files &lt;- list.files(paste0(DirDnld, &quot;/filesToStack00200&quot;)) dataIdx &lt;- rhdf5::h5read(file = paste0(DirDnld, &quot;/filesToStack00200/&quot;, max(files[grep(pattern = paste0(site,&quot;.*.&quot;, date,&quot;.*.h5&quot;), x = files)])), name = paste0(site, &quot;/&quot;)) if(!is.null(dataIdx)){ dataIdx$dp0p &lt;- NULL dataIdx$dp02 &lt;- NULL dataIdx$dp03 &lt;- NULL dataIdx$dp01$ucrt &lt;- NULL dataIdx$dp04$ucrt &lt;- NULL dataIdx$dp01$data &lt;- lapply(dataIdx$dp01$data,FUN=function(var){ nameTmi &lt;- names(var) var &lt;- var[grepl(&#39;_30m&#39;,nameTmi)] return(var)}) dataIdx$dp01$qfqm &lt;- lapply(dataIdx$dp01$qfqm,FUN=function(var){ nameTmi &lt;- names(var) var &lt;- var[grepl(&#39;_30m&#39;,nameTmi)] return(var)}) } return(dataIdx) }) Add names to list for year/month combinations names(dataList) &lt;- paste0(lubridate::year(setDate),sprintf(&quot;%02d&quot;,lubridate::month(setDate))) Remove NULL elements from list dataList &lt;- dataList[vapply(dataList, Negate(is.null), NA)] Determine tower horizontal &amp; vertical indices #Find the tower top level by looking at the vertical index of the turbulent CO2 concentration measurements LvlTowr &lt;- grep(pattern = &quot;_30m&quot;, names(dataList[[1]]$dp01$data$co2Turb), value = TRUE) LvlTowr &lt;- gsub(x = LvlTowr, pattern = &quot;_30m&quot;, replacement = &quot;&quot;) #get tower top level LvlTop &lt;- strsplit(LvlTowr,&quot;&quot;) LvlTop &lt;- base::as.numeric(LvlTop[[1]][6]) #Ameriflux vertical levels based off of https://ameriflux.lbl.gov/data/aboutdata/data-variables/ section 3.3.1 &quot;Indices must be in order, starting with the highest.&quot; idxVerAmfx &lt;- base::seq(from = 1, to = LvlTop, by = 1) #get the sequence from top to first level LvlMeas &lt;- base::seq(from = LvlTop, to = 1, by = -1) #Recreate NEON naming conventions LvlMeas &lt;- paste0(&quot;000_0&quot;,LvlMeas,&quot;0&quot;,sep=&quot;&quot;) #Give NEON naming conventions to Ameriflux vertical levels names(idxVerAmfx) &lt;- LvlMeas #Ameriflux horizontal index idxHorAmfx &lt;- 1 Subset to the Ameriflux variables to convert dataListFlux &lt;- lapply(names(dataList), function(x) { data.frame( &quot;TIMESTAMP_START&quot; = as.POSIXlt(dataList[[x]]$dp04$data$fluxCo2$turb$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;, tz = &quot;GMT&quot;), &quot;TIMESTAMP_END&quot; = as.POSIXlt(dataList[[x]]$dp04$data$fluxCo2$turb$timeEnd, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;, tz = &quot;GMT&quot;), # &quot;TIMESTAMP_START&quot; = strftime(as.POSIXlt(dataList[[x]][[idxSite]]$dp04$data$fluxCo2$turb$timeBgn, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;), format = &quot;%Y%m%d%H%M&quot;), # &quot;TIMESTAMP_END&quot; = strftime(as.POSIXlt(dataList[[x]][[idxSite]]$dp04$data$fluxCo2$turb$timeEnd, format=&quot;%Y-%m-%dT%H:%M:%OSZ&quot;) + 60, format = &quot;%Y%m%d%H%M&quot;), &quot;FC&quot;= dataList[[x]]$dp04$data$fluxCo2$turb$flux, &quot;SC&quot;= dataList[[x]]$dp04$data$fluxCo2$stor$flux, &quot;NEE&quot;= dataList[[x]]$dp04$data$fluxCo2$nsae$flux, &quot;LE&quot; = dataList[[x]]$dp04$data$fluxH2o$turb$flux, &quot;SLE&quot; = dataList[[x]]$dp04$data$fluxH2o$stor$flux, &quot;USTAR&quot; = dataList[[x]]$dp04$data$fluxMome$turb$veloFric, &quot;H&quot; = dataList[[x]]$dp04$data$fluxTemp$turb$flux, &quot;SH&quot; = dataList[[x]]$dp04$data$fluxTemp$stor$flux, &quot;FETCH_90&quot; = dataList[[x]]$dp04$data$foot$stat$distXaxs90, &quot;FETCH_MAX&quot; = dataList[[x]]$dp04$data$foot$stat$distXaxsMax, &quot;V_SIGMA&quot; = dataList[[x]]$dp04$data$foot$stat$veloYaxsHorSd, #&quot;W_SIGMA&quot; = dataList[[x]]$dp04$data$foot$stat$veloZaxsHorSd, &quot;CO2_1_1_1&quot; = dataList[[x]]$dp01$data$co2Turb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean, &quot;H2O_1_1_1&quot; = dataList[[x]]$dp01$data$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean, &quot;qfFinlH2oTurbFrt00Samp&quot; = dataList[[x]]$dp01$qfqm$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$frt00Samp$qfFinl, &quot;qfH2O_1_1_1&quot; = dataList[[x]]$dp01$qfqm$h2oTurb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl, &quot;qfCO2_1_1_1&quot; = dataList[[x]]$dp01$qfqm$co2Turb[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl, &quot;qfSC&quot; = dataList[[x]]$dp04$qfqm$fluxCo2$stor$qfFinl, &quot;qfSLE&quot; = dataList[[x]]$dp04$qfqm$fluxH2o$stor$qfFinl, &quot;qfSH&quot; = dataList[[x]]$dp04$qfqm$fluxTemp$stor$qfFinl, &quot;qfT_SONIC&quot; = dataList[[x]]$dp01$qfqm$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$qfFinl, &quot;qfWS_1_1_1&quot; = dataList[[x]]$dp01$qfqm$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$qfFinl, rbind.data.frame(lapply(names(idxVerAmfx), function(y) { tryCatch({rlog$debug(y)}, error=function(cond){print(y)}) rpt &lt;- list() rpt[[paste0(&quot;CO2_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$data$co2Stor[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean rpt[[paste0(&quot;H2O_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$data$h2oStor[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean rpt[[paste0(&quot;CO2_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$data$isoCo2[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryCo2$mean rpt[[paste0(&quot;H2O_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$data$isoCo2[[paste0(y,&quot;_30m&quot;)]]$rtioMoleDryH2o$mean rpt[[paste0(&quot;qfCO2_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$co2Stor[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl rpt[[paste0(&quot;qfH2O_1_&quot;,idxVerAmfx[y],&quot;_2&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$h2oStor[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl rpt[[paste0(&quot;qfCO2_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$isoCo2[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryCo2$qfFinl rpt[[paste0(&quot;qfH2O_1_&quot;,idxVerAmfx[y],&quot;_3&quot;)]] &lt;- dataList[[x]]$dp01$qfqm$isoH2o[[paste0(LvlTowr,&quot;_30m&quot;)]]$rtioMoleDryH2o$qfFinl rpt &lt;- rbind.data.frame(rpt) return(rpt) } )), &quot;WS_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$mean, &quot;WS_MAX_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$veloXaxsYaxsErth$max, &quot;WD_1_1_1&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$angZaxsErth$mean, &quot;T_SONIC&quot; = dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$mean, &quot;T_SONIC_SIGMA&quot; = base::sqrt(dataList[[x]]$dp01$data$soni[[paste0(LvlTowr,&quot;_30m&quot;)]]$tempSoni$mean) , stringsAsFactors = FALSE) }) names(dataListFlux) &lt;- names(dataList) Combine the monthly data into a single dataframe, remove lists and clean memory dataDfFlux &lt;- do.call(rbind.data.frame,dataListFlux) rm(list=c(&quot;dataListFlux&quot;,&quot;dataList&quot;)) gc() Regularize timeseries to 30 minutes in case timestamps are missing from NEON files due to processing errors timeRglr &lt;- eddy4R.base::def.rglr(timeMeas = as.POSIXlt(dataDfFlux$TIMESTAMP_START), dataMeas = dataDfFlux, BgnRglr = as.POSIXlt(dataDfFlux$TIMESTAMP_START[1]), EndRglr = as.POSIXlt(dataDfFlux$TIMESTAMP_END[length(dataDfFlux$TIMESTAMP_END)]), TzRglr = &quot;UTC&quot;, FreqRglr = 1/(60*30)) #Reassign data to data.frame dataDfFlux &lt;- timeRglr$dataRglr #Format timestamps dataDfFlux$TIMESTAMP_START &lt;- strftime(timeRglr$timeRglr + lubridate::hours(timeOfstUtc), format = &quot;%Y%m%d%H%M&quot;) dataDfFlux$TIMESTAMP_END &lt;- strftime(timeRglr$timeRglr + lubridate::hours(timeOfstUtc) + lubridate::minutes(30), format = &quot;%Y%m%d%H%M&quot;) Define validation times, and remove this data from the dataset. At NEON sites, validations with a series of gasses of known concentration are run every 23.5 hours. These values are used to correct for measurment drift and are run every 23.5 hours to achive daily resolution while also spreading the impact of lost measurements throughout the day. #Remove co2Turb and h2oTurb data based off of qfFlow (qfFinl frt00) dataDfFlux$FC[(which(dataDfFlux$qfCO2_1_1_1 == 1))] &lt;- NaN dataDfFlux$LE[(which(dataDfFlux$qfH2O_1_1_1 == 1))] &lt;- NaN dataDfFlux$USTAR[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$H[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$SC[(which(dataDfFlux$qfSC == 1))] &lt;- NaN dataDfFlux$SLE[(which(dataDfFlux$qfSLE == 1))] &lt;- NaN dataDfFlux$SH[(which(dataDfFlux$qfSH == 1))] &lt;- NaN dataDfFlux$T_SONIC[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$T_SONIC_SIGMA[(which(dataDfFlux$qfT_SONIC_1_1_1 == 1))] &lt;- NaN dataDfFlux$WS_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$WS_MAX_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$WD_1_1_1[(which(dataDfFlux$qfWS_1_1_1 == 1))] &lt;- NaN dataDfFlux$H2O_1_1_1[(which(dataDfFlux$qfH2O_1_1_1 == 1))] &lt;- NaN dataDfFlux$CO2_1_1_1[(which(dataDfFlux$qfCO2_1_1_1 == 1))] &lt;- NaN lapply(idxVerAmfx, function(x){ #x &lt;- 1 dataDfFlux[[paste0(&quot;H2O_1_&quot;,x,&quot;_2&quot;)]][(which(dataDfFlux[[paste0(&quot;qfH2O_1_&quot;,x,&quot;_2&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;H2O_1_&quot;,x,&quot;_3&quot;)]][(which(dataDfFlux[[paste0(&quot;qfH2O_1_&quot;,x,&quot;_3&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;CO2_1_&quot;,x,&quot;_2&quot;)]][(which(dataDfFlux[[paste0(&quot;qfCO2_1_&quot;,x,&quot;_2&quot;)]] == 1))] &lt;&lt;- NaN dataDfFlux[[paste0(&quot;CO2_1_&quot;,x,&quot;_3&quot;)]][(which(dataDfFlux[[paste0(&quot;qfCO2_1_&quot;,x,&quot;_3&quot;)]] == 1))] &lt;&lt;- NaN }) Remove quality flagging variables from output setIdxQf &lt;- grep(&quot;qf&quot;, names(dataDfFlux)) dataDfFlux[,setIdxQf] &lt;- NULL Set range thresholds #assign list Rng &lt;- list() Rng$Min &lt;- data.frame( &quot;FC&quot; = -100, #[umol m-2 s-1] &quot;SC&quot; = -100, #[umol m-2 s-1] &quot;NEE&quot; = -100, #[umol m-2 s-1] &quot;LE&quot; = -500, #[W m-2] &quot;H&quot; = -500, #[W m-2] &quot;USTAR&quot; = 0, #[m s-1] &quot;CO2&quot; = 200, #[umol mol-1] &quot;H2O&quot; = 0, #[mmol mol-1] &quot;WS_1_1_1&quot; = 0, #[m s-1] &quot;WS_MAX_1_1_1&quot; = 0, #[m s-1] &quot;WD_1_1_1&quot; = -0.1, #[deg] &quot;T_SONIC&quot; = -55.0 #[C] ) Set Max thresholds Rng$Max &lt;- data.frame( &quot;FC&quot; = 100, #[umol m-2 s-1] &quot;SC&quot; = 100, #[umol m-2 s-1] &quot;NEE&quot; = 100, #[umol m-2 s-1] &quot;LE&quot; = 1000, #[W m-2] &quot;H&quot; = 1000, #[W m-2] &quot;USTAR&quot; = 5, #[m s-1] &quot;CO2&quot; = 800, #[umol mol-1] &quot;H2O&quot; = 100, #[mmol mol-1] &quot;WS_1_1_1&quot; = 50, #[m s-1] &quot;WS_MAX_1_1_1&quot; = 50, #[m s-1] &quot;WD_1_1_1&quot; = 360, #[deg] &quot;T_SONIC&quot; = 45.0 #[C] ) Grab all CO2/H2O columns to apply same thresholds, replace missing values with -9999 nameCO2 &lt;- grep(&quot;CO2&quot;,names(dataDfFlux),value = TRUE) nameH2O &lt;- grep(&quot;H2O&quot;,names(dataDfFlux),value = TRUE) #Apply the CO2/H2O threshold to all variables in HOR_VER_REP Rng$Min[nameCO2] &lt;- Rng$Min$CO2 Rng$Min[nameH2O] &lt;- Rng$Min$H2O Rng$Max[nameCO2] &lt;- Rng$Max$CO2 Rng$Max[nameH2O] &lt;- Rng$Max$H2O #Apply the range test to the output, and replace values with NaN lapply(names(dataDfFlux), function(x) { dataDfFlux[which(dataDfFlux[,x]&lt;Rng$Min[[x]] | dataDfFlux[,x]&gt;Rng$Max[[x]]),x] &lt;&lt;- NaN}) # Delete any NEE that have either FC or SC removed dataDfFlux[is.na(dataDfFlux$FC) | is.na(dataDfFlux$SC),&quot;NEE&quot;] &lt;- NaN #Change NA to -9999 dataDfFlux[is.na(dataDfFlux)] &lt;- -9999 Write output data to csv #Create output filename based off of Ameriflux file naming convention nameFileOut &lt;- base::paste0(DirOut,&quot;/&quot;,siteNeon$SITE_ID,&#39;_HH_&#39;,dataDfFlux$TIMESTAMP_START[1],&#39;_&#39;,utils::tail(dataDfFlux$TIMESTAMP_END,n=1),&#39;_flux.csv&#39;) #Write output to .csv write.csv(x = dataDfFlux, file = nameFileOut, row.names = FALSE) Clean up environment rm(list=&quot;dataDfFlux&quot;) gc() 5.7 Exercises 5.7.1 Computational NEON data are submitted to AmeriFlux quarterly after one year of non-quality flagged or otherwise missing data are available. Use the workflow above to extend the data coverage of an already submitted NEON site by downloading existing data from the AmeriFlux website and recently published HDF5 files from the NEON data portal. Process the NEON data such that it is in AmeriFlux format and plot the entire timerseries. Hint: NEON sites start with US-x Using metScanR package, find co-located NEON and AmeriFlux sites. Download data for an overlapping time period, and compare FC and H values by making a scatter plot and seeing how far off the data are from a 1:1 line. "],
["neon-aop.html", "Chapter 6 NEON AOP 6.1 Hyperspectral Remote Sensing 6.2 Key Metadata for Hyperspectral Data 6.3 Intro to Working with Hyperspectral Remote Sensing Data in HDF5 Format 6.3 About Hyperspectral Remote Sensing Data 6.3 Read HDF5 data into R 6.3 Create a Georeferenced Raster 6.3 Calculating Forest Structural Diversity Metrics from NEON LiDAR Data 6.4 Introduction to Structural Diversity Metrics 6.5 NEON AOP Discrete Return LIDAR 6.6 Calculating Structural Diversity Metrics 6.7 Matching GEDI waveforms with NEON AOP LiDAR pointclouds 6.8 NEON AOP Written Questions: 6.9 NEON AOP Coding Lab 6.10 NEON AOP Culmination Write Up", " Chapter 6 NEON AOP 6.1 Hyperspectral Remote Sensing 6.1.1 Learning Objectives After completing this tutorial, you will be able to: Define hyperspectral remote sensing. Explain the fundamental principles of hyperspectral remote sensing data. Describe the key attributes that are required to effectively work with hyperspectral remote sensing data in tools like R or Python. Describe what a “band” is. 6.1.1.1 Mapping the Invisible 6.1.2 About Hyperspectral Remote Sensing Data The electromagnetic spectrum is composed of thousands of bands representing different types of light energy. Imaging spectrometers (instruments that collect hyperspectral data) break the electromagnetic spectrum into groups of bands that support classification of objects by their spectral properties on the earth’s surface. Hyperspectral data consists of many bands – up to hundreds of bands – that cover the electromagnetic spectrum. The NEON imaging spectrometer collects data within the 380nm to 2510nm portions of the electromagnetic spectrum within bands that are approximately 5nm in width. This results in a hyperspectral data cube that contains approximately 426 bands - which means big, big data. 6.2 Key Metadata for Hyperspectral Data 6.2.1 Bands and Wavelengths A band represents a group of wavelengths. For example, the wavelength values between 695nm and 700nm might be one band as captured by an imaging spectrometer. The imaging spectrometer collects reflected light energy in a pixel for light in that band. Often when you work with a multi or hyperspectral dataset, the band information is reported as the center wavelength value. This value represents the center point value of the wavelengths represented in that band. Thus in a band spanning 695-700 nm, the center would be 697.5). Imaging spectrometers collect reflected light information within defined bands or regions of the electromagnetic spectrum. Source: National Ecological Observatory Network (NEON) 6.2.2 Spectral Resolution The spectral resolution of a dataset that has more than one band, refers to the width of each band in the dataset. In the example above, a band was defined as spanning 695-700nm. The width or spatial resolution of the band is thus 5 nanometers. To see an example of this, check out the band widths for the Landsat sensors. 6.2.3 Full Width Half Max (FWHM) The full width half max (FWHM) will also often be reported in a multi or hyperspectral dataset. This value represents the spread of the band around that center point. The Full Width Half Max (FWHM) of a band relates to the distance in nanometers between the band center and the edge of the band. In this case, the FWHM for Band C is 5 nm. In the illustration above, the band that covers 695-700nm has a FWHM of 5 nm. While a general spectral resolution of the sensor is often provided, not all sensors create bands of uniform widths. For instance bands 1-9 of Landsat 8 are listed below (Courtesy of USGS) Band Wavelength range (microns) Spatial Resolution (m) Spectral Width (microns) Band 1 - Coastal aerosol 0.43 - 0.45 30 0.02 Band 2 - Blue 0.45 - 0.51 30 0.06 Band 3 - Green 0.53 - 0.59 30 0.06 Band 4 - Red 0.64 - 0.67 30 0.03 Band 5 - Near Infrared (NIR) 0.85 - 0.88 30 0.03 Band 6 - SWIR 1 1.57 - 1.65 30 0.08 Band 7 - SWIR 2 2.11 - 2.29 30 0.18 Band 8 - Panchromatic 0.50 - 0.68 15 0.18 Band 9 - Cirrus 1.36 - 1.38 30 0.02 6.3 Intro to Working with Hyperspectral Remote Sensing Data in HDF5 Format Contributors: Felipe Sanchez, Leah A. Wasser, Edmund Hart, Donal O’Leary Based on NEON Science Tutorial Series: https://www.neonscience.org/intro-hsi-r-series In this tutorial, we will explore reading and extracting spatial raster data stored within a HDF5 file using R. 6.3 Learning Objectives After completing this section, you will be able to: Explain how HDF5 data can be used to store spatial data and the associated benefits of this format when working with large spatial data cubes. Extract metadata from HDF5 files. Slice or subset HDF5 data. You will extract one band of pixels. Plot a matrix as an image and a raster. Export a final GeoTIFF (spatially projected) that can be used both in further analysis and in common GIS tools like QGIS. 6.3 R Libraries to Install: rhdf5: install.packages(&quot;BiocManager&quot;), BiocManager::install(&quot;rhdf5&quot;) raster: install.packages('raster') rgdal: install.packages('rgdal') 6.3 Data to Download Download NEON Teaching Data Subset: Imaging Spectrometer Data - HDF5 These hyperspectral remote sensing data provide information on the National Ecological Observatory Network’s San Joaquin Exerimental Range field site in March of 2019. The data were collected over the San Joaquin field site located in California (Domain 17) and processed at NEON headquarters. This data subset is derived from the mosaic tile named NEON_D17_SJER_DP3_257000_4112000_reflectance.h5. The entire dataset can be accessed by request from the NEON Data Portal. Download Dataset Remember that the example dataset linked here only has 1 out of every 4 bands included in a full NEON hyperspectral dataset (this substantially reduces the file size!). When we refer to bands in this section, we will note the band numbers for this example dataset, which are different from NEON production data. To convert a band number (b) from this example data subset to the equivalent band in a full NEON hyperspectral file (b’), use the following equation: b’ = 1+4*(b-1). 6.3 About Hyperspectral Remote Sensing Data The electromagnetic spectrum is composed of thousands of bands representing different types of light energy. Imaging spectrometers (instruments that collect hyperspectral data) break the electromagnetic spectrum into groups of bands that support classification of objects by their spectral properties on the Earth’s surface. Hyperspectral data consists of many bands - up to hundreds of bands - that cover the electromagnetic spectrum. The NEON imaging spectrometer (NIS) collects data within the 380 nm to 2510 nm portions of the electromagnetic spectrum within bands that are approximately 5 nm in width. This results in a hyperspectral data cube that contains approximately 428 bands - which means BIG DATA. Remember that the example dataset used here only has 1 out of every 4 bands included in a full NEON hyperspectral dataset (this substantially reduces size!). When we refer to bands in this tutorial, we will note the band numbers for this example dataset, which may be different from NEON production data. A data cube of NEON hyperspectral data. Each layer in the cube represents a band. The HDF5 data model natively compresses data stored within it (makes it smaller) and supports data slicing (extracting only the portions of the data that you need to work with rather than reading the entire dataset into memory). These features in addition to the ability to support spatial data and associated metadata make it ideal for working with large data cubes such as those generated by imaging spectrometers. In this section we will explore reading and extracting spatial raster data stored within a HDF5 file using R. 6.3 Read HDF5 data into R We will use the raster and rhdf5 packages to read in the HDF5 file that contains hyperspectral data for the NEON San Joaquin (SJER) field site. Let’s start by calling the needed packages and reading in our NEON HDF5 file. Please be sure that you have at least version 2.10 of rhdf5 installed. Use: packageVersion(&quot;rhdf5&quot;) to check the package version. # Load `raster` and `rhdf5` packages and read NIS data into R library(raster) ## Loading required package: sp library(rhdf5) library(rgdal) ## rgdal: version: 1.5-16, (SVN revision 1050) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28 ## Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal ## GDAL binary built with GEOS: FALSE ## Loaded PROJ runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION: 520] ## Path to PROJ shared files: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj ## Linking to sp version:1.4-2 ## Overwritten PROJ_LIB was /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj # Define the file name to be opened f &lt;- (&#39;./data/AOP/NEON_hyperspectral_tutorial_example_subset.h5&#39;) Data Tip: To update all packages installed in R, use update.packages(). # look at the HDF5 file structure head(h5ls(f,all=T)) When you look at the structure of the data, take note of the “map info” dataset, the “Coordinate_System” group, and the “wavelength” and “Reflectance” datasets. The “Coordinate_System” folder contains the spatial attributes of the data including its EPSG Code, which is easily converted to a Coordinate Reference System (CRS). The CRS documents how the data are physically located on the Earth. The “wavelength” dataset contains the middle wavelength values for each band in the data. The “Reflectance” dataset contains the image data that we will use for both data processing and visualization. More Information on raster metadata: Raster Data in R The Basics - this tutorial explains more about how rasters work in R and their associated metadata. About Hyperspectral Remote Sensing Data -this tutorial explains more about metadata and important concepts associated with multi-band (multi and hyperspectral) rasters. Data Tip - HDF5 Structure: Note that the structure of individual HDF5 files may vary depending on who produced the data. In this case, the Wavelength and reflectance data within the file are both datasets. However, the spatial information is contained within a group. Data downloaded from another organization like NASA, may look different. This is why it’s important to explore the data before diving into using it! We can use the h5readAttributes() function to read and extract metadata from the HDF5 file. Let’s start by learning about the wavelengths described within this file. # get information about the wavelengths of this dataset wavelengthInfo &lt;- h5readAttributes(f,&quot;/SJER/Reflectance/Metadata/Spectral_Data/Wavelength&quot;) wavelengthInfo ## $Description ## [1] &quot;Central wavelength of the reflectance bands.&quot; ## ## $Units ## [1] &quot;nanometers&quot; Next, we can use the h5read function to read the data contained within the HDF5 file. Let’s read in the wavelengths of the band centers: # read in the wavelength information from the HDF5 file wavelengths &lt;- h5read(f,&quot;/SJER/Reflectance/Metadata/Spectral_Data/Wavelength&quot;) head(wavelengths) ## [1] 381.5437 401.5756 421.6075 441.6394 461.6713 481.7032 tail(wavelengths) ## [1] 2404.764 2424.796 2444.828 2464.860 2484.892 2504.924 Which wavelength is band 6 associated with? (Hint: look at the wavelengths vector that we just imported and check out the data located at index 6 - wavelengths[6]). 482 nanometers falls within the blue portion of the electromagnetic spectrum. Source: National Ecological Observatory Network Band 6 has a associate wavelength center of 481.7032 nanometers (nm) which is in the blue portion of the visible electromagnetic spectrum (~ 400-700 nm). 6.3 Bands and Wavelengths A band represents a group of wavelengths. For example, the wavelength values between 695 nm and 700 nm might be one band as captured by an imaging spectrometer. The imaging spectrometer collects reflected light energy in a pixel for light in that band. Often when you work with a multi or hyperspectral dataset, the band information is reported as the center wavelength value. This value represents the center point value of the wavelengths represented in that band. Thus in a band spanning 695-700 nm, the center would be 697.5 nm). The full width half max (FWHM) will also be reported. This value represents the spread of the band around that center point. So, a band that covers 800 nm-805 nm might have a FWHM of 5 nm and a wavelength value of 802.5 nm. Bands represent a range of values (types of light) within the electromagnetic spectrum. Values for each band are often represented as the center point value of each band. Source: National Ecological Observatory Network (NEON) The HDF5 dataset that we are working with in this activity may contain more information than we need to work with. For example, we don’t necessarily need to process all 107 bands available in this example dataset (or all 426 bands available in a full NEON hyperspectral reflectance file, for that matter) - if we are interested in creating a product like NDVI which only uses bands in the near infra-red and red portions of the spectrum. Or we might only be interested in a spatial subset of the data - perhaps a region where we have plots in the field. The HDF5 format allows us to slice (or subset) the data - quickly extracting the subset that we need to process. Let’s extract one of the green bands in our dataset - band 9. By the way - what is the center wavelength value associated with band 9? Hint: wavelengths[9]. How do we know this band is a green band in the visible portion of the spectrum? In order to effectively subset our data, let’s first read the important reflectance metadata stored as attributes in the “Reflectance_Data” dataset. # First, we need to extract the reflectance metadata: reflInfo &lt;- h5readAttributes(f, &quot;/SJER/Reflectance/Reflectance_Data&quot;) reflInfo ## $Cloud_conditions ## [1] &quot;For cloud conditions information see Weather Quality Index dataset.&quot; ## ## $Cloud_type ## [1] &quot;Cloud type may have been selected from multiple flight trajectories.&quot; ## ## $Data_Ignore_Value ## [1] -9999 ## ## $Description ## [1] &quot;Atmospherically corrected reflectance.&quot; ## ## $Dimension_Labels ## [1] &quot;Line, Sample, Wavelength&quot; ## ## $Dimensions ## [1] 500 500 107 ## ## $Interleave ## [1] &quot;BSQ&quot; ## ## $Scale_Factor ## [1] 10000 ## ## $Spatial_Extent_meters ## [1] 257500 258000 4112500 4113000 ## ## $Spatial_Resolution_X_Y ## [1] 1 1 ## ## $Units ## [1] &quot;Unitless.&quot; ## ## $Units_Valid_range ## [1] 0 10000 ## ## $dim ## [1] 107 500 500 # Next, we read the different dimensions nRows &lt;- reflInfo$Dimensions[1] nCols &lt;- reflInfo$Dimensions[2] nBands &lt;- reflInfo$Dimensions[3] nRows ## [1] 500 nCols ## [1] 500 nBands ## [1] 107 The HDF5 read function reads data in the order: Bands, Cols, Rows. This is different from how R reads data. We’ll adjust for this later. # Extract or &quot;slice&quot; data for band 9 from the HDF5 file b9 &lt;- h5read(f,&quot;/SJER/Reflectance/Reflectance_Data&quot;,index=list(9,1:nCols,1:nRows)) # what type of object is b9? class(b9) ## [1] &quot;array&quot; 6.3 A Note About Data Slicing in HDF5 Data slicing allows us to extract and work with subsets of the data rather than reading in the entire dataset into memory. Thus, in this case, we can extract and plot the green band without reading in all 107 bands of information. The ability to slice large datasets makes HDF5 ideal for working with big data. Next, let’s convert our data from an array (more than 2 dimensions) to a matrix (just 2 dimensions). We need to have our data in a matrix format to plot it. # convert from array to matrix by selecting only the first band b9 &lt;- b9[1,,] # check it class(b9) ## [1] &quot;matrix&quot; 6.3 Arrays vs. Matrices Arrays are matrices with more than 2 dimensions. When we say dimension, we are talking about the “z” associated with the data (imagine a series of tabs in a spreadsheet). Put the other way: matrices are arrays with only 2 dimensions. Arrays can have any number of dimensions one, two, ten or more. Here is a matrix that is 4 x 3 in size (4 rows and 3 columns): Metric species 1 species 2 total number 23 45 average weight 14 5 average length 2.4 3.5 average height 32 12 6.3 Dimensions in Arrays An array contains 1 or more dimensions in the “z” direction. For example, let’s say that we collected the same set of species data for every day in a 30 day month. We might then have a matrix like the one above for each day for a total of 30 days making a 4 x 3 x 30 array (this dataset has more than 2 dimensions). More on R object types here (links to external site, DataCamp). A matrix has only 2 dimensions An array has more than 2 dimensions. Next, let’s look at the metadata for the reflectance data. When we do this, take note of 1) the scale factor and 2) the data ignore value. Then we can plot the band 9 data. Plotting spatial data as a visual “data check” is a good idea to make sure processing is being performed correctly and all is well with the image. # look at the metadata for the reflectance dataset h5readAttributes(f,&quot;/SJER/Reflectance/Reflectance_Data&quot;) ## $Cloud_conditions ## [1] &quot;For cloud conditions information see Weather Quality Index dataset.&quot; ## ## $Cloud_type ## [1] &quot;Cloud type may have been selected from multiple flight trajectories.&quot; ## ## $Data_Ignore_Value ## [1] -9999 ## ## $Description ## [1] &quot;Atmospherically corrected reflectance.&quot; ## ## $Dimension_Labels ## [1] &quot;Line, Sample, Wavelength&quot; ## ## $Dimensions ## [1] 500 500 107 ## ## $Interleave ## [1] &quot;BSQ&quot; ## ## $Scale_Factor ## [1] 10000 ## ## $Spatial_Extent_meters ## [1] 257500 258000 4112500 4113000 ## ## $Spatial_Resolution_X_Y ## [1] 1 1 ## ## $Units ## [1] &quot;Unitless.&quot; ## ## $Units_Valid_range ## [1] 0 10000 ## ## $dim ## [1] 107 500 500 # plot the image image(b9) Figure 6.1: Plot of reflectance values for band 9 data. This plot shows a very washed out image lacking any detail. That is hard to visually interpret. What happens if we plot a log of the data? image(log(b9)) What do you notice about the first image? It’s washed out and lacking any detail. What could be causing this? It got better when plotting the log of the values, but still not great. Let’s look at the distribution of reflectance values in our data to figure out what is going on. # Plot range of reflectance values as a histogram to view range # and distribution of values. hist(b9,breaks=40,col=&quot;darkmagenta&quot;) Figure 6.2: Histogram of reflectance values for band 9. The x-axis represents the reflectance values and ranges from 0 to 8000. The frequency of these values is on the y-axis. The histogram shows reflectance values are skewed to the right, where the majority of the values lie between 0 and 1000. We can conclude that reflectance values are not equally distributed across the range of reflectance values, resulting in a washed out image. # View values between 0 and 5000 hist(b9,breaks=40,col=&quot;darkmagenta&quot;,xlim = c(0, 5000)) Figure 6.3: Histogram of reflectance values between 0 and 5000 for band 9. Reflectance values are on the x-axis, and the frequency is on the y-axis. The x-axis limit has been set 5000 in order to better visualize the distribution of reflectance values. We can confirm that the majority of the values are indeed within the 0 to 4000 range. # View higher values hist(b9, breaks=40,col=&quot;darkmagenta&quot;,xlim = c(5000, 15000),ylim=c(0,100)) Figure 6.4: Histogram of reflectance values between 5000 and 15000 for band 9. Reflectance values are on the x-axis, and the frequency is on the y-axis. Plot shows that a very few number of pixels have reflectance values larger than 5,000. These values are skewing how the image is being rendered and heavily impacting the way the image is drawn on our monitor. As you’re examining the histograms above, keep in mind that reflectance values range between 0-1. The data scale factor in the metadata tells us to divide all reflectance values by 10,000. Thus, a value of 5,000 equates to a reflectance value of 0.50. Storing data as integers (without decimal places) compared to floating points (with decimal places) creates a smaller file. You will see this done often when working with remote sensing data. Notice in the data that there are some larger reflectance values (&gt;5,000) that represent a smaller number of pixels. These pixels are skewing how the image renders. 6.3 Data Ignore Value Image data in raster format will often contain a data ignore value and a scale factor. The data ignore value represents pixels where there are no data. Among other causes, no data values may be attributed to the sensor not collecting data in that area of the image or to processing results which yield null values. Remember that the metadata for the Reflectance dataset designated -9999 as data ignore value. Thus, let’s set all pixels with a value == -9999 to NA (no value). If we do this, R won’t try to render these pixels. # there is a no data value in our raster - let&#39;s define it myNoDataValue &lt;- as.numeric(reflInfo$Data_Ignore_Value) myNoDataValue ## [1] -9999 # set all values equal to -9999 to NA b9[b9 == myNoDataValue] &lt;- NA # plot the image now image(b9) Figure 6.5: Plot of reflectance values for band 9 data with values equal to -9999 set to NA. Image data in raster format will often contain no data values, which may be attributed to the sensor not collecting data in that area of the image or to processing results which yield null values. Reflectance datasets designate -9999 as data ignore values. As such, we will reassign -9999 values to NA so R won’t try to render these pixels. 6.3 Reflectance Values and Image Stretch Our image still looks dark because R is trying to render all reflectance values between 0 and 14999 as if they were distributed equally in the histogram. However we know they are not distributed equally. There are many more values between 0-5000 then there are values &gt;5000. Images have a distribution of reflectance values. A typical image viewing program will render the values by distributing the entire range of reflectance values across a range of “shades” that the monitor can render - between 0 and 255. However, often the distribution of reflectance values is not linear. For example, in the case of our data, most of the reflectance values fall between 0 and 0.5. Yet there are a few values &gt;0.8 that are heavily impacting the way the image is drawn on our monitor. Imaging processing programs like ENVI, QGIS and ArcGIS (and even Adobe Photoshop) allow you to adjust the stretch of the image. This is similar to adjusting the contrast and brightness in Photoshop. The proper way to adjust our data would be what’s called an image stretch. We will learn how to stretch our image data, later. For now, let’s plot the values as the log function on the pixel reflectance values to factor out those larger values. image(log(b9)) Figure 6.6: Plot of log transformed reflectance values for the previous b9 image. Applying the log to the image increases the contrast making it look more like an image by factoring out those larger values. While an improvement, the image is still far from great. The proper way to adjust an image is by doing whats called an image stretch. The log applied to our image increases the contrast making it look more like an image. However, look at the images below. The top one is what our log adjusted image looks like when plotted. The bottom on is an RGB version of the same image. Notice a difference? TOP: The image as it should look. BOTTOM: the image that we outputted from the code above. Notice a difference? 6.3 Transpose Image Notice that there are three data dimensions for this file: Bands x Rows x Columns. However, when R reads in the dataset, it reads them as: Columns x Bands x Rows. The data are flipped. We can quickly transpose the data to correct for this using the t or transpose command in R. The orientation is rotated in our log adjusted image. This is because R reads in matrices starting from the upper left hand corner. Whereas, most rasters read pixels starting from the lower left hand corner. In the next section, we will deal with this issue by creating a proper georeferenced (spatially located) raster in R. The raster format will read in pixels following the same methods as other GIS and imaging processing software like QGIS and ENVI do. # We need to transpose x and y values in order for our # final image to plot properly b9 &lt;- t(b9) image(log(b9), main=&quot;Transposed Image&quot;) Figure 6.7: Plot showing the transposed image of the log transformed reflectance values of b9. The orientation of the image is rotated in our log transformed image, because R reads in the matrices starting from the upper left hand corner. 6.3 Create a Georeferenced Raster Next, we will create a proper raster using the b9 matrix. The raster format will allow us to define and manage: Image stretch Coordinate reference system &amp; spatial reference Resolution and other raster attributes… It will also account for the orientation issue discussed above. To create a raster in R, we need a few pieces of information, including: The coordinate reference system (CRS) The spatial extent of the image 6.3 Define Raster CRS First, we need to define the Coordinate reference system (CRS) of the raster. To do that, we can first grab the EPSG code from the HDF5 attributes, and covert the EPSG to a CRS string. Then we can assign that CRS to the raster object. # Extract the EPSG from the h5 dataset myEPSG &lt;- h5read(f, &quot;/SJER/Reflectance/Metadata/Coordinate_System/EPSG Code&quot;) # convert the EPSG code to a CRS string myCRS &lt;- crs(paste0(&quot;+init=epsg:&quot;,myEPSG)) # define final raster with projection info # note that capitalization will throw errors on a MAC. # if UTM is all caps it might cause an error! b9r &lt;- raster(b9, crs=myCRS) # view the raster attributes b9r ## class : RasterLayer ## dimensions : 500, 500, 250000 (nrow, ncol, ncell) ## resolution : 0.002, 0.002 (x, y) ## extent : 0, 1, 0, 1 (xmin, xmax, ymin, ymax) ## crs : +init=epsg:32611 +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## source : memory ## names : layer ## values : 0, 9210 (min, max) # let&#39;s have a look at our properly oriented raster. Take note of the # coordinates on the x and y axis. image(log(b9r), xlab = &quot;UTM Easting&quot;, ylab = &quot;UTM Northing&quot;, main = &quot;Properly Oriented Raster&quot;) Figure 6.8: Plot of the properly oriented raster image of the band 9 data. In order to orient the image correctly, the coordinate reference system was defined and assigned to the raster object. X-axis represents the UTM Easting values, and the Y-axis represents the Northing values. Next we define the extents of our raster. The extents will be used to calculate the raster’s resolution. Fortunately, the spatial extent is provided in the HDF5 file “Reflectance_Data” group attributes that we saved before as reflInfo. # Grab the UTM coordinates of the spatial extent xMin &lt;- reflInfo$Spatial_Extent_meters[1] xMax &lt;- reflInfo$Spatial_Extent_meters[2] yMin &lt;- reflInfo$Spatial_Extent_meters[3] yMax &lt;- reflInfo$Spatial_Extent_meters[4] # define the extent (left, right, top, bottom) rasExt &lt;- extent(xMin,xMax,yMin,yMax) rasExt ## class : Extent ## xmin : 257500 ## xmax : 258000 ## ymin : 4112500 ## ymax : 4113000 # assign the spatial extent to the raster extent(b9r) &lt;- rasExt # look at raster attributes b9r ## class : RasterLayer ## dimensions : 500, 500, 250000 (nrow, ncol, ncell) ## resolution : 1, 1 (x, y) ## extent : 257500, 258000, 4112500, 4113000 (xmin, xmax, ymin, ymax) ## crs : +init=epsg:32611 +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## source : memory ## names : layer ## values : 0, 9210 (min, max) The extent of a raster represents the spatial location of each corner. The coordinate units will be determined by the spatial projection/ coordinate reference system that the data are in. Source: National Ecological Observatory Network (NEON) Learn more about raster attributes including extent, and coordinate reference systems here. We can adjust the colors of our raster too if we want. # let&#39;s change the colors of our raster and adjust the zlims col &lt;- terrain.colors(25) image(b9r, xlab = &quot;UTM Easting&quot;, ylab = &quot;UTM Northing&quot;, main= &quot;Raster w Custom Colors&quot;, col=col, zlim=c(0,3000)) Figure 6.9: Plot of the properly oriented raster image of B9 with custom colors. We can adjust the colors of the image by adjusting the z limits, which in this case makes the highly reflective surfaces more vibrant. This color adjustment is more apparent in the bottom left of the image, where the parking lot, buildings and bare surfaces are located. X-axis represents the UTM Easting values, and the Y-axis represents the Northing values. We’ve now created a raster from band 9 reflectance data. We can export the data as a raster, using the writeRaster command. # write out the raster as a geotiff writeRaster(b9r, file=paste0(wd,&quot;band9.tif&quot;), format=&quot;GTiff&quot;, overwrite=TRUE) # It&#39;s always good practice to close the H5 connection before moving on! # close the H5 file H5close() 6.3 Challenge: Work with Rasters Try these three extensions on your own: Create rasters using other bands in the dataset. Vary the distribution of values in the image to mimic an image stretch. e.g. b9[b9 &gt; 6000 ] &lt;- 6000 Use what you know to extract ALL of the reflectance values for ONE pixel rather than for an entire band. HINT: this will require you to pick an x and y value and then all values in the z dimension: aPixel&lt;- h5read(f,&quot;Reflectance&quot;,index=list(NULL,100,35)). Plot the spectra output. 6.3 Calculating Forest Structural Diversity Metrics from NEON LiDAR Data Contributors: Jeff Atkins, Keith Krause, Atticus Stovall Authors: Elizabeth LaRue, Donal O’Leary 6.3 Learning Objectives After completing this tutorial, you will be able to: Read a NEON LiDAR file (laz) into R Visualize a spatial subset of the LiDAR tile Correct a spatial subset of the LiDAR tile for topographic varation Calculate 13 structural diversity metrics described in LaRue, Wagner, et al. (2020) 6.3 R Libraries to Install: lidR: install.packages('lidR') gstat: install.packages('gstat') Important Note: If you have R version 3.6 or above you’ll need to update data.table: data.table::update.dev.pkg() 6.3 Data to Download For this tutorial, we will be using two .laz files containing NEON AOP point clouds for 1km tiles from the Harvard Forest (HARV) and Lower Teakettle (TEAK) sites. Link to download .laz files on Google Drive Here. 6.3 Recommended Skills For this tutorial, you should have an understanding of Light Detection And Ranging (LiDAR) technology, specifically how discrete return lidar data are collected and represented in las/laz files. For more information on how lidar works, please see NEON’s Introduction to Lidar Tutorial Series. 6.4 Introduction to Structural Diversity Metrics Forest structure influences many important ecological processes, including biogeochemical cycling, wildfires, species richness and diversity, and many others. Quantifying forest structure, hereafter referred to as “structural diversity,” presents a challenge for many reasons, including difficulty in measuring individual trees, limbs, and leaves across large areas. In order to overcome this challenge, today’s researchers use Light Detection And Ranging (LiDAR) technology to measure large areas of forest. It is also challenging to calculate meaningful summary metrics of a forest’s structure that 1) are ecologically relevant and 2) can be used to compare different forested locations. In this tutorial, you will be introduced to a few tools that will help you to explore and quantify forest structure using LiDAR data collected at two field sites of the National Ecological Observatory Network. 6.5 NEON AOP Discrete Return LIDAR The NEON Airborne Observation Platform (AOP) . has several sensors including discrete-return LiDAR, which is useful for measuring forest structural diversity that can be summarized into four categories of metrics: (1) canopy height, (2) canopy cover and openness, and (3) canopy heterogeneity (internal and external), and (4) vegetation area. We will be comparing the structural diversity of two NEON sites that vary in their structural characteristics. First, we will look at Harvard Forest (HARV), which is located in Massachusetts. It is a lower elevation, mixed deciduous and evergreen forest dominated by Quercus rubra, Acer rubrum, and Aralia nudicaulis. Second, we will look at Lower Teakettle (TEAK), which is a high elevation forested NEON site in California. TEAK is an evergreen forest dominated by Abies magnifica, Abies concolor, Pinus jeffreyi, and Pinus contorta. As you can imagine, these two forest types will have both similarities and differences in their structural attributes. We can quantify these attributes by calculating several different structural diversity metrics, and comparing the results. 6.5.1 Loading the LIDAR Products To begin, we first need to load our required R packages, and set our working directory to the location where we saved the input LiDAR .laz files that can be downloaded from the NEON Data Portal. library(lidR) library(gstat) library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following object is masked from &#39;package:raster&#39;: ## ## shift library(kableExtra) Next, we will read in the LiDAR data using the lidR::readLAS() function. Note that this function can read in both .las and .laz file formats. # Read in LiDAR data #2017 1 km2 tile .laz file type for HARV and TEAK #Watch out for outlier Z points - this function also allows for the #ability to filter outlier points well above or below the landscape #(-drop_z_blow and -drop_z_above). See how we have done this here #for you. HARV &lt;- lidR::readLAS(&#39;/Users/kdw223/Research/katharynduffy.github.io/data/NEON_D01_HARV_DP1_727000_4702000_classified_point_cloud_colorized.laz&#39;,filter = &quot;-drop_z_below 150 -drop_z_above 325&quot;) TEAK &lt;- lidR::readLAS(&#39;/Users/kdw223/Research/katharynduffy.github.io/data/NEON_D17_TEAK_DP1_316000_4091000_classified_point_cloud_colorized.laz&#39;,filter = &quot;-drop_z_below 1694 -drop_z_above 2500&quot;) Let’s check out: the extent, coordinate system, and a 3D plot of each .laz file. Note that on Mac computers you may need to install XQuartz for 3D plots - see xquartz.org summary(HARV) lidR::plot(HARV) class : LAS (v1.3 format 3) memory : 521.6 Mb extent : 727000, 728000, 4702000, 4703000 (xmin, xmax, ymin, ymax) coord. ref. : +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 area : 1 km² points : 5.94 million points density : 5.94 points/m² File signature: LASF� File source ID: 211 Global encoding: - GPS Time Type: GPS Week Time - Synthetic Return Numbers: no - Well Know Text: CRS is GeoTIFF - Aggregate Model: false Project ID - GUID: 00000000-0000-0000-0000-000000000000 Version: 1.3 System identifier: LAStools (c) by rapidlasso GmbH Generating software: lascolor (190812) commercial File creation d/y: 278/2019 header size: 235 Offset to point data: 329 Num. var. length record: 1 Point data format: 3 Point data record length: 34 Num. of point records: 5944282 Num. of points by return: 4581566 1255117 104991 2608 0 Scale factor X Y Z: 0.01 0.01 0.01 Offset X Y Z: 7e+05 4700000 0 min X Y Z: 727000 4702000 223.2 max X Y Z: 728000 4703000 324.99 Variable length records: Variable length record 1 of 1 Description: Tags: Key 1024 value 1 Key 1025 value 2 Key 3072 value 32618 Key 4099 value 9001 1 km-squared point cloud from Harvard Forest showing a gentle slope covered in a continuous canopy of mixed forest. summary(TEAK) lidR::plot(TEAK) class : LAS (v1.3 format 3) memory : 439.2 Mb extent : 316000, 317000, 4091231, 4092000 (xmin, xmax, ymin, ymax) coord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 area : 0.75 km² points : 5.01 million points density : 6.7 points/m² File signature: LASF� File source ID: 211 Global encoding: - GPS Time Type: GPS Week Time - Synthetic Return Numbers: no - Well Know Text: CRS is GeoTIFF - Aggregate Model: false Project ID - GUID: 00000000-0000-0000-0000-000000000000 Version: 1.3 System identifier: LAStools (c) by rapidlasso GmbH Generating software: lascolor (190812) commercial File creation d/y: 206/2019 header size: 235 Offset to point data: 329 Num. var. length record: 1 Point data format: 3 Point data record length: 34 Num. of point records: 5006002 Num. of points by return: 3315380 1182992 416154 91476 0 Scale factor X Y Z: 0.01 0.01 0.01 Offset X Y Z: 3e+05 4e+06 0 min X Y Z: 316000 4091231 2117.03 max X Y Z: 317000 4092000 2376.98 Variable length records: Variable length record 1 of 1 Description: Tags: Key 1024 value 1 Key 1025 value 2 Key 3072 value 32611 Key 4099 value 9001 1 km-squared point cloud from Lower Teakettle showing mountainous terrain covered in a patchy conifer forest, with tall, skinny conifers clearly visible emerging from the discontinuous canopy. 6.5.2 Normalizing Tree Height to Ground To begin, we will take a look at the structural diversity of the dense mixed deciduous/evergreen forest of HARV. We’re going to choose a 40 x 40 m spatial extent for our analysis, but first we need to normalize the height values of this LiDAR point cloud from an absolute elevation above mean sea level to height above the ground using the lidR::lasnormalize() function. This function relies on spatial interpolation, and therefore we want to perform this step on an area that is quite a bit larger than our area of interest to avoid edge effects. To be safe, we will clip out an area of 200 x 200 m, normalize it, and then clip out our smaller area of interest. # Correct for elevation #We&#39;re going to choose a 40 x 40 m spatial extent, which is the #extent for NEON base plots. #First set the center of where you want the plot to be (note easting #and northing works in units of m because these data are in a UTM #proejction as shown in the summary above). x &lt;- 727500 #easting y &lt;- 4702500 #northing #Cut out a 200 x 200 m buffer by adding 100 m to easting and #northing coordinates (x,y). data.200m &lt;- lasclipRectangle(HARV, xleft = (x - 100), ybottom = (y - 100), xright = (x + 100), ytop = (y + 100)) #Correct for ground height using a kriging function to interpolate #elevation from ground points in the .laz file. #If the function will not run, then you may need to checkfor outliers #by adjusting the &#39;drop_z_&#39; arguments when reading in the .laz files. dtm &lt;- grid_terrain(data.200m, 1, kriging(k = 10L)) data.200m &lt;- lasnormalize(data.200m, dtm) #Will often give a warning if not all points could be corrected, #but visually check to see if it corrected for ground height. lidR::plot(data.200m) #There&#39;s only a few uncorrected points and we&#39;ll fix these in #the next step. #Clip 20 m out from each side of the easting and northing #coordinates (x,y). data.40m &lt;- lasclipRectangle(data.200m, xleft = (x - 20), ybottom = (y - 20), xright = (x + 20), ytop = (y + 20)) data.40m@data$Z[data.40m@data$Z &lt;= .5] &lt;- NA #This line filters out all z_vals below .5 m as we are less #interested in shrubs/trees. #You could change it to zero or another height depending on interests. #visualize the clipped plot point cloud lidR::plot(data.40m) 40 meter by 40 meter point cloud from Harvard Forest showing a cross-section of the forest structure with a complex canopy- and sub-canopy structure with many rounded crowns, characteristic of a deciduous-dominated section of forest. 6.6 Calculating Structural Diversity Metrics Now that we have our area of interest normalized and clipped, we can proceed with calculating our structural diversity metrics. 6.6.1 GENERATE CANOPY HEIGHT MODEL (CHM) (i.e. a 1 m2 raster grid of vegetations heights) # Structural diversity metrics #res argument specifies pixel size in meters and dsmtin is #for raster interpolation chm &lt;- grid_canopy(data.40m, res = 1, dsmtin()) #visualize CHM lidR::plot(chm) 6.6.1.1 MEAN OUTER CANOPY HEIGHT (MOCH) #calculate MOCH, the mean CHM height value mean.max.canopy.ht &lt;- mean(chm@data@values, na.rm = TRUE) 6.6.1.2 MAX CANOPY HEIGHT #calculate HMAX, the maximum CHM height value max.canopy.ht &lt;- max(chm@data@values, na.rm=TRUE) 6.6.1.3 RUMPLE #calculate rumple, a ratio of outer canopy surface area to #ground surface area (1600 m^2) rumple &lt;- rumple_index(chm) 6.6.1.4 TOP RUGOSITY #top rugosity, the standard deviation of pixel values in chm and #is a measure of outer canopy roughness top.rugosity &lt;- sd(chm@data@values, na.rm = TRUE) 6.6.1.5 DEEP GAPS &amp; DEEP GAP FRACTION #number of cells in raster (also area in m2) cells &lt;- length(chm@data@values) chm.0 &lt;- chm chm.0[is.na(chm.0)] &lt;- 0 #replace NAs with zeros in CHM #create variable for the number of deep gaps, 1 m^2 canopy gaps zeros &lt;- which(chm.0@data@values == 0) deepgaps &lt;- length(zeros) #number of deep gaps #deep gap fraction, the number of deep gaps in the chm relative #to total number of chm pixels deepgap.fraction &lt;- deepgaps/cells 6.6.1.6 COVER FRACTION #cover fraction, the inverse of deep gap fraction cover.fraction &lt;- 1 - deepgap.fraction 6.6.1.7 HEIGHT SD The standard deviation of height values for all points in the plot point cloud #height SD, the standard deviation of height values for all points #in the plot point cloud vert.sd &lt;- cloud_metrics(data.40m, sd(Z, na.rm = TRUE)) #SD of VERTICAL SD of HEIGHT #rasterize plot point cloud and calculate the standard deviation #of height values at a resolution of 1 m^2 sd.1m2 &lt;- grid_metrics(data.40m, sd(Z), 1) #standard deviation of the calculated standard deviations #from previous line #This is a measure of internal and external canopy complexity sd.sd &lt;- sd(sd.1m2[,3], na.rm = TRUE) #some of the next few functions won&#39;t handle NAs, so we need #to filter these out of a vector of Z points Zs &lt;- data.40m@data$Z Zs &lt;- Zs[!is.na(Zs)] 6.6.1.8 ENTROPY Quantifies diversity &amp; evenness of point cloud heights #by = 1 partitions point cloud in 1 m tall horizontal slices #ranges from 0-1, with 1 being more evenly distributed points #across the 1 m tall slices entro &lt;- entropy(Zs, by = 1) 6.6.1.9 GAP FRACTION PROFILE Gap fraction profile, assesses the distribution of gaps in the canopy volume #dz = 1 partitions point cloud in 1 m horizontal slices #z0 is set to a reasonable height based on the age and height of #the study sites gap_frac &lt;- gap_fraction_profile(Zs, dz = 1, z0=3) #defines gap fraction profile as the average gap fraction in each #1 m horizontal slice assessed in the previous line GFP.AOP &lt;- mean(gap_frac$gf) 6.6.1.10 VAI Leaf area density, assesses leaf area in the canopy volume #k = 0.5 is a standard extinction coefficient for foliage #dz = 1 partitions point cloud in 1 m horizontal slices #z0 is set to the same height as gap fraction profile above LADen&lt;-LAD(Zs, dz = 1, k=0.5, z0=3) #vegetation area index, sum of leaf area density values for #all horizontal slices assessed in previous line VAI.AOP &lt;- sum(LADen$lad, na.rm=TRUE) 6.6.1.11 VCI A vertical complexity index, fixed normalization of entropy metric calculated above #set zmax comofortably above maximum canopy height #by = 1 assesses the metric based on 1 m horizontal slices in #the canopy VCI.AOP &lt;- VCI(Zs, by = 1, zmax=100) We now have 13 different structural diversity metrics. Let’s organize them into a new dataframe: #OUTPUT CALCULATED METRICS INTO A TABLE #creates a dataframe row, out.plot, containing plot descriptors #and calculated metrics HARV_structural_diversity &lt;- data.frame(matrix(c(x, y, mean.max.canopy.ht, max.canopy.ht, rumple, deepgaps,deepgap.fraction, cover.fraction,top.rugosity, vert.sd, sd.sd, entro,GFP.AOP, VAI.AOP, VCI.AOP), ncol = 15)) #provide descriptive names for the calculated metrics colnames(HARV_structural_diversity) &lt;- c(&quot;easting&quot;, &quot;northing&quot;, &quot;mean.max.canopy.ht.aop&quot;, &quot;max.canopy.ht.aop&quot;, &quot;rumple.aop&quot;, &quot;deepgaps.aop&quot;, &quot;deepgap.fraction.aop&quot;,&quot;cover.fraction.aop&quot;, &quot;top.rugosity.aop&quot;, &quot;vert.sd.aop&quot;, &quot;sd.sd.aop&quot;, &quot;entropy.aop&quot;, &quot;GFP.AOP.aop&quot;, &quot;VAI.AOP.aop&quot;, &quot;VCI.AOP.aop&quot;) #View the results HARV_structural_diversity 6.6.2 Combining Everything Into One Function Now that we have run through how to measure each structural diversity metric, let’s create a convenient function to run these a little faster on the TEAK site for a comparison of structural diversity with HARV. #Let&#39;s correct for elevation and measure structural diversity for TEAK x &lt;- 316400 y &lt;- 4091700 data.200m &lt;- lasclipRectangle(TEAK, xleft = (x - 100), ybottom = (y - 100), xright = (x + 100), ytop = (y + 100)) dtm &lt;- grid_terrain(data.200m, 1, kriging(k = 10L)) data.200m &lt;- lasnormalize(data.200m, dtm) data.40m &lt;- lasclipRectangle(data.200m, xleft = (x - 20), ybottom = (y - 20), xright = (x + 20), ytop = (y + 20)) data.40m@data$Z[data.40m@data$Z &lt;= .5] &lt;- 0 plot(data.40m) 40 meter by 40 meter point cloud from Lower Teakettle showing a cross-section of the forest structure with several tall, pointed conifers separated by deep gaps in the canopy. #Zip up all the code we previously used and write function to #run all 13 metrics in a single function. structural_diversity_metrics &lt;- function(data.40m) { chm &lt;- grid_canopy(data.40m, res = 1, dsmtin()) mean.max.canopy.ht &lt;- mean(chm@data@values, na.rm = TRUE) max.canopy.ht &lt;- max(chm@data@values, na.rm=TRUE) rumple &lt;- rumple_index(chm) top.rugosity &lt;- sd(chm@data@values, na.rm = TRUE) cells &lt;- length(chm@data@values) chm.0 &lt;- chm chm.0[is.na(chm.0)] &lt;- 0 zeros &lt;- which(chm.0@data@values == 0) deepgaps &lt;- length(zeros) deepgap.fraction &lt;- deepgaps/cells cover.fraction &lt;- 1 - deepgap.fraction vert.sd &lt;- cloud_metrics(data.40m, sd(Z, na.rm = TRUE)) sd.1m2 &lt;- grid_metrics(data.40m, sd(Z), 1) sd.sd &lt;- sd(sd.1m2[,3], na.rm = TRUE) Zs &lt;- data.40m@data$Z Zs &lt;- Zs[!is.na(Zs)] entro &lt;- entropy(Zs, by = 1) gap_frac &lt;- gap_fraction_profile(Zs, dz = 1, z0=3) GFP.AOP &lt;- mean(gap_frac$gf) LADen&lt;-LAD(Zs, dz = 1, k=0.5, z0=3) VAI.AOP &lt;- sum(LADen$lad, na.rm=TRUE) VCI.AOP &lt;- VCI(Zs, by = 1, zmax=100) out.plot &lt;- data.frame( matrix(c(x, y, mean.max.canopy.ht,max.canopy.ht, rumple,deepgaps, deepgap.fraction, cover.fraction, top.rugosity, vert.sd, sd.sd, entro, GFP.AOP, VAI.AOP,VCI.AOP), ncol = 15)) colnames(out.plot) &lt;- c(&quot;easting&quot;, &quot;northing&quot;, &quot;mean.max.canopy.ht.aop&quot;, &quot;max.canopy.ht.aop&quot;, &quot;rumple.aop&quot;, &quot;deepgaps.aop&quot;, &quot;deepgap.fraction.aop&quot;, &quot;cover.fraction.aop&quot;, &quot;top.rugosity.aop&quot;,&quot;vert.sd.aop&quot;,&quot;sd.sd.aop&quot;, &quot;entropy.aop&quot;, &quot;GFP.AOP.aop&quot;, &quot;VAI.AOP.aop&quot;, &quot;VCI.AOP.aop&quot;) print(out.plot) } TEAK_structural_diversity &lt;- structural_diversity_metrics(data.40m) 6.6.3 Comparing Metrics Between Forests How does the structural diversity of the evergreen TEAK forest compare to the mixed deciduous/evergreen forest from HARV? Let’s combine the result data.frames for a direct comparison: combined_results=rbind(HARV_structural_diversity, TEAK_structural_diversity) # Add row names for clarity row.names(combined_results)=c(&quot;HARV&quot;,&quot;TEAK&quot;) # Take a look to compare combined_results 6.7 Matching GEDI waveforms with NEON AOP LiDAR pointclouds author: Donal O’Leary GEDI has amazing coverage around the globe, but is limited in its spatial resolution. Here, we extract NEON pointcloud data corresponding to individual GEDI waveforms to better understand how the GEDI return waveform gets its shape. 6.7.1 Learning Objectives After completing this section you will be able to: Search for GEDI data based on a NEON site bounding box Extract NEON LiDAR pointcloud data corresponding to a specific GEDI footprint Visualize NEON and GEDI LiDAR data together in 3D 6.7.2 Things You’ll Need To Complete This GEDI Section 6.7.3 R Packages to Install Prior to starting the tutorial ensure that the following packages are installed. raster: install.packages(&quot;raster&quot;) rGEDI: install.packages(&quot;rGEDI&quot;) sp: install.packages(&quot;sp&quot;) sf: install.packages(&quot;sf&quot;) lidR: install.packages(&quot;lidR&quot;) neonUtilities: install.packages(&quot;neonUtilities&quot;) viridis: install.packages(&quot;viridis&quot;) maptools: install.packages(&quot;maptools&quot;) 6.7.4 Example Data Set 6.7.4.1 GEDI Example Data Subset This dataset has been subset from a full GEDI orbit retaining only the ‘shots’ that correspond to a single 1km AOP tile, and only the ‘datasets’ (attributes) that are needed to visualize the GEDI waveform as shown below. Download GEDI Example Dataset 6.7.4.2 Datum difference between WGS84 and NAD83 This dataset describes the differences between two common standards for vertical data in North America. Download Datum Difference Dataset 6.7.5 Getting Started In this section we will compare NEON and GEDI LiDAR data by comparing the information that they both capture in the same location. NEON data are actually one of the datasets used by the GEDI mission to calibrate and validate GEDI waveforms, so this makes for a valuable comparison! In order to compare these data, we will need to download GEDI data that overlap a NEON site. Fortunately, Carlos Silva et al. have made a convenient R package clled rGEDI and this excellent tutorial hosted on CRAN desribing how to work with GEDI data in R. However, GEDI data are currently only available to download per complete orbit, which means that the vast majority of the orbit’s data does not fall within a NEON site. The GEDI orbit datasets come in HDF5 data format, and contain about 7Gb of data, so you may want to run the first few sections of this tutorial to get the GEDI download started, and refresh your memory of HDF5 files with NEON’s Intro to HDF5 series. First, we will load the required libraries and set our working directory: library(rGEDI) library(raster) library(sp) library(sf) ## Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0 library(rgl) library(lidR) library(neonUtilities) library(viridis) ## Loading required package: viridisLite library(maptools) ## Checking rgeos availability: TRUE #wd &lt;- &quot;./data&quot; # This will depend on your local environment #setwd(wd) Next, we will download a Canopy Height Model (CHM) tile from the Wind River Experiemental Forest (WREF) using the byTileAOP() function to use as a preliminary map on which to overlay the GEDI data. There is one particularly interesting mosaic tile which we will select using the easting and northing arguments. We then load the raster into R and make a simple plot. For more information about CHMs, please see our tutorial What is a CHM? # Define the SITECODE as a variable because # we will use it several times in this tutorial SITECODE &lt;- &quot;WREF&quot; byTileAOP(dpID = &quot;DP3.30015.001&quot;, site = SITECODE, year = 2017, easting = 580000, northing = 5075000, check.size = F, savepath = &#39;./data&#39;) ## Downloading files totaling approximately 4.0 MB ## Downloading 6 files ## | | | 0% | |============== | 20% | |============================ | 40% | |========================================== | 60% | |======================================================== | 80% | |======================================================================| 100% ## Successfully downloaded 6 files. ## NEON_D16_WREF_DP3_580000_5075000_CHM.tif downloaded to ./data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/L3/DiscreteLidar/CanopyHeightModelGtif ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.kml downloaded to ./data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/kmls ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.dbf downloaded to ./data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.shp downloaded to ./data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.shx downloaded to ./data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.prj downloaded to ./data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps chm &lt;- raster(&#39;./data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/L3/DiscreteLidar/CanopyHeightModelGtif/NEON_D16_WREF_DP3_580000_5075000_CHM.tif&#39;) plot(chm) As you can see, this particular CHM is showing a conspicuous, triangle-shaped clearcut in this section of the experimental forest, where the tree canopy is much shorter than the towering 60m+ trees in the undisturbed areas. This variation will give us a variety of forest structures to investigate. 6.7.6 Downloading GEDI data This next section on downloading and working with GEDI data is loosely based on the excellent rGEDI package tutorial posted on CRAN here. If you would prefer to avoid a large download (&gt;7Gb) of a full GEDI orbit, you can forego these steps and use the example GEDI dataset here, and skip to the readLevel1B() function below. The Global Ecosystem Dynamics Investigation (GEDI) is a NASA mission with the primary data collection being performed by a novel waveform lidar instrument mounted on the International Space Station (ISS). Please see this open-access paper published in Science of Remote Sensing that describes this mission in detail. The ISS orbits around the world every 90 minutes, and can be tracked on this cool NASA website. As described here on the Land Processes Distributed Active Archive Center (LP DAAC), “the sole GEDI observable is the waveform from which all other data products are derived. Each waveform is captured with a nominal ~30 m diameter.” As of the date of publication, GEDI data are only offered in HDF5 format, with each file containing the data for a full orbit. The LP DAAC has developed a tool that allows researchers to input a bounding box, which will return a list of every orbit that has a “shot” (waveform return) that falls within that box. Unfortunately, at this time, the tool will not subset out the specific shots that fall within that bounding box; you must download the entire orbit (~7Gb). This functionality may be improved in the future. Our next few steps involve defining our bounding box, requesting the list of GEDI orbits that contain data relevant to that bounding box, and downloading those data. Let’s focus on the extent of our CHM that we downloaded above - but we will first need to re-project the CHM from its UTM projection into WGS84. To do so, we will refer to the EPSG code for WGS84. To look up any of these codes, please see the amazing resource spatialrerference.org. # Project CHM to WGS84 chm_WGS = projectRaster(chm, crs=CRS(&quot;+init=epsg:4326&quot;)) # Study area boundary box coordinates ul_lat&lt;- extent(chm_WGS)[4] lr_lat&lt;- extent(chm_WGS)[3] ul_lon&lt;- extent(chm_WGS)[1] lr_lon&lt;- extent(chm_WGS)[2] Next, we use that bounding box information as an input to the gedifinder() funciton. # Specifying the optional date range, if desired daterange=c(&quot;2019-03-25&quot;, #first date of GEDI availability &quot;2020-07-15&quot;) # Get path to GEDI data # These lines use an API request to determine which orbits are available # that intersect the bounding box of interest. # Note that you still must download the entire orbit and then subset to # your area of interest! gLevel1B &lt;- gedifinder(product=&quot;GEDI01_B&quot;, ul_lat, ul_lon, lr_lat,lr_lon, version=&quot;001&quot;,daterange=NULL) # # # View list of available data gLevel1B ## [1] &quot;https://e4ftl01.cr.usgs.gov/GEDI/GEDI01_B.001/2019.10.07/GEDI01_B_2019280204828_O04642_T03216_02_003_01.h5&quot; ## [2] &quot;https://e4ftl01.cr.usgs.gov/GEDI/GEDI01_B.001/2019.07.25/GEDI01_B_2019206022612_O03482_T00370_02_003_01.h5&quot; ## [3] &quot;https://e4ftl01.cr.usgs.gov/GEDI/GEDI01_B.001/2019.06.08/GEDI01_B_2019159013955_O02752_T01597_02_003_01.h5&quot; Great! There are several GEDI orbits available that have at least 1 ‘shot’ within our bounding box of interest. For more information about GEDI filename conventions, and other valuable information about GEDI data, see this page on the LP DAAC. However, as mentioned before, each of these files are quite large (~7Gb), so let’s focus on just the first one for now. # Downloading GEDI data, if you haven&#39;t already # Note that this will download a large file (&gt;7Gb) which # can be avoided by using the example dataset provided above. # wd=&#39;./data/&#39; # if(!file.exists(paste0(wd,basename(gLevel1B[2])))){ # gediDownload(filepath=gLevel1B[2],outdir=wd) # } Next, we use the rGEDI package to read in the GEDI data. First, we need to make a ‘gedi.level1b’ object using the readLevel1B() function. Next, we extract the geographic position of the center of each shot from the ‘gedi.level1b’ object using the getLevel1BGeo() function, and display the head of the resulting table. gedilevel1b&lt;-readLevel1B(level1Bpath = file.path(&#39;./data/NEON_WREF_GEDI_subset.h5&#39;)) #gedilevel1b&lt;-readLevel1B(level1Bpath = file.path(wd, &quot;GEDI01_B_2019206022612_O03482_T00370_02_003_01.h5&quot;)) level1bGeo&lt;-getLevel1BGeo(level1b=gedilevel1b,select=c(&quot;elevation_bin0&quot;)) ## | | | 0% | |============ | 17% | |======================= | 33% | |=================================== | 50% | |=============================================== | 67% | |========================================================== | 83% | |======================================================================| 100% head(level1bGeo) ## shot_number latitude_bin0 latitude_lastbin longitude_bin0 longitude_lastbin ## 1: 1 45.82541 45.82542 -121.9700 -121.9699 ## 2: 2 45.82566 45.82567 -121.9693 -121.9693 ## 3: 3 45.82590 45.82591 -121.9687 -121.9686 ## 4: 4 45.82615 45.82616 -121.9680 -121.9680 ## 5: 5 45.82639 45.82641 -121.9674 -121.9674 ## 6: 6 45.82664 45.82665 -121.9667 -121.9667 ## elevation_bin0 ## 1: 505.6233 ## 2: 483.9717 ## 3: 505.3455 ## 4: 496.2213 ## 5: 500.5322 ## 6: 494.9825 6.7.7 Plot GEDI footprints on CHM Let’s visualize where the GEDI footprints are located on the CHM tile. To do so, we will need to first convert the GEDI data into a spatial object. For this example, we will use a ‘spatial features’ object type from the ‘sp’ package: # drop any shots with missing latitude/longitude values level1bGeo = level1bGeo[!is.na(level1bGeo$latitude_bin0)&amp; !is.na(level1bGeo$longitude_bin0),] # Convert the GEDI data.frame into an &#39;sf&#39; object level1bGeo_spdf&lt;-st_as_sf(level1bGeo, coords = c(&quot;longitude_bin0&quot;, &quot;latitude_bin0&quot;), crs=CRS(&quot;+init=epsg:4326&quot;)) # crop to the CHM that is in WGS84 level1bgeo_WREF=st_crop(level1bGeo_spdf, chm_WGS) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries Next, project the GEDI geospatial data into the UTM zone that the CHM is within (Zone 10 North). These data come as the point location at the center of the GEDI footprint, so we next convert the GEDI footprint center (lat/long) into a circle using the buffer() function. Finally, we can plot the CHM and overlay the GEDI footprint circles, and label with the last three digits of the ‘shot’ number. # project to UTM level1bgeo_WREF_UTM=st_transform( level1bgeo_WREF, crs=chm$NEON_D16_WREF_DP3_580000_5075000_CHM@crs) # buffer the GEDI shot center by a radius of 12.5m # to represent the full 25m diameter GEDI footprint level1bgeo_WREF_UTM_buffer=st_buffer(level1bgeo_WREF_UTM, dist=12.5) # plot CHM and overlay GEDI data plot(chm) plot(level1bgeo_WREF_UTM_buffer, add=T, col=&quot;transparent&quot;) # add labes with the last three digits of the GEDI shot_number pointLabel(st_coordinates(level1bgeo_WREF_UTM), labels=level1bgeo_WREF_UTM$shot_number, cex=1) 6.7.8 Extract Waveform for a single Shot Let’s take a look at a waveform for a single GEDI shot. We can select a shot by using its shot_number as shown below. Note, however, that the example data subset shots have been re-numbered, and those numbers will not correspond with full orbit GEDI data. # Extracting GEDI full-waveform for a given shot_number wf &lt;- getLevel1BWF(gedilevel1b,shot_number = 20) # or, if using a full GEDI dataset, # wf &lt;- getLevel1BWF(gedilevel1b,shot_number = level1bgeo_WREF_UTM_buffer$shot_number[which(level1bgeo_WREF_UTM_buffer$shot_number==1786)]) # Save current plotting parameters to revert to oldpar &lt;- par() # Set up plotting paramters par(mfrow = c(1,2), mar=c(4,4,1,1), cex.axis = 1.5) # Plot filled-in waveform plot(wf, relative=FALSE, polygon=TRUE, type=&quot;l&quot;, lwd=2, col=&quot;forestgreen&quot;, xlab=&quot;Waveform Amplitude&quot;, ylab=&quot;Elevation (m)&quot;) grid() #add a grid to the plot # Plot a simple line with no fill plot(wf, relative=TRUE, polygon=FALSE, type=&quot;l&quot;, lwd=2, col=&quot;forestgreen&quot;, xlab=&quot;Waveform Amplitude (%)&quot;, ylab=&quot;Elevation (m)&quot;) grid()#add a grid to the plot # Revert plotting parameters to previous values. par(oldpar) This waveform shows some noise above and below the main ecosystem return, with a fairly dense canopy around 370m elevation, an a characteristic ground return spike at about 340m. While the GEDI data are extremely valuable and offer a near-global coverage, it is hard to get a good sense of what the ecosystem really looks like from this GEDI waveform. Let’s download some NEON AOP pointcloud data to pair up with this waveform to get a better sense of what GEDI is reporting. 6.7.9 Download and Plot NEON AOP LiDAR pointcloud data Here, we will use the byTileAOP() function from the ‘neonUtilities’ package to download the classified pointcloud mosaic data (DP1.30003.001). Since this is a mosaic tile like the CHM, we can just pass this function the lower left corner of the CHM tile to get the corresponding lidar pointcloud mosaic tile for our analysis. # Download the pointcloud data if you don&#39;t have it already if (!file.exists(&#39;./data/DP1.30003.001/2017/FullSite/D16/2017_WREF_1/L1/DiscreteLidar/ClassifiedPointCloud/NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.laz&#39;)){ byTileAOP(dpID = &quot;DP1.30003.001&quot;, site = SITECODE, year = 2017, easting = extent(chm)[1], northing=extent(chm)[3], check.size = F, savepath = &#39;./data/&#39;) # Edit savepath as needed } After downloading the point cloud data, let’s read them into our R session using the readLAS() function from the lidaR package, and plot them in 3D. Note, you may need to update your ‘XQuartz’ if you are using a Mac. WREF_LAS=readLAS(&quot;./data/DP1.30003.001/2017/FullSite/D16/2017_WREF_1/L1/DiscreteLidar/ClassifiedPointCloud/NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.laz&quot;) lidR::plot(WREF_LAS) Oh, yikes! There are a lot of outliers above the actual forest, and a few below. Let’s use some simple statistics to throw out those outliers. We will first calculate the mean and standard deviation for the verical axis, and then use the ‘filter’ options of the readLAS() function to eliminate the vertical outliers. # ### remove outlier lidar point outliers using mean and sd statistics Z_sd=sd(WREF_LAS@data$Z) Z_mean=mean(WREF_LAS@data$Z) # make filter string in form filter = &quot;-drop_z_below 50 -drop_z_above 1000&quot; # You can increase or decrease (from 4) the number of sd&#39;s to filter outliers f = paste(&quot;-drop_z_below&quot;,(Z_mean-4*Z_sd),&quot;-drop_z_above&quot;,(Z_mean+4*Z_sd)) # Read in LAS file, trimming off vertical outlier points WREF_LAS=readLAS(&quot;./data/DP1.30003.001/2017/FullSite/D16/2017_WREF_1/L1/DiscreteLidar/ClassifiedPointCloud/NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.laz&quot;, filter = f) #Plot the full LiDAR point cloud mosaic tile (1km^2) plot(WREF_LAS) Ahhh, that’s better. 6.7.10 Clip AOP LiDAR Pointcloud to GEDI footprints We can now use the GEDI footprint circles (that we made using the buffer() function) to clip out the NEON AOP LiDAR points that correspond with the GEDI footprints: # Clip the pointcloud by the GEDI footprints created by buffering above. # This makes a &#39;list&#39; of LAS objects WREF_GEDI_footprints=lasclip(WREF_LAS, geometry = level1bgeo_WREF_UTM_buffer) # we can now plot individual footprint clips plot(WREF_GEDI_footprints[[8]]) 6.7.11 Plot GEDI Waveform with AOP Pointcloud in 3D space Now that we can extract individual waveforms, and the AOP LiDAR pointcloud that corresponds with each GEDI waveform, let’s see if we can plot them both in 3D space. We already know how to plot the AOP LiDAR points, so let’s write a function to draw the GEDI waveform, too, using the points3d() function: for(shot_n in c(20)){ # First, plot the NEON AOP LiDAR clipped to the GEDI footprint # We save the plot as an object &#39;p&#39; which gives the (x,y) offset for the lower # left corner of the plot. The &#39;rgl&#39; package offsets all (x,y) locations # to this point, so we will need to subtract these values from any other # (x,y) points that we want to add to the plot p=plot(WREF_GEDI_footprints[[shot_n]]) # Extract the specific waveform from the GEDI data wf &lt;- getLevel1BWF(gedilevel1b,shot_number = level1bgeo_WREF$shot_number[shot_n]) # Make a new data.frame &#39;d&#39; to convert the waveform data coordinates into 3D space d=wf@dt # normalize rxwaveform to 0-1 d$rxwaveform=d$rxwaveform-min(d$rxwaveform) d$rxwaveform=d$rxwaveform/max(d$rxwaveform) # Add xy data in UTMs, and offset lower left corner of &#39;p&#39; d$x=st_coordinates(level1bgeo_WREF_UTM[shot_n,])[1]-p[1] d$y=st_coordinates(level1bgeo_WREF_UTM[shot_n,])[2]-p[2] # Make a new column &#39;x_wf&#39; where we place the GEDI waveform in space with the # NEON AOP LiDAR data, we scale the waveform to 30m in the x-dimension, and # offset by 12.5m (the radius of the GEDI footprint) in the x-dimension. d$x_wf=d$x+d$rxwaveform*30+12.5 # Add GEDI points to 3D space in &#39;green&#39; color points3d(x=d$x_wf, y=d$y, z=d$elevation, col=&quot;green&quot;, add=T) } Whoa, it looks like there is a bad vertical mismatch between those two data sources. This is because the two data sources have a different vertical datum. The NEON AOP data are delivered in the GEOID12A datum, while the GEDI data are delivered in the WGS84 native datum. We will need to convert one to the other to get them to line up correctly. 6.7.12 Datum, Geoid, and how to best measure the Earth We are seeing a vertical mismatch between the NEON and GEDI LiDAR data sources because they are using different standards for measuring the Earth. Here, we will briefly describe the main differences between these two datum models, and then show how to correct for this discrepancy. 6.7.12.1 WGS84 As described in this great Wikipedia article the World Geodedic System (WGS) is a global standard for cartography, geodesy, and navigation including GPS. Most people refer to “WGS84,” which is the latest revision to this system in 1984. To describe elevation, WGS84 uses an idealized mathematical model to describe the ‘oblate spheroid’ of the Earth (basically, it looks kind of like a sphere that is wider around the equator and shorter from pole to pole). This model, called a ‘datum,’ defines the relative elevational surface of the Earth. However, the Earth isn’t exactly this idealized shape - it has a lot of undulations caused by topogrophy and local differences in its gravitational field. In the locations where the natural variations of the Earth do not coencide with the mathematical model, it can be harder to accurately describe the elevation of certain landforms and structures. Still, for a global datum, WGS84 does a great job overall. 6.7.12.2 GEOID12A While WGS84 is a convenient global standard, precisely describing location information is a common problem in geography, and is often remedied by using a local datum or projection. For latitude and longitude spatial information in North America, this is often done by using the North American Datum (NAD83) or various Universal Transverse Mercator (UTM) or State Plane projections based on the NAD83 datum. The NAD83 Datum was developed in 1983 as the horizontal and geometric control datum for the United States, Canada, Mexico, and Central America. This is the official datum for the government of the United States of America, and therefore many public datasets that are specific to this geographic region use this datum. In order to further refine the vertical precision over a wide geographic area, we can model the gravitational field of the Earth - this surface is called a “Geoid”. According to NOAA, a geoid can be defined as “the equipotential surface of the Earth’s gravity field which best fits, in a least squares sense, global mean sea level.” For a nice description of the North American geoid, please see this page on the NOAA website written by Dr. Allen Joel Anderson. As described by Dr. Anderson, this gravitational field is changing all the time, as can global mean sea level (due to melting glaciers, etc.). Therefore local and regional geoids must be updated regularly to reflect these changes, as shown in this list of NOAA geoids. One recent standard from 2012 is &lt;GEOID12A - this is the geoid selected by NEON as the vertical reference for the Z-dimension of our LiDAR data. Note that GEOID12B superscedes GEOID12A, however they are identical everywhere except for Puerto Rico and the US Virgin Islands according to NOAA. If you are working with any NEON data from Puerto Rico, please be aware that these data are delivered in GEOID12A format and may need to be converted to GEOID12B for some purposes. For more information about these standards, please see this ESRI help document. 6.7.13 Aligning the Vertical Datum AOP data have a relatively small area of coverage, and are therefore delivered with the coordinate reference system (CRS) in the UTM zone in which they were collected, set to the GEOID12A vertical datum (roughly equaling mean sea level). Meanwhile, GEDI data are global, so they are delivered with the common WGS84 ellipsoidal refrence datum. We need to align these vertical datums to ensure that the two data products line up correctly. However, this is not a trivial step, nor as simple as a reprojection. In this example, we will keep the NEON LiDAR data in its current datum, and convert the vertical position of the GEDI data from WGS84 into GEOID12A vertical position. NOAA has a useful tool called Vdatum that will convert from one datum to another. For this example, we will use Vdatum to convert from WGS84 to NAD83. Seeing as GEOID12A is based on the NAD83 datum, we can first convert the GEDI data’s vertical position from WGS84 to NAD83 datum, then apply the offset between NAD83 and GEOID12A. Rather than use the Vdatum tool for each point, we will use a raster created by NEON scientists that reports the vertical difference between WGS84 and NAD83 for all points in the conterminous USA. Please visit this Sharepoint page (same as the link at the top of this page) to download the raster. We will then read it into our R session: # Edit this filepath as needed for your machine datum_diff=raster(&#39;./data/WGS84_NAD83_seperation.tif&#39;) 6.7.13.1 GEOID12A Height Model GEOID12A is a surface model that is described in terms of its relative height compared to NAD83. You can use this interactive webpage to find the geoid height for any location within North America. However, that would be combersome to have to use this webpage for every location. Instead, we will download a &lt;a href=&quot;https://www.ngs.noaa.gov/GEOID/GEOID12A/GEOID12A_CONUS.shtml&gt;binary file from the NOAA website that describes this geoid’s height, and convert that into a raster similar to the one that we just downloaded above. We have included comments here from the NOAA website about the structure of the binary file. We use this information to extract the dimensions of this dataset in order to construct a raster in R from these binary data. # Download binary file of offset from GEOID12A to NAD83 if(!file.exists(&#39;./data/g2012au0.bin&#39;)){ download.file(&quot;https://www.ngs.noaa.gov/PC_PROD/GEOID12A/Format_unix/g2012au0.bin&quot;, destfile = &#39;./data/g2012au0.bin&#39;) } # Read header information. See https://www.ngs.noaa.gov/GEOID/GEOID12B/g2012Brme.txt # File Structure # --------------- # The files (ASCII and binary) follow the same structure of a one-line header # followed by the data in row-major format. The one-line header contains four # double (real*8) words followed by three long (int*4) words. # These parameters define the geographic extent of the area: # # SLAT: Southernmost North latitude in whole degrees. # Use a minus sign (-) to indicate South latitudes. # WLON: Westernmost East longitude in whole degrees. # DLAT: Distance interval in latitude in whole degrees # (point spacing in E-W direction) # DLON: Distance interval in longitude in whole degrees # (point spacing in N-S direction) # NLAT: Number of rows # (starts with SLAT and moves northward DLAT to next row) # NLON: Number of columns # (starts with WLON and moves eastward DLON to next column) # IKIND: Always equal to one (indicates data are real*4 and endian condition) to.read = file(&#39;./data/g2012au0.bin&#39;, &quot;rb&quot;) header1=readBin(to.read, double(), endian = &quot;big&quot;, n=4) header1 #SLAT, WLON, DLAT, DLON ## [1] 24.00000000 230.00000000 0.01666667 0.01666667 header2=readBin(to.read, integer(), endian = &quot;big&quot;, n=3) header2 #NLAT, NLON, IKIND ## [1] 2041 4201 1 GEOID12A_diff_vals=readBin(to.read, n=header2[1]*header2[2], numeric(), endian = &quot;big&quot;, size=4) # Create a new raster using the dimensions extracted from the headers GEOID12A_diff_rast &lt;- raster(ncol=header2[2], nrow=header2[1], xmn=header1[2]-360, xmx=header1[2]+(header1[4]*header2[2])-360, ymn=header1[1], ymx=header1[1]+(header1[3]*header2[1]), crs = crs(datum_diff)) GEOID12A_diff_rast &lt;- setValues(GEOID12A_diff_rast, values = GEOID12A_diff_vals) # we need to use the &#39;flip&#39; function to put the map &#39;upright&#39; because R expects to see raster values from the top left corner and fills by rows, but this dataset is delivered in sucha a way that it describes the bottom left corner and fills by rows up to the top of the image (this is actually the convention for most traditional remote sensing software - and leads to a similar problem that is explained in the Hyperspectral tutorial series.) GEOID12A_diff_rast &lt;- flip(GEOID12A_diff_rast, &#39;y&#39;) ## let&#39;s crop out only CONUS for plotting purposes - we will still refer to the fill image when extracting values. diff_resp=resample(datum_diff, GEOID12A_diff_rast) # resample to match pixel size/registration for cropping diff_resp=crop(diff_resp, GEOID12A_diff_rast) GEOID12A_diff_rast=crop(GEOID12A_diff_rast, extent(diff_resp)) GEOID12A_diff_rast_mask=mask(GEOID12A_diff_rast, diff_resp) Now that we have the two offset rasters, let’s plot them together to compare their spatial patterns. par(mfrow=c(2,1), mar=c(2.5,2.5, 3.5,1)) plot(GEOID12A_diff_rast_mask, col=viridis(100),main=&quot;GEOID12A minus NAD83 (m)&quot;) plot(diff_resp, main=&quot;WGS84 minus NAD83 (m)&quot;) As you can see, the differences between the GEOID12A geoid, and the NAD83 sphereoid vary quite a lot across space, especially in mountainous areas. The magnitude of these differences is also large, upwards of 35m in some areas. Meanwhile, the differences between the NAD83 and WGS84 sphereoids shows a smooth gradient that is relatively small, with total difference less than 2m across the Conterminous USA. 6.7.14 Extract vertical offset for GEDI shots Now that we know the relative vertical offsets between WGS84, NAD83, and GEOID12A for all of the conterminous USA, we can use the extract() function to retrieve those relative offsets for any location. By combining those offsets together, we can finally rectify the vertical position of the NEON and GEDI LiDAR data. ***** DO I need to convert GEDI x/y locations from WGS84 to NAD83?? ***** # Make a new DF to store the GEDI footprint (x,y) locations, and the relative datum/geoid offsets footprint_df=as.data.frame( cbind(level1bgeo_WREF$longitude_lastbin, level1bgeo_WREF$latitude_lastbin)) WGS_NAD_diff &lt;- extract(datum_diff, footprint_df) GEOID12A_NAD_diff &lt;- extract(GEOID12A_diff_rast_mask,footprint_df) # Add together the offsets to calculate a vector of net differences in elevation net_diff=WGS_NAD_diff+GEOID12A_NAD_diff 6.7.15 Plot vertically corrected GEDI waveform in 3D Now that we have a vertical offset for each of the GEDI footprints, let’s try again to plot the NEON AOP pointcloud data with the GEDI waveform. # You can enter whichever shots that you want to plot in the for loop here #for(shot_n in 1:length(WREF_GEDI_footprints)){ for(shot_n in c(20)){ # First, plot the NEON AOP LiDAR clipped to the GEDI footprint # We save the plot as an object &#39;p&#39; which gives the (x,y) offset for the lower # left corner of the plot. The &#39;rgl&#39; package offsets all (x,y) locations # to this point, so we will need to subtract these values from any other # (x,y) points that we want to add to the plot p=plot(WREF_GEDI_footprints[[shot_n]]) # Extract the specific waveform from the GEDI data wf &lt;- getLevel1BWF(gedilevel1b,shot_number = level1bgeo_WREF$shot_number[shot_n]) # Make a new data.frame &#39;d&#39; to convert the waveform data coordinates into 3D space d=wf@dt # normalize rxwaveform to 0-1 d$rxwaveform=d$rxwaveform-min(d$rxwaveform) d$rxwaveform=d$rxwaveform/max(d$rxwaveform) # Add xy data in UTMs, and offset lower left corner of &#39;p&#39; d$x=st_coordinates(level1bgeo_WREF_UTM[shot_n,])[1]-p[1] d$y=st_coordinates(level1bgeo_WREF_UTM[shot_n,])[2]-p[2] # Make a new column &#39;x_wf&#39; where we place the GEDI waveform in space with the # NEON AOP LiDAR data, we scale the waveform to 30m in the x-dimension, and # offset by 12.5m (the radius of the GEDI footprint) in the x-dimension. d$x_wf=d$x+d$rxwaveform*30+12.5 # Add GEDI points to 3D space in &#39;green&#39; color # This time, subtracting the elevation difference for that shot points3d(x=d$x_wf, y=d$y, z=d$elevation-net_diff[shot_n], col=&quot;green&quot;, add=T) } 6.7.16 Optional - NEON base plots You may also be interested to see if any of the GEDI footprints intersect a NEON base plot, which would allow for a direct comparison of the GEDI waveform with many of the datasets which are collected within the base plots, such as the vegetation structure data product containing height, DBH, and species identification of all trees &gt;10cm DBH. While it is statistically pretty unlikely that a GEDI footprint will intersect with your base plot of interest, it is possible that some GEDI footprint will intersetc with some base plot in your study area, so we may as well take a look: setwd(&quot;./data/&quot;) # This will depend upon your local environment # Download the NEON TOS plots polygons directly from the NEON website download.file(url=&quot;https://data.neonscience.org/api/v0/documents/All_NEON_TOS_Plots_V8&quot;, destfile=&quot;All_NEON_TOS_Plots_V8.zip&quot;) unzip(&quot;All_NEON_TOS_Plots_V8.zip&quot;) NEON_all_plots &lt;- st_read(&#39;All_NEON_TOS_Plots_V8/All_NEON_TOS_Plot_Polygons_V8.shp&#39;) ## Reading layer `All_NEON_TOS_Plot_Polygons_V8&#39; from data source `/Users/kdw223/Research/katharynduffy.github.io/data/All_NEON_TOS_Plots_V8/All_NEON_TOS_Plot_Polygons_V8.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 3841 features and 36 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -156.6516 ymin: 17.9514 xmax: -66.82358 ymax: 71.3169 ## CRS: 4326 # Select just the WREF site SITECODE = &#39;WREF&#39; base_plots_SPDF &lt;- NEON_all_plots[ (NEON_all_plots$siteID == SITECODE)&amp; (NEON_all_plots$subtype == &#39;basePlot&#39;),] rm(NEON_all_plots) base_crop=st_crop(base_plots_SPDF, extent(chm_WGS)) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries plot(chm_WGS) plot(base_crop$geometry, border = &#39;blue&#39;, add=T) 6.8 NEON AOP Written Questions: Reminder: these questions are largely based on the assigned video lectures. I highly recommend you watching or re-watching them before tackling these questions. What is the overlap in the type of data generated by NEON AOP and PhenoCam. What metrics from PhenoCam could you apply to NEON AOP data, especially once there are &gt;10 years of aerial data? List 4 challenges to timing AOP flight campaigns. When using NEON AOP data, when should you use byTileAOP versus byFileAOP? What additional processing might you need to do if you worked with the byFileAOP data? Hint: try pulling the data in both formats (this will take a while) How do the additional bands in NEON AOP improve our contraint of Biodiversity and Ecosystem Structure relative to satelite-derived data? What role do campagins such as NEON AOP play in ‘filling the sampling gap’ of essential in situ data such as FLUXNET (Eddy Co-Variance) data? List 3 attributes of NEON AOP data that are unique to other publicly served data. Hints: Band width? Resolution? Something else? Which data product(s) derived from the NEON AOP campaign align with remotely-sensed products from NASA? Hint: start with vegetation indicies 6.9 NEON AOP Coding Lab For the purpose of this coding lab we will evaluate 2 different forested sites in 2018: Guanica (GUAN) in Puerto Rico: Bartlett Experimental Forest (BART) in New Hampshire: 1: How are these two forests similar? How are they different? (3-5 sentences) 2: Using this NEON tutorial and the tutorials we’ve covered in this textbook (hint: you did half of this workflow in your very first coding lab) pull the NEON AOP derived Canopy Height Model (CHM, DP3.30015.001) and High-resolution orthorectified camera imagery mosaic DP3.30010.001 for each forest and overlay the NEON TOS Woody Vegetation Structure DP1.10098.001 basal diameter data product to evaluate how well AOP captures trees in each forest. Generate a labeled 2x2 plot panel including: Each RGB image with basal diameter overlaid Each CHM with basal diameter overlaid Hints/reminders from section 2.9: &lt;- loadByProduct &lt;- getLocTOS &lt;- merge Write 2-3 sentences summarizing your findings and thoughts. 3: Use the byTileAOP function of the neonUtilities package to pull a subset of the descrete LiDAR pointcloud for each forest (Hint: You can feed byTileAOP Easting and Northing from your Vegetation Structure dataframe(s)). Use the structural_diversity_metrics function that you defined in section 5.6 of the textbook to process discrete return LiDAR for each site and generte structural diversity metrics. Using lidR generate a labeled 2-panel plot of your canopy height model for each forest Using lidR generate a labeled 2-panel plot of a cross-section for each forest Use section 5.6.3 Comparing Metrics Between Forests to compare each forest and generate a a clean summary table via kable Using Table 2 from LaRue, Wagner, et al. (2020) as a reference, write 1-2 paragraphs summarizing the differences in forest structural diversity between the two forests and how they may relate to your answers to Question 1. 6.10 NEON AOP Culmination Write Up Write up a 1-page derived data product or research pipeline proposal summary of a project that you might want to explore using NEON AOP data. Include the types of NEON (and other data) that you would need to implement this project. Save this summary as you will be refining and adding to your ideas over the course of the semester. Sugestions: Tables or bullet lists of specific data products An overarching high-quality figure to show how these data align One paragaph summarizing how this data or analysis is useful to you and/or the infrastructure. "],
["nasas-earth-observing-system-eos.html", "Chapter 7 NASA’s Earth Observing System (EOS) 7.1 Learning Objectives 7.2 NASA EOS Project Mission &amp; Design 7.3 NASA EOS Earth Data Assignment: 7.4 Distributed Active Archive Centers 7.5 The LPDAAC Mission: Process, Archive, Distribute, Apply 7.6 AppEEARS 7.7 Hands on: Pulling AppEEARS Data via the API 7.8 Getting Started with the AppEEARS API (Point Request) 7.9 1. Getting Started 7.10 1a. Load Packages 7.11 1b. Set Up the Output Directory 7.12 1c. Login 7.13 2. Query Available Products 7.14 2a. Search and Explore Available Products 7.15 2b. Search and Explore Available Layers 7.16 3. Submit a Point Request 7.17 3a. Compile a JSON Object 7.18 3b. Submit a Task Request 7.19 3c. Retrieve Task Status 7.20 4. Download a Request 7.21 4a. Explore Files in Request Output 7.22 4b. Download Files in a Request (Automation) 7.23 5. Explore AppEEARS Quality Service 7.24 5a. List Quality Layers 7.25 5b. Show Quality Values 7.26 5c. Decode Quality Values 7.27 6. BONUS: Load Request Output and Visualize 7.28 6a. Load a CSV 7.29 6b. Plot Results (Line/Scatter Plots)", " Chapter 7 NASA’s Earth Observing System (EOS) Estimated Time: 2 hours 7.1 Learning Objectives 7.2 NASA EOS Project Mission &amp; Design NASA’s Earth Observing System (EOS) is a coordinated series of polar-orbiting and low inclination satellites for long-term global observations of the land surface, biosphere, solid Earth, atmosphere, and oceans. As a major component of the Earth Science Division of NASA’s Science Mission Directorate, EOS enables an improved understanding of the Earth as an integrated system. Review NASA EOS’s Mission Profile Completeed Missions Current Missions Future Missions Credit: NASA’s Goddard Space Flight Center 7.3 NASA EOS Earth Data Assignment: The EOSDIS Earthdata Login provides a centralized and simplified mechanism for user registration and profile management for all EOSDIS system components. End users may register and edit their profile information in one location allowing them access to the wide array of EOSDIS data and services. The EOSDIS Earthdata Login also helps the EOSDIS program better understand the user demographics and access patterns in support of planning for new value-added features and customized services that can be directed to specific users or user groups resulting in better user experience. Earthdata Login provides user registration and authentication services and a common set of user information to all EOSDIS data centers in a manner that permits the data center to integrate their additional requirements with the Earthdata Login services. Below is a brief description of services provided by the Earthdata Login. To turn in: 1) Follow the steps in the NASA EOSDIS documentation to sign up for an Earth Data account. 2) Write an R script that stores your user and password called EARTHDATA_Token.R and submit the following line of code via .Rmd and PDF: source(&#39;./Tokens/EARTHDATA_Token.R&#39;) #path will change based on where you stored it exists(&#39;user&#39;) 7.4 Distributed Active Archive Centers 7.4.1 LP DAAC The Land Processes Distributed Active Archive Center (LP DAAC) is one of several discipline-specific data centers within the NASA EOS Data and Information System (EOSDIS). The LP DAAC operates as a partnership between the U.S. Geological Survey (USGS) and the National Aeronautics and Space Administration (NASA). Data specialists, system engineers, user service representatives, and science communicators work in collaboration to support LP DAAC activities. Watch this 4:02 minute video on LP DAAC’s 2019-2021 Prosectus 7.5 The LPDAAC Mission: Process, Archive, Distribute, Apply The LP DAAC processes, archives, and distributes land data products to hundreds of thousands of users in the earth science community. Land data products are made universally accessible and support the ongoing monitoring of Earth’s land dynamics and environmental systems to facilitate interdisciplinary research, education, and decision-making. Process: Raw data collected from specific satellite sensors, such as ASTER onboard NASA’s Terra satellite, are received and processed into a readable and interpretable format here at the LP DAAC, while other data undergo processing in other facilities around the country before arriving to the LP DAAC to be archived and distributed to the public. Archive: The LP DAAC continually archives a wide variety of land remote sensing data products collected by sensors onboard satellites, aircraft, and the International Space Station (ISS). The archive currently totals more than 3.5 petabytes of data, the equivalent of listening to 800 million songs, and distributes data to over 200,000 global users. Distribute: All data products in the archive are distributed free of charge through NASA Earthdata Search and USGS EarthExplorer search and download clients. The LP DAAC also supports tools and services, like the Application for Extracting and Exploring Analysis Ready Samples (AppEEARS), which allows users to transform and visualize data before download while offering enhanced subsetting and reprojecting capabilities. 7.5.1 How you can use LP DAAC’s data Watch this 2:13 minute long video on searching for data at the LP DAAC 7.6 AppEEARS The Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. The AppEEARS API offers users programmatic access to all features available in AppEEARS, with the exception of visualizations. The API features are demonstrated in this tutorial. 7.7 Hands on: Pulling AppEEARS Data via the API The following section was adapted from LPDAAC’s E-Learning Tutorials on the AppEEARS API and modified to request NEON areas. Contributing Authors: Material written by Mahsa Jami1 and Cole Krehbiel1 Contact: LPDAAC@usgs.gov Voice: +1-866-573-3222 Organization: Land Processes Distributed Active Archive Center (LP DAAC) Website: https://lpdaac.usgs.gov/ Date last modified: 06-12-2020 1 KBR, Inc., contractor to the U.S. Geological Survey, Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, USA. Work performed under USGS contract G15PD00467 for LP DAAC2. 2 LP DAAC Work performed under NASA contract NNG14HH33I. Run the following chunk to install any packages you will need for this section: # Packages you will need for AppEEARS API Tutorials packages = c(&#39;getPass&#39;,&#39;httr&#39;,&#39;jsonlite&#39;,&#39;ggplot2&#39;,&#39;dplyr&#39;,&#39;tidyr&#39;,&#39;readr&#39;,&#39;geojsonio&#39;,&#39;geojsonR&#39;,&#39;rgdal&#39;,&#39;sp&#39;, &#39;raster&#39;, &#39;rasterVis&#39;, &#39;RColorBrewer&#39;, &#39;jsonlite&#39;) # Identify missing packages new.packages = packages[!(packages %in% installed.packages()[,&quot;Package&quot;])] # Loop through and download the required packages if (length(new.packages)[1]==0){ message(&#39;All packages already installed&#39;) }else{ for (i in 1:length(new.packages)){ message(paste0(&#39;Installing: &#39;, new.packages)) install.packages(new.packages[i]) } } 7.8 Getting Started with the AppEEARS API (Point Request) This section demonstrates how to use R to connect to the AppEEARS API The Application for Extracting and Exploring Analysis Ready Samples (AppEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. The AppEEARS API offers users programmatic access to all features available in AppEEARS, with the exception of visualizations. The API features are demonstrated in this tutorial. 7.8.1 Example: Submit a point request with multiple points in U.S. National Parks for extracting vegetation and land surface temperature data In this tutorial, Connecting to the AppEEARS API, querying the list of available products, submitting a point sample request, downloading the request, working with the AppEEARS Quality API, and loading the results into R for visualization are covered. AppEEARS point requests allow users to subset their desired data using latitude/longitude geographic coordinate pairs (points) for a time period of interest, and for specific data layers within data products. AppEEARS returns the valid data from the parameters defined within the sample request. 7.8.1.1 Data Used in the Example: Data layers: Combined MODIS Leaf Area Index (LAI) MCD15A3H.006, 500m, 4 day: ‘Lai_500m’ Terra MODIS Land Surface Temperature MOD11A2.006, 1000m, 8 day: ‘LST_Day_1km’, ‘LST_Night_1km’ 7.8.2 Topics Covered in this tutorial: Getting Started 1a. Load Packages 1b. Set Up the Output Directory 1c. Login Query Available Products 2a. Search and Explore Available Products 2b. Search and Explore Available Layers Submit a Point Request 3a. Compile a JSON Object 3b. Submit a Task Request 3c. Retrieve Task Status Download a Request 4a. Explore Files in Request Output 4b. Download Files in a Request (Automation) Explore AppEEARS Quality API 5a. List Quality Layers 5b. Show Quality Values 5c. Decode Quality Values BONUS: Load Request Output and Visualize 6a. Load CSV 6b. Plot Results (Line/Scatter Plots) 7.8.3 Prerequisites: A NASA Earthdata Login account is required to login to the AppEEARS API and submit a request . You can create an account at the link provided. Install R and RStudio. These tutorials have been tested on Windows and MAc systems using R Version 4.0.0, RStudio version 1.1.463, and the specifications listed below. Required packages: getPass httr jsonlite warnings To read and visualize the tabular data: dplyr tidyr readr ggplot2 7.8.4 Procedures: 7.8.4.1 Getting Started: Clone/download AppEEARS API Getting Started in R Repository from the LP DAAC Data User Resources Repository or pull code from this textbook. Open the AppEEARS_API_R.Rproj file to directly open the project. Next, select the AppEEARS_API_Point_R.Rmd from the files list and open it. 7.8.4.2 Environment Setup: 7.8.4.3 1. Check the version of R using version and RStudio using RStudio.Version() and update them if needed. Windows Install and load installr: Install.packages(&quot;installr&quot;);library(installr) Copy/Update the existing packages to the new R installation: UpdateR() Open RStudio, go to Help &gt; Check for Updates to install newer version of RStudio (if available). Mac Go to https://cloud.r-project.org/bin/macosx/. Download the latest release (R-4.0.1.pkg) and finish the installation. Open RStudio, go to Help &gt; Check for Updates to install newer version of RStudio (if available). To update packages, go to Tools &gt; Check for Package Updates. If updates are available, select All, and click Install Updates. 7.8.5 AppEEARS Information: To access AppEEARS, visit: https://lpdaacsvc.cr.usgs.gov/appeears/. For comprehensive documentation of the full functionality of the AppEEARS API, please see the AppEEARS API Documentation. 7.9 1. Getting Started 7.10 1a. Load Packages First, load the R packages necessary to run the tutorial. # Load necessary packages into R library(getPass) # A micro-package for reading passwords library(httr) # To send a request to the server/receive a response from the server library(jsonlite) # Implements a bidirectional mapping between JSON data and the most important R data types library(ggplot2) # Functions for graphing and mapping library(tidyr) # Function for working with tabular data library(dplyr) # Function for working with tabular data library(readr) # Read rectangular data like CSV 7.11 1b. Set Up the Output Directory Set your input directory, and create an output directory for the results. outDir &lt;- file.path(&#39;./data/&#39;) # Create an output directory if it doesn&#39;t exist suppressWarnings(dir.create(outDir)) 7.12 1c. Login To submit a request, you must first login to the AppEEARS API. Use the getpass package to enter your NASA Earthdata login Username and Password. When prompted after executing the code block below, enter your username followed by your password. source(&#39;./Tokens/EARTHDATA_Token.R&#39;) #path will change based on where you stored it exists(&#39;user&#39;) ## [1] TRUE Decode the username and password to be used to post login request. secret &lt;- jsonlite::base64_enc(paste(user, password, sep = &quot;:&quot;)) # Encode the string of username and password Next, assign the AppEEARS API URL to a static variable. API_URL = &#39;https://lpdaacsvc.cr.usgs.gov/appeears/api/&#39; # Set the AppEEARS API to a variable Use the httr package to post your username and password. A successful login will provide you with a token to be used later in this tutorial to submit a request. For more information or if you are experiencing difficulties, please see the API Documentation. # Insert API URL, call login service, set the component of HTTP header, and post the request to the server response &lt;- httr::POST(paste0(API_URL,&quot;login&quot;), add_headers(&quot;Authorization&quot; = paste(&quot;Basic&quot;, gsub(&quot;\\n&quot;, &quot;&quot;, secret)), &quot;Content-Type&quot; =&quot;application/x-www-form-urlencoded;charset=UTF-8&quot;), body = &quot;grant_type=client_credentials&quot;) response_content &lt;- content(response) # Retrieve the content of the request token_response &lt;- toJSON(response_content, auto_unbox = TRUE) # Convert the response to the JSON object remove(user, password, secret, response) # Remove the variables that are not needed anymore prettify(token_response) # Print the prettified response ## { ## &quot;token_type&quot;: &quot;Bearer&quot;, ## &quot;token&quot;: &quot;dZBdb-Pn7AdlNi8uxzWpWbTAK_6M-H6PBKKmm5DOOvnb-GI-6TFM5c-e_48CkPJyM71fWQ3MbRb9Dx7hNwD8QA&quot;, ## &quot;expiration&quot;: &quot;2020-10-09T20:33:11Z&quot; ## } ## Above, you should see a Bearer token. Notice that this token will expire approximately 48 hours after being acquired. 7.13 2. Query Available Products The product API provides details about all of the products and layers available in AppEEARS. For more information, please see the API Documentation. Below, call the product API to list all of the products available in AppEEARS. prods_req &lt;- GET(paste0(API_URL, &quot;product&quot;)) # Request the info of all products from product service prods_content &lt;- content(prods_req) # Retrieve the content of request all_Prods &lt;- toJSON(prods_content, auto_unbox = TRUE) # Convert the info to JSON object remove(prods_req, prods_content) # Remove the variables that are not needed anymore # prettify(all_Prods) # Print the prettified product response 7.14 2a. Search and Explore Available Products Create a list indexed by product name to make it easier to query a specific product. # Divides information from each product. divided_products &lt;- split(fromJSON(all_Prods), seq(nrow(fromJSON(all_Prods)))) # Create a list indexed by the product name and version products &lt;- setNames(divided_products,fromJSON(all_Prods)$ProductAndVersion) # Print no. products available in AppEEARS sprintf(&quot;AppEEARS currently supports %i products.&quot; ,length(products)) ## [1] &quot;AppEEARS currently supports 120 products.&quot; Next, look at the product’s names and descriptions. Below, the ‘ProductAndVersion’ and ‘Description’ are printed for all products. # Loop through the products in the list and print the product name and description for (p in products){ print(paste0(p$ProductAndVersion,&quot; is &quot;,p$Description,&quot; from &quot;,p$Source)) } ## [1] &quot;GPW_DataQualityInd.004 is Quality of Input Data for Population Count and Density Grids from SEDAC&quot; ## [1] &quot;GPW_UN_Adj_PopCount.004 is UN-adjusted Population Count from SEDAC&quot; ## [1] &quot;GPW_UN_Adj_PopDensity.004 is UN-adjusted Population Density from SEDAC&quot; ## [1] &quot;MCD12Q1.006 is Land Cover Type from LP DAAC&quot; ## [1] &quot;MCD12Q2.006 is Land Cover Dynamics from LP DAAC&quot; ## [1] &quot;MCD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD15A3H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MCD43A1.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A3.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD43A4.006 is Bidirectional Reflectance Distribution Function (BRDF) and Albedo from LP DAAC&quot; ## [1] &quot;MCD64A1.006 is Burned Area (fire) from LP DAAC&quot; ## [1] &quot;MOD09A1.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09GA.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MOD09GQ.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD09Q1.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MOD10A1.005 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MOD10A1.006 is Snow Cover (NDSI) from NSIDC DAAC&quot; ## [1] &quot;MOD10A2.005 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MOD10A2.006 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MOD11A1.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD11A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MOD13A1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A2.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13A3.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD13Q1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MOD14A2.006 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MOD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MOD16A2.006 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A2GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD16A3GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MOD17A2H.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A2HGF.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MOD17A3HGF.006 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MOD44B.006 is Vegetation Continuous Fields (VCF) from LP DAAC&quot; ## [1] &quot;MOD44W.006 is Land/Water Mask from LP DAAC&quot; ## [1] &quot;MODOCGA.006 is Ocean Reflectance Bands 8-16 from LP DAAC&quot; ## [1] &quot;MODTBGA.006 is Thermal Bands and Albedo from LP DAAC&quot; ## [1] &quot;MYD09A1.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09GA.006 is Surface Reflectance Bands 1-7 from LP DAAC&quot; ## [1] &quot;MYD09GQ.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD09Q1.006 is Surface Reflectance Bands 1-2 from LP DAAC&quot; ## [1] &quot;MYD10A1.005 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MYD10A1.006 is Snow Cover (NDSI) from NSIDC DAAC&quot; ## [1] &quot;MYD10A2.005 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MYD10A2.006 is Snow Cover from NSIDC DAAC&quot; ## [1] &quot;MYD11A1.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD11A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD13A1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A2.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13A3.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD13Q1.006 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;MYD14A2.006 is Thermal Anomalies and Fire from LP DAAC&quot; ## [1] &quot;MYD15A2H.006 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;MYD16A2.006 is Evapotranspiration (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A2GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD16A3GF.006 is Net Evapotranspiration Gap-Filled (ET &amp; LE) from LP DAAC&quot; ## [1] &quot;MYD17A2H.006 is Gross Primary Productivity (GPP) from LP DAAC&quot; ## [1] &quot;MYD17A2HGF.006 is Gross Primary Productivity (GPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD17A3HGF.006 is Net Primary Production (NPP) Gap-Filled from LP DAAC&quot; ## [1] &quot;MYD21A1D.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A1N.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYD21A2.006 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;MYDOCGA.006 is Ocean Reflectance Bands 8-16 from LP DAAC&quot; ## [1] &quot;MYDTBGA.006 is Thermal Bands and Albedo from LP DAAC&quot; ## [1] &quot;NASADEM_NC.001 is Elevation from LP DAAC&quot; ## [1] &quot;NASADEM_NUMNC.001 is Source from LP DAAC&quot; ## [1] &quot;SPL3SMP_E.003 is Enhanced L3 Radiometer Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL3SMP.006 is Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL4CMDL.004 is Carbon Net Ecosystem Exchange from NSIDC DAAC&quot; ## [1] &quot;SPL4SMGP.004 is Surface and Root Zone Soil Moisture from NSIDC DAAC&quot; ## [1] &quot;SPL3FTP.002 is Freeze/Thaw State from NSIDC DAAC&quot; ## [1] &quot;SRTMGL1_NC.003 is Elevation (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL1_NUMNC.003 is Source (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL3_NC.003 is Elevation (DEM) from LP DAAC&quot; ## [1] &quot;SRTMGL3_NUMNC.003 is Source (DEM) from LP DAAC&quot; ## [1] &quot;ASTGTM_NC.003 is Elevation from LP DAAC&quot; ## [1] &quot;ASTGTM_NUMNC.003 is Source from LP DAAC&quot; ## [1] &quot;ASTWBD_ATTNC.001 is Water Bodies Database Attributes from LP DAAC&quot; ## [1] &quot;ASTWBD_NC.001 is Water Bodies Database Elevation from LP DAAC&quot; ## [1] &quot;VNP09H1.001 is Surface Reflectance from LP DAAC&quot; ## [1] &quot;VNP09A1.001 is Surface Reflectance from LP DAAC&quot; ## [1] &quot;VNP13A1.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP13A2.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP13A3.001 is Vegetation Indices (NDVI &amp; EVI) from LP DAAC&quot; ## [1] &quot;VNP14A1.001 is Thermal Anomalies/Fire from LP DAAC&quot; ## [1] &quot;VNP15A2H.001 is Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR) from LP DAAC&quot; ## [1] &quot;VNP21A1D.001 is Land Surface Temperature &amp; Emissivity Day (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP21A1N.001 is Land Surface Temperature &amp; Emissivity Night (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP21A2.001 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;VNP43IA1.001 is BRDF-Albedo Model Parameters from LP DAAC&quot; ## [1] &quot;VNP43IA2.001 is BRDF-Albedo Quality from LP DAAC&quot; ## [1] &quot;VNP43IA3.001 is Albedo (BRDF) from LP DAAC&quot; ## [1] &quot;VNP43IA4.001 is Nadir BRDF-Adjusted Reflectance from LP DAAC&quot; ## [1] &quot;VNP43MA1.001 is BRDF-Albedo Model Parameters from LP DAAC&quot; ## [1] &quot;VNP43MA2.001 is BRDF-Albedo Quality from LP DAAC&quot; ## [1] &quot;VNP43MA3.001 is Albedo (BRDF) from LP DAAC&quot; ## [1] &quot;VNP43MA4.001 is Nadir BRDF-Adjusted Reflectance from LP DAAC&quot; ## [1] &quot;CU_LC08.001 is CONUS Landsat 8 Surface Reflectance from USGS&quot; ## [1] &quot;CU_LE07.001 is CONUS Landsat 7 Surface Reflectance from USGS&quot; ## [1] &quot;CU_LT05.001 is CONUS Landsat 5 Surface Reflectance from USGS&quot; ## [1] &quot;CU_LT04.001 is CONUS Landsat 4 Surface Reflectance from USGS&quot; ## [1] &quot;AK_LC08.001 is Alaska Landsat 8 Surface Reflectance from USGS&quot; ## [1] &quot;AK_LE07.001 is Alaska Landsat 7 Surface Reflectance from USGS&quot; ## [1] &quot;AK_LT05.001 is Alaska Landsat 5 Surface Reflectance from USGS&quot; ## [1] &quot;AK_LT04.001 is Alaska Landsat 4 Surface Reflectance from USGS&quot; ## [1] &quot;HI_LC08.001 is Hawaii Landsat 8 Surface Reflectance from USGS&quot; ## [1] &quot;HI_LE07.001 is Hawaii Landsat 7 Surface Reflectance from USGS&quot; ## [1] &quot;HI_LT05.001 is Hawaii Landsat 5 Surface Reflectance from USGS&quot; ## [1] &quot;HI_LT04.001 is Hawaii Landsat 4 Surface Reflectance from USGS&quot; ## [1] &quot;DAYMET.003 is Daily Surface Weather Data for North America from ORNL&quot; ## [1] &quot;SSEBop_ET.004 is SSEBop Actual Evapotranspiration (ETa) from USGS&quot; ## [1] &quot;eMODIS_Smoothed_NDVI.001 is eMODIS Smoothed Normalized Difference Vegetation Index (NDVI) from USGS&quot; ## [1] &quot;ECO2LSTE.001 is Land Surface Temperature &amp; Emissivity (LST&amp;E) from LP DAAC&quot; ## [1] &quot;ECO2CLD.001 is Cloud Mask from LP DAAC&quot; ## [1] &quot;ECO3ETPTJPL.001 is Evapotranspiration PT-JPL from LP DAAC&quot; ## [1] &quot;ECO3ANCQA.001 is L3/L4 Ancillary Data Quality Assurance (QA) Flags from LP DAAC&quot; ## [1] &quot;ECO4ESIPTJPL.001 is Evaporative Stress Index PT-JPL from LP DAAC&quot; ## [1] &quot;ECO4WUE.001 is Water Use Efficiency from LP DAAC&quot; ## [1] &quot;ECO1BGEO.001 is Geolocation from LP DAAC&quot; ## [1] &quot;ECO1BMAPRAD.001 is Resampled Radiance from LP DAAC&quot; The product service provides many useful details, including if a product is currently available in AppEEARS, a description, and information on the spatial and temporal resolution. Below, the product details are retrieved using ‘ProductAndVersion’. # Convert the MCD15A3H.006 info to JSON object and print the prettified info prettify(toJSON(products$&quot;MCD15A3H.006&quot;)) ## [ ## { ## &quot;Product&quot;: &quot;MCD15A3H&quot;, ## &quot;Platform&quot;: &quot;Combined MODIS&quot;, ## &quot;Description&quot;: &quot;Leaf Area Index (LAI) and Fraction of Photosynthetically Active Radiation (FPAR)&quot;, ## &quot;RasterType&quot;: &quot;Tile&quot;, ## &quot;Resolution&quot;: &quot;500m&quot;, ## &quot;TemporalGranularity&quot;: &quot;4 day&quot;, ## &quot;Version&quot;: &quot;006&quot;, ## &quot;Available&quot;: true, ## &quot;DocLink&quot;: &quot;https://doi.org/10.5067/MODIS/MCD15A3H.006&quot;, ## &quot;Source&quot;: &quot;LP DAAC&quot;, ## &quot;TemporalExtentStart&quot;: &quot;2002-07-04&quot;, ## &quot;TemporalExtentEnd&quot;: &quot;Present&quot;, ## &quot;Deleted&quot;: false, ## &quot;DOI&quot;: &quot;10.5067/MODIS/MCD15A3H.006&quot;, ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## Also, the products can be searched using their description. Below, search for products containing Leaf Area Index in their description and make a list of their productAndVersion. LAI_Products &lt;- list() # Create an empty list for (p in products){ # Loop through the product list if (grepl(&#39;Leaf Area Index&#39;, p$Description )){ # Look through the product description for a keyword LAI_Products &lt;- append(LAI_Products, p$ProductAndVersion) # Append the LAI products to the list } } LAI_Products ## [[1]] ## [1] &quot;MCD15A2H.006&quot; ## ## [[2]] ## [1] &quot;MCD15A3H.006&quot; ## ## [[3]] ## [1] &quot;MOD15A2H.006&quot; ## ## [[4]] ## [1] &quot;MYD15A2H.006&quot; ## ## [[5]] ## [1] &quot;VNP15A2H.001&quot; Using the info above, Create a list of desired products. desired_products &lt;- c(&#39;MCD15A3H.006&#39;,&#39;MOD11A2.006&#39;) # Create a vector of desired products desired_products ## [1] &quot;MCD15A3H.006&quot; &quot;MOD11A2.006&quot; 7.15 2b. Search and Explore Available Layers This API call will list all of the layers available for a given product. Each product is referenced by its ProductAndVersion property which is also referred to as the product_id. First, request the layers for the MCD15A3H.006 product. # Request layers for the 1st product in the list: MCD15A3H.006 MCD15A3H_req &lt;- GET(paste0(API_URL,&quot;product/&quot;, desired_products[1])) # Request the info of a product from product URL MCD15A3H_content &lt;- content(MCD15A3H_req) # Retrieve content of the request MCD15A3H_response &lt;- toJSON(MCD15A3H_content, auto_unbox = TRUE) # Convert the content to JSON object remove(MCD15A3H_req, MCD15A3H_content) # Remove the variables that are not needed anymore #prettify(MCD15A3H_response) # Print the prettified response names(fromJSON(MCD15A3H_response)) # print the layer&#39;s names ## [1] &quot;FparExtra_QC&quot; &quot;FparLai_QC&quot; &quot;FparStdDev_500m&quot; &quot;Fpar_500m&quot; ## [5] &quot;LaiStdDev_500m&quot; &quot;Lai_500m&quot; Next, request the layers for the MOD11A2.006 product. MOD11_req &lt;- GET(paste0(API_URL,&quot;product/&quot;, desired_products[2])) # Request the info of a product from product URL MOD11_content &lt;- content(MOD11_req) # Retrieve content of the request MOD11_response &lt;- toJSON(MOD11_content, auto_unbox = TRUE) # Convert the content to JSON object remove(MOD11_req, MOD11_content) # Remove the variables that are not needed anymore names(fromJSON(MOD11_response)) # print the layer names ## [1] &quot;Clear_sky_days&quot; &quot;Clear_sky_nights&quot; &quot;Day_view_angl&quot; &quot;Day_view_time&quot; ## [5] &quot;Emis_31&quot; &quot;Emis_32&quot; &quot;LST_Day_1km&quot; &quot;LST_Night_1km&quot; ## [9] &quot;Night_view_angl&quot; &quot;Night_view_time&quot; &quot;QC_Day&quot; &quot;QC_Night&quot; Lastly, select the desired layers and pertinent products and make a data frame using this information. This data frame will be inserted into the nested data frame that will be used to create a JSON object to submit a request in Section 3. desired_layers &lt;- c(&quot;LST_Day_1km&quot;,&quot;LST_Night_1km&quot;,&quot;Lai_500m&quot;) # Create a vector of desired layers desired_prods &lt;- c(&quot;MOD11A2.006&quot;,&quot;MOD11A2.006&quot;,&quot;MCD15A3H.006&quot;) # Create a vector of products including the desired layers # Create a data frame including the desired data products and layers layers &lt;- data.frame(product = desired_prods, layer = desired_layers) 7.16 3. Submit a Point Request The Submit task API call provides a way to submit a new request to be processed. It can accept data via JSON or query string. In the example below, create a JSON object and submit a request. Tasks in AppEEARS correspond to each request associated with your user account. Therefore, each of the calls to this service requires an authentication token (see Section 1c.). 7.17 3a. Compile a JSON Object In this section, begin by setting up the information needed for a nested data frame that will be later converted to a JSON object for submitting an AppEEARS point request. For detailed information on required JSON parameters, see the API Documentation. For point requests, beside the date range and desired layers information, the coordinates property must also be inside the task object. Optionally, set id and category properties to further identify your selected coordinates. We’ll start by requesting point-based data for NEON.D17.SOAP and NEON.D17.SJER: startDate &lt;- &quot;01-01-2020&quot; # Start of the date range for which to extract data: MM-DD-YYYY endDate &lt;- &quot;10-01-2020&quot; # End of the date range for which to extract data: MM-DD-YYYY recurring &lt;- FALSE # Specify True for a recurring date range #yearRange &lt;- [2000,2016] # If recurring = True, set yearRange, change start/end date to MM-DD lat &lt;- c(37.0334, 37.1088) # Latitude of the point sites lon &lt;- c(-119.2622, -119.7323) # Longitude of the point sites id &lt;- c(&quot;0&quot;,&quot;1&quot;) # ID for the point sites category &lt;- c(&quot;SOAP&quot;, &quot;SJER&quot;) # Category for point sites taskName &lt;- &#39;NEON SOAP SJER Vegetation&#39; # Enter name of the task here taskType &lt;- &#39;point&#39; # Specify the task type, it can be either &quot;area&quot; or &quot;point&quot; To be able to successfully submit a task, the JSON object should be structured in a certain way. The code chunk below uses the information from the previous chunk to create a nested data frame. This nested data frame will be converted to JSON object that can be used to complete the request. # Create a data frame including the date range for the request date &lt;- data.frame(startDate = startDate, endDate = endDate) # Create a data frame including lat and long coordinates. ID and category name is optional. coordinates &lt;- data.frame(id = id, longitude = lon, latitude = lat, category = category) task_info &lt;- list(date,layers, coordinates) # Create a list of data frames names(task_info) &lt;- c(&quot;dates&quot;, &quot;layers&quot;, &quot;coordinates&quot;) # Assign names task &lt;- list(task_info, taskName, taskType) # Create a nested list names(task) &lt;- c(&quot;params&quot;, &quot;task_name&quot;, &quot;task_type&quot;) # Assign names remove(date, layers, coordinates, task_info) # Remove the variables that are not needed anymore toJSON function from jsonlite package converts the type of data frame to a string that can be recognized as a JSON object to be submitted as a point request. task_json &lt;- toJSON(task,auto_unbox = TRUE) # Convert to JSON object 7.18 3b. Submit a Task Request Token information is needed to submit a request. Below the login token is assigned to a variable. token &lt;- paste(&quot;Bearer&quot;, fromJSON(token_response)$token) # Save login token to a variable Below, post a call to the API task service, using the task_json created above. # Post the point request to the API task service response &lt;- POST(paste0(API_URL, &quot;task&quot;), body = task_json , encode = &quot;json&quot;, add_headers(Authorization = token, &quot;Content-Type&quot; = &quot;application/json&quot;)) task_content &lt;- content(response) # Retrieve content of the request task_response &lt;- prettify(toJSON(task_content, auto_unbox = TRUE))# Convert the content to JSON object remove(response, task_content) # Remove the variables that are not needed anymore task_response # Print the prettified task response ## { ## &quot;task_id&quot;: &quot;3a808dfa-ff0b-48f9-ac7b-9f598d73233b&quot;, ## &quot;status&quot;: &quot;pending&quot; ## } ## 7.19 3c. Retrieve Task Status This API call will list all of the requests associated with your user account, automatically sorted by date descending with the most recent requests listed first. The AppEEARS API contains some helpful formatting resources. Below, limit the API response to 2 entries for the last 2 requests and set pretty to True to format the response as an organized JSON object to make it easier to read. Additional information on AppEEARS API retrieve task, pagination, and formatting can be found in the API documentation. params &lt;- list(limit = 2, pretty = TRUE) # Set up query parameters # Request the task status of last 2 requests from task URL response_req &lt;- GET(paste0(API_URL,&quot;task&quot;), query = params, add_headers(Authorization = token)) response_content &lt;- content(response_req) # Retrieve content of the request status_response &lt;- toJSON(response_content, auto_unbox = TRUE) # Convert the content to JSON object remove(response_req, response_content) # Remove the variables that are not needed anymore prettify(status_response) # Print the prettified response ## [ ## { ## &quot;params&quot;: { ## &quot;dates&quot;: [ ## { ## &quot;endDate&quot;: &quot;10-01-2020&quot;, ## &quot;startDate&quot;: &quot;01-01-2020&quot; ## } ## ], ## &quot;layers&quot;: [ ## { ## &quot;layer&quot;: &quot;LST_Day_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;LST_Night_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;Lai_500m&quot;, ## &quot;product&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## }, ## &quot;status&quot;: &quot;pending&quot;, ## &quot;created&quot;: &quot;2020-10-07T21:37:14.871259&quot;, ## &quot;task_id&quot;: &quot;3a808dfa-ff0b-48f9-ac7b-9f598d73233b&quot;, ## &quot;updated&quot;: &quot;2020-10-07T21:37:14.883937&quot;, ## &quot;user_id&quot;: &quot;kdw223@nau.edu&quot;, ## &quot;estimate&quot;: { ## &quot;request_size&quot;: 204 ## }, ## &quot;task_name&quot;: &quot;NEON SOAP SJER Vegetation&quot;, ## &quot;task_type&quot;: &quot;point&quot;, ## &quot;api_version&quot;: &quot;v1&quot;, ## &quot;svc_version&quot;: &quot;2.47&quot;, ## &quot;web_version&quot;: { ## ## }, ## &quot;expires_on&quot;: &quot;2020-11-06T21:37:14.883937&quot; ## }, ## { ## &quot;error&quot;: { ## ## }, ## &quot;params&quot;: { ## &quot;dates&quot;: [ ## { ## &quot;endDate&quot;: &quot;10-01-2020&quot;, ## &quot;startDate&quot;: &quot;01-01-2020&quot; ## } ## ], ## &quot;layers&quot;: [ ## { ## &quot;layer&quot;: &quot;LST_Day_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;LST_Night_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;Lai_500m&quot;, ## &quot;product&quot;: &quot;MCD15A3H.006&quot; ## } ## ] ## }, ## &quot;status&quot;: &quot;done&quot;, ## &quot;created&quot;: &quot;2020-10-07T21:00:20.497312&quot;, ## &quot;task_id&quot;: &quot;de45360e-d6e4-4577-97a1-760b9893c002&quot;, ## &quot;updated&quot;: &quot;2020-10-07T21:02:10.990738&quot;, ## &quot;user_id&quot;: &quot;kdw223@nau.edu&quot;, ## &quot;attempts&quot;: 1, ## &quot;estimate&quot;: { ## &quot;request_size&quot;: 204 ## }, ## &quot;retry_at&quot;: { ## ## }, ## &quot;completed&quot;: &quot;2020-10-07T21:02:10.987115&quot;, ## &quot;task_name&quot;: &quot;NEON SOAP SJER Vegetation&quot;, ## &quot;task_type&quot;: &quot;point&quot;, ## &quot;api_version&quot;: &quot;v1&quot;, ## &quot;svc_version&quot;: &quot;2.47&quot;, ## &quot;web_version&quot;: { ## ## }, ## &quot;size_category&quot;: &quot;0&quot;, ## &quot;expires_on&quot;: &quot;2020-11-06T21:02:10.990738&quot; ## } ## ] ## The task_id that was generated when submitting your request can also be used to retrieve a task status. task_id &lt;- fromJSON(task_response)$task_id # Extract the task_id of submitted point request # Request the task status of a task with the provided task_id from task URL status_req &lt;- GET(paste0(API_URL,&quot;task/&quot;, task_id), add_headers(Authorization = token)) status_content &lt;- content(status_req) # Retrieve content of the request statusResponse &lt;-toJSON(status_content, auto_unbox = TRUE) # Convert the content to JSON object stat &lt;- fromJSON(statusResponse)$status # Assign the task status to a variable remove(status_req, status_content) # Remove the variables that are not needed anymore prettify(statusResponse) # Print the prettified response ## { ## &quot;params&quot;: { ## &quot;dates&quot;: [ ## { ## &quot;endDate&quot;: &quot;10-01-2020&quot;, ## &quot;startDate&quot;: &quot;01-01-2020&quot; ## } ## ], ## &quot;layers&quot;: [ ## { ## &quot;layer&quot;: &quot;LST_Day_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;LST_Night_1km&quot;, ## &quot;product&quot;: &quot;MOD11A2.006&quot; ## }, ## { ## &quot;layer&quot;: &quot;Lai_500m&quot;, ## &quot;product&quot;: &quot;MCD15A3H.006&quot; ## } ## ], ## &quot;coordinates&quot;: [ ## { ## &quot;id&quot;: &quot;0&quot;, ## &quot;category&quot;: &quot;SOAP&quot;, ## &quot;latitude&quot;: 37.0334, ## &quot;longitude&quot;: -119.2622 ## }, ## { ## &quot;id&quot;: &quot;1&quot;, ## &quot;category&quot;: &quot;SJER&quot;, ## &quot;latitude&quot;: 37.1088, ## &quot;longitude&quot;: -119.7323 ## } ## ] ## }, ## &quot;status&quot;: &quot;pending&quot;, ## &quot;created&quot;: &quot;2020-10-07T21:37:14.871259&quot;, ## &quot;task_id&quot;: &quot;3a808dfa-ff0b-48f9-ac7b-9f598d73233b&quot;, ## &quot;updated&quot;: &quot;2020-10-07T21:37:14.883937&quot;, ## &quot;user_id&quot;: &quot;kdw223@nau.edu&quot;, ## &quot;estimate&quot;: { ## &quot;request_size&quot;: 204 ## }, ## &quot;task_name&quot;: &quot;NEON SOAP SJER Vegetation&quot;, ## &quot;task_type&quot;: &quot;point&quot;, ## &quot;api_version&quot;: &quot;v1&quot;, ## &quot;svc_version&quot;: &quot;2.47&quot;, ## &quot;web_version&quot;: { ## ## }, ## &quot;expires_on&quot;: &quot;2020-11-06T21:37:14.883937&quot; ## } ## Retrieve the task status every 5 seconds. The task status should be done to be able to download the output. while (stat != &#39;done&#39;) { Sys.sleep(5) # Request the task status and retrieve content of request from task URL stat_content &lt;- content(GET(paste0(API_URL,&quot;task/&quot;, task_id), add_headers(Authorization = token))) stat &lt;-fromJSON(toJSON(stat_content, auto_unbox = TRUE))$status # Get the status remove(stat_content) print(stat) } ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;pending&quot; ## [1] &quot;processing&quot; ## [1] &quot;processing&quot; ## [1] &quot;processing&quot; ## [1] &quot;processing&quot; ## [1] &quot;processing&quot; ## [1] &quot;processing&quot; ## [1] &quot;done&quot; 7.20 4. Download a Request 7.21 4a. Explore Files in Request Output Before downloading the request output, examine the files contained in the request output. # Request the task bundle info from API bundle URL response &lt;- GET(paste0(API_URL, &quot;bundle/&quot;, task_id), add_headers(Authorization = token)) response_content &lt;- content(response) # Retrieve content of the request bundle_response &lt;- toJSON(response_content, auto_unbox = TRUE) # Convert the content to JSON object prettify(bundle_response) # Print the prettified response ## { ## &quot;files&quot;: [ ## { ## &quot;sha256&quot;: &quot;e12e3634d11491fd77aa4c8f347b94304885746713852d21a08c635b2a9e4852&quot;, ## &quot;file_id&quot;: &quot;82ab979e-0dbe-4869-b676-88193c79727f&quot;, ## &quot;file_name&quot;: &quot;NEON-SOAP-SJER-Vegetation-MCD15A3H-006-results.csv&quot;, ## &quot;file_size&quot;: 68260, ## &quot;file_type&quot;: &quot;csv&quot; ## }, ## { ## &quot;sha256&quot;: &quot;e465df89151b28d63a1dda2420cedec7f24b383d03e0482083839565a27f8d5d&quot;, ## &quot;file_id&quot;: &quot;ab2e3574-3c50-44dd-bafd-c30b1fb4d99d&quot;, ## &quot;file_name&quot;: &quot;NEON-SOAP-SJER-Vegetation-MOD11A2-006-results.csv&quot;, ## &quot;file_size&quot;: 28570, ## &quot;file_type&quot;: &quot;csv&quot; ## }, ## { ## &quot;sha256&quot;: &quot;100dd40b43760430c900eea1d39b78a73bf880ae397020ad29bcf1c58947e8b5&quot;, ## &quot;file_id&quot;: &quot;765639a0-7278-497d-aca6-433ebf41d021&quot;, ## &quot;file_name&quot;: &quot;NEON-SOAP-SJER-Vegetation-granule-list.txt&quot;, ## &quot;file_size&quot;: 11402, ## &quot;file_type&quot;: &quot;txt&quot; ## }, ## { ## &quot;sha256&quot;: &quot;6b824abc02ce3e49409e332e5b941533a66b2ff4c2a362b4d3b5b31b4266ecd5&quot;, ## &quot;file_id&quot;: &quot;16eae3d4-8876-403e-b673-2e29f2d8324a&quot;, ## &quot;file_name&quot;: &quot;NEON-SOAP-SJER-Vegetation-request.json&quot;, ## &quot;file_size&quot;: 883, ## &quot;file_type&quot;: &quot;json&quot; ## }, ## { ## &quot;sha256&quot;: &quot;3e025646ef6eadb42eb4d9f172db96ed824bbc72333519f0c8f44c3449fe4d7d&quot;, ## &quot;file_id&quot;: &quot;ff3f16d7-75c2-4185-8ce6-f6a3cc4de17c&quot;, ## &quot;file_name&quot;: &quot;NEON-SOAP-SJER-Vegetation-MCD15A3H-006-metadata.xml&quot;, ## &quot;file_size&quot;: 17255, ## &quot;file_type&quot;: &quot;xml&quot; ## }, ## { ## &quot;sha256&quot;: &quot;650cd511fcfc442fe675e35d14a244247c565085d5dcbb215782354d7ceda774&quot;, ## &quot;file_id&quot;: &quot;ffa0432e-cc7a-4cde-8264-6ad97dbed7c3&quot;, ## &quot;file_name&quot;: &quot;NEON-SOAP-SJER-Vegetation-MOD11A2-006-metadata.xml&quot;, ## &quot;file_size&quot;: 17217, ## &quot;file_type&quot;: &quot;xml&quot; ## }, ## { ## &quot;sha256&quot;: &quot;337cd424036258f85f9d216debfe1f9b0649bf2971ea1ce356a44b9c9ca51078&quot;, ## &quot;file_id&quot;: &quot;aa98b009-c955-468d-bd03-9ca7068969b9&quot;, ## &quot;file_name&quot;: &quot;README.md&quot;, ## &quot;file_size&quot;: 17894, ## &quot;file_type&quot;: &quot;txt&quot; ## } ## ], ## &quot;created&quot;: &quot;2020-10-07T21:38:39.407692&quot;, ## &quot;task_id&quot;: &quot;3a808dfa-ff0b-48f9-ac7b-9f598d73233b&quot;, ## &quot;updated&quot;: &quot;2020-10-07T21:39:07.732995&quot;, ## &quot;bundle_type&quot;: &quot;point&quot; ## } ## 7.22 4b. Download Files in a Request (Automation) The bundle API provides information about completed tasks. For any completed task, a bundle can be queried to return the files contained as a part of the task request. Below, call the bundle API and return all of the output files. Next, read the contents of the bundle in JSON format and loop through file_id to automate downloading all of the output files into the output directory. For more information, please see AppEEARS API Documentation. bundle &lt;- fromJSON(bundle_response)$files for (id in bundle$file_id){ # retrieve the filename from the file_id filename &lt;- bundle[bundle$file_id == id,]$file_name # create a destination directory to store the file in filepath &lt;- paste(outDir,filename, sep = &quot;/&quot;) suppressWarnings(dir.create(dirname(filepath))) # write the file to disk using the destination directory and file name response &lt;- GET(paste0(API_URL, &quot;bundle/&quot;, task_id, &quot;/&quot;, id), write_disk(filepath, overwrite = TRUE), progress(), add_headers(Authorization = token)) } 7.23 5. Explore AppEEARS Quality Service The quality API provides quality details about all of the data products available in AppEEARS. Below are examples of how to query the quality API for listing quality products, layers, and values. The final example (Section 5c.) demonstrates how AppEEARS quality services can be leveraged to decode pertinent quality values for your data. For more information visit AppEEARS API documentation. First, reset pagination to include offset which allows you to set the number of results to skip before starting to return entries. Next, make a call to list all of the data product layers and the associated quality product and layer information. params &lt;- list(limit = 6, offset = 20, pretty = TRUE) # Set up the query parameters q_req &lt;- GET(paste0(API_URL, &quot;quality&quot;), query = params) # Request the quality info from quality API_URL q_content &lt;- content(q_req) # Retrieve the content of request q_response &lt;- toJSON(q_content, auto_unbox = TRUE) # Convert the info to JSON object remove(params, q_req, q_content) # Remove the variables that are not needed prettify(q_response) # Print the prettified quality information ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;Layer&quot;: &quot;SRB1&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ] ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;Layer&quot;: &quot;SRB2&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ] ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;Layer&quot;: &quot;SRB3&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ] ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;Layer&quot;: &quot;SRB4&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ] ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;Layer&quot;: &quot;SRB5&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ] ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;Layer&quot;: &quot;SRB7&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;CU_LT05.001&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;PIXELQA&quot; ## ] ## } ## ] ## 7.24 5a. List Quality Layers This API call will list all of the quality layer information for a product. For more information visit AppEEARS API documentation productAndVersion &lt;- &#39;MCD15A3H.006&#39; # Assign ProductAndVersion to a variable # Request the quality info from quality API for a specific product MCD15A3H_req &lt;- GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion)) MCD15A3H_content &lt;- content(MCD15A3H_req) # Retrieve the content of request MCD15A3H_quality &lt;- toJSON(MCD15A3H_content, auto_unbox = TRUE)# Convert the info to JSON object remove(MCD15A3H_req, MCD15A3H_content) # Remove the variables that are not needed anymore prettify(MCD15A3H_quality) # Print the prettified quality information ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;Fpar_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ] ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;FparStdDev_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ] ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;Lai_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ] ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;Layer&quot;: &quot;LaiStdDev_500m&quot;, ## &quot;QualityProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayers&quot;: [ ## &quot;FparLai_QC&quot;, ## &quot;FparExtra_QC&quot; ## ] ## } ## ] ## 7.25 5b. Show Quality Values This API call will list all of the values for a given quality layer. quality_layer &lt;- &#39;FparLai_QC&#39; # assign a quality layer to a variable # Request the specified quality layer info from quality API quality_req &lt;- GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion, &quot;/&quot;, quality_layer, sep = &quot;&quot;)) quality_content &lt;- content(quality_req) # Retrieve the content of request quality_response &lt;- toJSON(quality_content, auto_unbox = TRUE) # Convert the info to JSON object remove(quality_req, quality_content) # Remove the variables that are not needed prettify(quality_response) # Print the quality response as a data frame ## [ ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;MODLAND&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Good quality (main algorithm with or without saturation)&quot;, ## &quot;Acceptable&quot;: true ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;MODLAND&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Other Quality (back-up algorithm or fill values)&quot;, ## &quot;Acceptable&quot;: false ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;Sensor&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Terra&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;Sensor&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Aqua&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;DeadDetector&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Detectors apparently fine for up to 50% of channels 1, 2&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;DeadDetector&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Dead detectors caused &gt;50% adjacent detector retrieval&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Significant clouds NOT present (clear)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Significant clouds WERE present&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 2, ## &quot;Description&quot;: &quot;Mixed cloud present in pixel&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;CloudState&quot;, ## &quot;Value&quot;: 3, ## &quot;Description&quot;: &quot;Cloud state not defined, assumed clear&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 0, ## &quot;Description&quot;: &quot;Main (RT) method used, best result possible (no saturation)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 1, ## &quot;Description&quot;: &quot;Main (RT) method used with saturation. Good, very usable&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 2, ## &quot;Description&quot;: &quot;Main (RT) method failed due to bad geometry, empirical algorithm used&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 3, ## &quot;Description&quot;: &quot;Main (RT) method failed due to problems other than geometry, empirical algorithm used&quot;, ## &quot;Acceptable&quot;: { ## ## } ## }, ## { ## &quot;ProductAndVersion&quot;: &quot;MCD15A3H.006&quot;, ## &quot;QualityLayer&quot;: &quot;FparLai_QC&quot;, ## &quot;Name&quot;: &quot;SCF_QC&quot;, ## &quot;Value&quot;: 4, ## &quot;Description&quot;: &quot;Pixel not produced at all, value couldn&#39;t be retrieved (possible reasons: bad L1B data, unusable MOD09GA data)&quot;, ## &quot;Acceptable&quot;: { ## ## } ## } ## ] ## 7.26 5c. Decode Quality Values This API call will decode the bits for a given quality value. quality_value &lt;- 1 # Assign a quality value to a variable # Request and retrieve information for provided quality value from quality API URL response &lt;- content(GET(paste0(API_URL, &quot;quality/&quot;, productAndVersion, &quot;/&quot;, quality_layer, &quot;/&quot;, quality_value))) q_response &lt;- toJSON(response, auto_unbox = TRUE) # Convert the info to JSON object remove(response) # Remove the variables that are not needed anymore prettify(q_response) # Print the prettified response ## { ## &quot;Binary Representation&quot;: &quot;0b00000001&quot;, ## &quot;MODLAND&quot;: { ## &quot;bits&quot;: &quot;0b1&quot;, ## &quot;description&quot;: &quot;Other Quality (back-up algorithm or fill values)&quot; ## }, ## &quot;Sensor&quot;: { ## &quot;bits&quot;: &quot;0b0&quot;, ## &quot;description&quot;: &quot;Terra&quot; ## }, ## &quot;DeadDetector&quot;: { ## &quot;bits&quot;: &quot;0b0&quot;, ## &quot;description&quot;: &quot;Detectors apparently fine for up to 50% of channels 1, 2&quot; ## }, ## &quot;CloudState&quot;: { ## &quot;bits&quot;: &quot;0b00&quot;, ## &quot;description&quot;: &quot;Significant clouds NOT present (clear)&quot; ## }, ## &quot;SCF_QC&quot;: { ## &quot;bits&quot;: &quot;0b000&quot;, ## &quot;description&quot;: &quot;Main (RT) method used, best result possible (no saturation)&quot; ## } ## } ## 7.27 6. BONUS: Load Request Output and Visualize Here, load the CSV file containing the results from your request using readr package, and create some basic visualizations using the ggplot2 package. 7.28 6a. Load a CSV Use the readr package to load the CSV file containing the results from the AppEEARS request. # Make a list of csv files in the output directory files &lt;- list.files(outDir, pattern = &quot;\\\\MOD11A2-006-results.csv$&quot;) # Read the MOD11A2 results df &lt;- read_csv(paste0(outDir,&quot;/&quot;, files)) Select the MOD11A2.006 LST Day column for the data from Grand Canyon National Park using dplyr package. lst_GC &lt;- df %&gt;% # Filter df for the point from GC filter(Category == &quot;SOAP&quot;) %&gt;% # Select desired columns select(Latitude, Longitude, Date ,MOD11A2_006_LST_Day_1km, MOD11A2_006_LST_Night_1km) Extract information for LST_DAY_1KM from MOD11_response of product service call from earlier in the tutorial. #fromJSON(MOD11_response)$LST_Day_1km # Extract all the info for LST_Day_1km layer fillValue &lt;- fromJSON(MOD11_response)$LST_Day_1km$FillValue # Assign fill value to a variable unit &lt;- fromJSON(MOD11_response)$LST_Day_1km$Units # Assign unit to a variable sprintf(&quot;Fill value for LST_DAY_1KM is: %i&quot; ,fillValue) # Print LST_DAY_1KM fill value ## [1] &quot;Fill value for LST_DAY_1KM is: 0&quot; sprintf(&quot;Unit for LST_DAY_1KM is: %s&quot; ,unit) # Print LST_DAY_1KM unit ## [1] &quot;Unit for LST_DAY_1KM is: Kelvin&quot; 7.29 6b. Plot Results (Line/Scatter Plots) Next, plot a time series of daytime LST for the selected point in Grand Canyon National Park for 2018. Below, filter the LST data to exclude fill values. lst_GC &lt;- lst_GC %&gt;% # exclude NoData filter(MOD11A2_006_LST_Day_1km != fillValue)%&gt;% filter(MOD11A2_006_LST_Night_1km != fillValue) Next, plot LST Day as a time series with some additional formatting using ggplot2. ggplot(lst_GC)+ geom_line(aes(x= Date, y = MOD11A2_006_LST_Day_1km), size=1, color=&quot;blue&quot;)+ geom_point(aes(x= Date, y = MOD11A2_006_LST_Day_1km), shape=18 , size = 3, color=&quot;blue&quot;)+ labs(title = &quot;Time Series&quot;, x = &quot;Date&quot;, y = sprintf( &quot;LST_Day_1km (%s)&quot;, unit))+ scale_x_date(date_breaks = &quot;16 day&quot;)+ scale_y_continuous(limits = c(250, 325), breaks = seq(250, 325, 10))+ theme(plot.title = element_text(face = &quot;bold&quot;,size = rel(2.5),hjust = 0.5), axis.title = element_text(face = &quot;bold&quot;,size = rel(1)), panel.background = element_rect(fill = &quot;lightgray&quot;, colour = &quot;black&quot;), axis.text.x = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, angle= 315 , size = 10), axis.text.y = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, angle= 0, size = 10) ) Using the tidyr package, the LST Day and Night values for Grand Canyon NP are being gathered in a single column to be used to make a plot including both LST_Day_1km and LST_Night_1km. lst_GC_DN &lt;- tidyr::gather(lst_GC, key = Tstat , value = LST, MOD11A2_006_LST_Day_1km, MOD11A2_006_LST_Night_1km) lst_GC_DN[1:5,] # print the five first observations ## # A tibble: 5 x 5 ## Latitude Longitude Date Tstat LST ## &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 37.0 -119. 2019-12-27 MOD11A2_006_LST_Day_1km 278. ## 2 37.0 -119. 2020-01-01 MOD11A2_006_LST_Day_1km 283. ## 3 37.0 -119. 2020-01-09 MOD11A2_006_LST_Day_1km 280. ## 4 37.0 -119. 2020-01-17 MOD11A2_006_LST_Day_1km 283. ## 5 37.0 -119. 2020-01-25 MOD11A2_006_LST_Day_1km 285. Next, plot LST Day and Night as a time series with some additional formatting. ggplot(lst_GC_DN)+ geom_line(aes(x= Date, y = LST, color = Tstat), size=1)+ geom_point(aes(x= Date, y = LST, color = Tstat), shape=18 , size = 3)+ scale_fill_manual(values=c(&quot;red&quot;, &quot;blue&quot;))+ scale_color_manual(values=c(&#39;red&#39;,&#39;blue&#39;))+ labs(title = &quot;Time Series&quot;, x = &quot;Date&quot;, y = sprintf( &quot;LST_Day_1km (%s)&quot;,unit))+ scale_x_date(date_breaks = &quot;16 day&quot;)+ scale_y_continuous(limits = c(250, 325), breaks = seq(250, 325, 10))+ theme(plot.title = element_text(face = &quot;bold&quot;,size = rel(2.5), hjust = 0.5), axis.title = element_text(face = &quot;bold&quot;,size = rel(1)), panel.background = element_rect(fill = &quot;lightgray&quot;, colour = &quot;black&quot;), axis.text.x = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, angle= 315 , size = 10), axis.text.y = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, angle= 0, size = 10), legend.position = &quot;bottom&quot;, legend.title = element_blank() ) Finally, bring in the daytime LST data from SJER , and compare with daytime LST at SOAP , shown below in a scatterplot using ggplot2 package. Here, the dplyr is used to extract the LST_DAY_1km for Zion National Park. lst_Z &lt;- df %&gt;% filter(MOD11A2_006_LST_Day_1km != fillValue) %&gt;% # Filter fill value filter(Category == &quot;SJER&quot;)%&gt;% # Filter Zion national park select(Date, MOD11A2_006_LST_Day_1km) # Select desired columns Make a scatterplot. ggplot()+ geom_point(aes(x=lst_Z$MOD11A2_006_LST_Day_1km, y=lst_GC$MOD11A2_006_LST_Day_1km), shape=18 , size = 3, color=&quot;blue&quot;)+ labs(title = &quot;MODIS LST: SOAP vs. SJER, 2020&quot;, x = sprintf(&quot;SOAP: LST_Day_1km (%s)&quot;,unit), y = sprintf( &quot;SJER: LST_Day_1km (%s)&quot;,unit))+ theme(plot.title = element_text(face = &quot;bold&quot;,size = rel(1.5), hjust = 0.5), axis.title = element_text(face = &quot;bold&quot;,size = rel(1)), panel.background = element_rect(fill = &quot;lightgray&quot;, colour = &quot;black&quot;), axis.text.x = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, size = 10), axis.text.y = element_text(face =&quot;bold&quot;,color=&quot;black&quot;, size = 10) ) This example can provide a template to use for your own research workflows. Leveraging the AppEEARS API for searching, extracting, and formatting analysis ready data, and loading it directly into R means that you can keep your entire research workflow in a single software program, from start to finish. 7.29.1 Submit an Area Request "],
["frequently-asked-questions.html", "Frequently Asked Questions: 7.30 Where can I find due dates for assignments? 7.31 How do I submit assignments? 7.32 Do I still have to submit written exercises as .Rmd and .pdf? 7.33 What’s better for code, conciseness or readability? 7.34 How find I find resources to navigate the NEON Data Portal? 7.35 How can I best prepare for class and succeed?", " Frequently Asked Questions: 7.30 Where can I find due dates for assignments? All assignment deadlines can be found in BBLearn. Within this textbook we have suggestions for the timing of all writen questions, coding labs and final culmination writes ups. All material by infrastructure is due before we begin the next infrastructure. 7.31 How do I submit assignments? All assignments (except the very first git assignment) are submitted via BBLearn as both .Rmds and .pdfs 7.32 Do I still have to submit written exercises as .Rmd and .pdf? Yes. 7.33 What’s better for code, conciseness or readability? Readability &gt; Conciseness I personally almost always use dplyr, my thoughts, pulled largely from Dr. Derek Sonderegger’s Statistical Computing Course are below: The pipe command in dplyr %&gt;% allows for very readable code. The idea is that the %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). This operator works on any function and was introduced in the magrittr package. The beauty of this comes when you have a suite of functions that take input arguments of the same type as their output. They are human readable! For example if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h(). Dr. Hadley Wickham (aka R genius) gave the following example of readability: bopping( scooping_up( hopping_through(foo_foo), field_mice), head) is more readably written: foo_foo %&gt;% hopping_through(forest) %&gt;% scooping_up( field_mice) %&gt;% bopping( head ) In dplyr, all the functions take a data set as its first argument and outputs an appropriately modified data set. This allows me to chain together commands in a readable fashion. Then in 3 months I don’t have to wonder what on earth I was doing last time I opened this project, if I filtered the data, etc etc. Your future self will sincerely thank your past self. 7.34 How find I find resources to navigate the NEON Data Portal? A fantastic powerpoint giving you step-by-step directions can be found here. 7.35 How can I best prepare for class and succeed? Read the textbook, click on linked resources including videos, review materials as we go or read ahead. Complete assignments as we go or ahead of time. Do not wait until the last minute. Pick a ‘coding buddy’ and help each other tackle errors that arise. Reach out to your instructors if you need clarification on assignments. "]
]
