
# NASA's Earth Observing System (EOS)

> Estimated Time: 3 hours

## Learning Objectives

  * Describe the mission of NASA EOS
  * Describe the roles of DAACs in distributing data
  * Interact with the AppEEARS API to query the list of available products
  * Submit and download submitting a point and area sample requests, download the request 
  * Filter data based on quality

```{r, echo=FALSE}
knitr::include_graphics('./images/CURRENT-Earth-Missions10_2019.png')
```

## NASA EOS Project Mission & Design

NASA’s Earth Observing System (EOS) is a coordinated series of polar-orbiting and low inclination satellites for long-term global observations of the land surface, biosphere, solid Earth, atmosphere, and oceans. As a major component of the Earth Science Division of NASA’s Science Mission Directorate, EOS enables an improved understanding of the Earth as an integrated system. 

[**Review NASA EOS's Mission Profile**](https://eospso.nasa.gov/files/mission_profile.pdf)

* Completeed Missions

* Current Missions

* Future Missions

<iframe width="560" height="315" src="https://www.youtube.com/embed/pDvRE0iF9ME" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

>Credit: NASA's Goddard Space Flight Center

## NASA EOS Earth Data Account:

The EOSDIS Earthdata Login provides a centralized and simplified mechanism for user registration and profile management for all EOSDIS system components. End users may register and edit their profile information in one location allowing them access to the wide array of EOSDIS data and services. The EOSDIS Earthdata Login also helps the EOSDIS program better understand the user demographics and access patterns in support of planning for new value-added features and customized services that can be directed to specific users or user groups resulting in better user experience.

Earthdata Login provides user registration and authentication services and a common set of user information to all EOSDIS data centers in a manner that permits the data center to integrate their additional requirements with the Earthdata Login services. Below is a brief description of services provided by the Earthdata Login.

## **NASA EOS Coding Assignment 1**

*Suggested completion: following lecture 1 on NASA EOS*

**To submit via BBLearn:**
1) Follow the [steps in the NASA EOSDIS documentation](https://urs.earthdata.nasa.gov/documentation) to sign up for an Earth Data account.

2) Write an R script that stores your `user` and `password` called `EARTHDATA_Token.R` and submit the following line of code via .Rmd and PDF:

```
source('./Tokens/EARTHDATA_Token.R') #path will change based on where you stored it
exists('user')
```

## Distributed Active Archive Centers

### LP DAAC

The [Land Processes Distributed Active Archive Center (LP DAAC)](https://lpdaac.usgs.gov/about/) is one of several discipline-specific data centers within the NASA EOS Data and Information System [(EOSDIS)](https://earthdata.nasa.gov/about-eosdis?_ga=2.174168546.1999623198.1596733217-204014717.1592177507).  The LP DAAC operates as a partnership between the U.S. Geological Survey (USGS) and the National Aeronautics and Space Administration (NASA). Data specialists, system engineers, user service representatives, and science communicators work in collaboration to support LP DAAC activities.

**Watch this 4:02 minute video on LP DAAC's 2019-2021 Prosectus**

<iframe width="560" height="315" src="https://www.youtube.com/embed/pCuE007qEqI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## The LPDAAC Mission: Process, Archive, Distribute, Apply

The LP DAAC processes, archives, and distributes land data products to hundreds of thousands of users in the earth science community. Land data products are made universally accessible and support the ongoing monitoring of Earth’s land dynamics and environmental systems to facilitate interdisciplinary research, education, and decision-making.

**Process:** Raw data collected from specific satellite sensors, such as ASTER onboard NASA’s Terra satellite, are received and processed into a readable and interpretable format here at the LP DAAC, while other data undergo processing in other facilities around the country before arriving to the LP DAAC to be archived and distributed to the public.

**Archive:** The LP DAAC continually archives a wide variety of land remote sensing data products collected by sensors onboard satellites, aircraft, and the International Space Station (ISS). The archive currently totals more than 3.5 petabytes of data, the equivalent of listening to 800 million songs, and distributes data to over 200,000 global users.

**Distribute:** All data products in the archive are distributed free of charge through NASA [Earthdata Search](https://search.earthdata.nasa.gov/search) and [USGS EarthExplorer](https://earthexplorer.usgs.gov/) search and download clients. The LP DAAC also supports tools and services, like the [Application for Extracting and Exploring Analysis Ready Samples (AppEEARS)](https://lpdaacsvc.cr.usgs.gov/appeears/), which allows users to transform and visualize data before download while offering enhanced subsetting and reprojecting capabilities.

### How you can use LP DAAC's data

**Watch this 2:13 minute long video on searching for data at the LP DAAC**

<iframe width="560" height="315" src="https://www.youtube.com/embed/zX-cuTWeh8Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## AppEEARS

The Application for Extracting and Exploring Analysis Ready Samples ([AppEEARS](https://lpdaacsvc.cr.usgs.gov/appeears/)) offers a simple and efficient way to access and transform [geospatial data](https://lpdaacsvc.cr.usgs.gov/appeears/products) from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. The [AppEEARS API](https://lpdaacsvc.cr.usgs.gov/appeears/api/) offers users **programmatic access** to all features available in AppEEARS, with the exception of visualizations. The API features are demonstrated in this tutorial.

## Hands on: Pulling AppEEARS Data via the API

*The following section was adapted from [LPDAAC's E-Learning Tutorials on the AppEEARS API](https://lpdaac.usgs.gov/resources/e-learning/getting-started-appeears-api-r-point-request/) and modified to request data for NEON sites.*


> Contributing Authors: 
 Material written by Mahsa Jami^1^ and Cole Krehbiel^1^                                                     
 Contact: LPDAAC@usgs.gov   
 Voice: +1-866-573-3222   
 Organization: Land Processes Distributed Active Archive Center (LP DAAC)  
 Website: https://lpdaac.usgs.gov/  
 Date last modified: 06-12-2020      
 
> ^1^ KBR, Inc., contractor to the U.S. Geological Survey, Earth Resources Observation and Science (EROS) Center,  
> Sioux Falls, South Dakota, USA. Work performed under USGS contract G15PD00467 for LP DAAC^2^.  
 
>^2^ LP DAAC Work performed under NASA contract NNG14HH33I.
 
 
Run the following chunk to install any packages you will need for this section:

```{r, eval=FALSE}
# Packages you will need for AppEEARS API Tutorials
packages = c('getPass','httr','jsonlite','ggplot2','dplyr','tidyr','readr','geojsonio','geojsonR','rgdal','sp', 'raster', 'rasterVis', 'RColorBrewer', 'jsonlite')



# Identify missing packages
new.packages = packages[!(packages %in% installed.packages()[,"Package"])]

# Loop through and download the required packages
if (length(new.packages)[1]==0){
  message('All packages already installed')
  }else{
  for (i in 1:length(new.packages)){
    message(paste0('Installing: ', new.packages))
    install.packages(new.packages[i])
    }
  }
```

## Getting Started with the AppEEARS API (Point Request)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
***

**This section demonstrates how to use R to connect to the AppEEARS API**
The Application for Extracting and Exploring Analysis Ready Samples ([AppEEARS](https://lpdaacsvc.cr.usgs.gov/appeears/)) offers a simple and efficient way to access and transform [geospatial data](https://lpdaacsvc.cr.usgs.gov/appeears/products) from a variety of federal data archives in an easy-to-use web application interface. AppEEARS enables users to subset geospatial data spatially, temporally, and by band/layer for point and area samples. AppEEARS returns not only the requested data, but also the associated quality values, and offers interactive visualizations with summary statistics in the web interface. The [AppEEARS API](https://lpdaacsvc.cr.usgs.gov/appeears/api/) offers users **programmatic access** to all features available in AppEEARS, with the exception of visualizations. The API features are demonstrated in this tutorial.

***
### Example: Submit a point request for multiple NEON sites to extract vegetation and land surface temperature data   

In this tutorial, Connecting to the AppEEARS API, querying the list of available products, submitting a point sample request, downloading the request, working with the AppEEARS Quality API, and loading the results into R for visualization are covered. AppEEARS point requests allow users to subset their desired data using latitude/longitude geographic coordinate pairs (points) for a time period of interest, and for specific data layers within data products. AppEEARS returns the valid data from the parameters defined within the sample request.

#### Data Used in the Example:
- Data layers: 
    - Combined MODIS Leaf Area Index (LAI)  
        - [MCD15A3H.006](https://doi.org/10.5067/MODIS/MCD15A3H.006), 500m, 4 day: 'Lai_500m'      
    - Terra MODIS Land Surface Temperature    
        - [MOD11A2.006](https://doi.org/10.5067/MODIS/MOD11A2.006), 1000m, 8 day: 'LST_Day_1km', 'LST_Night_1km'  

***
## Topics Covered in this section:
1. **Getting Started**  
    1a. Load Packages
    1b. Set Up the Output Directory  
    1c. Login   
2. **Query Available Products**  
    2a. Search and Explore Available Products   
    2b. Search and Explore Available Layers     
3. **Submit a Point Request**  
    3a. Compile a JSON Object 
    3b. Submit a Task Request 
    3c. Retrieve Task Status  
4. **Download a Request**    
    4a. Explore Files in Request Output   
    4b. Download Files in a Request (Automation)  
5. **Explore AppEEARS Quality API**  
    5a. List Quality Layers    
    5b. Show Quality Values  
    5c. Decode Quality Values  
6. **BONUS: Load Request Output and Visualize**    
    6a. Load CSV     
    6b. Plot Results (Line/Scatter Plots)  
    
***
### Prerequisites:
+ A [NASA Earthdata Login](https://urs.earthdata.nasa.gov/) account is required to login to the AppEEARS API and submit a request . You can create an account at the link provided.

+ R and RStudio. 
  These tutorials have been tested on Windows and MAC systems using R Version 4.0.0, RStudio version 1.1.463, and the specifications listed below.      

  + Required packages: 
    + `getPass`  
    + `httr`  
    + `jsonlite`  
    + `warnings` 
  + To read and visualize the tabular data:
    + `dplyr`        
    + `tidyr`
    + `readr`    
    + `ggplot2`

***

#### Getting Started:
  1. Clone/download [AppEEARS API Getting Started in R Repository](https://git.earthdata.nasa.gov/rest/api/latest/projects/LPDUR/repos/appeears-api-getting-started_r/archive?format=zip) from the LP DAAC Data User Resources Repository or pull code from this textbook.
 
  2. If you opted to clone LP DAAC's repo open the `AppEEARS_API_R.Rproj` file to directly open the project. Next, select the `AppEEARS_API_Point_R.Rmd` from the files list and open it. 


***
### AppEEARS Information:
To access AppEEARS, visit: https://lpdaacsvc.cr.usgs.gov/appeears/.

For comprehensive documentation of the full functionality of the AppEEARS API, please see the AppEEARS [API Documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/).

***
## Getting Started with the AppEEARS API       
### Load Packages
First, load the R packages necessary to run the tutorial. 
```{r, warning=FALSE, message=FALSE}
# Load necessary packages into R                                               
library(getPass)            # A micro-package for reading passwords
library(httr)               # To send a request to the server/receive a response from the server
library(jsonlite)           # Implements a bidirectional mapping between JSON data and the most important R data types
library(ggplot2)            # Functions for graphing and mapping
library(tidyr)              # Function for working with tabular data
library(dplyr)              # Function for working with tabular data
library(readr)              # Read rectangular data like CSV
```
***
### Set Up the Output Directory
Set your input directory, and create an output directory for the results.
```{r}
outDir <- file.path('./data/')                 # Create an output directory if it doesn't exist
suppressWarnings(dir.create(outDir))                 
```

***
### Login to Earth Data

To submit a request, you must first login to the AppEEARS API. Use your private `R Script` to enter your NASA Earthdata login Username and Password. 

```{r}
source('./Tokens/EARTHDATA_Token.R') #path will change based on where you stored it
exists('user')
```

Decode the username and password to be used to post login request.

```{r}
secret <- jsonlite::base64_enc(paste(user, password, sep = ":"))  # Encode the string of username and password
```

Next, assign the AppEEARS API URL to a static variable. 

```{r}
API_URL = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'  # Set the AppEEARS API to a variable
```
Use the `httr` package to post your username and password. A successful login will provide you with a token to be used later in this tutorial to submit a request. For more information or if you are experiencing difficulties, please see the [API Documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#login).
```{r}
# Insert API URL, call login service, set the component of HTTP header, and post the request to the server
response <- httr::POST(paste0(API_URL,"login"), 
                       add_headers("Authorization" = paste("Basic", gsub("\n", "", secret)),
                                   "Content-Type" ="application/x-www-form-urlencoded;charset=UTF-8"),
                       body = "grant_type=client_credentials")

response_content <- content(response)                          # Retrieve the content of the request
token_response <- toJSON(response_content, auto_unbox = TRUE)  # Convert the response to the JSON object
remove(user, password, secret, response)                       # Remove the variables that are not needed anymore 
prettify(token_response)                                       # Print the prettified response
```

**Above, you should see a Bearer token. Notice that this token will expire approximately 48 hours after being acquired.**

***
## Query Available Products
The product API provides details about all of the products and layers available in AppEEARS. For more information, please see the [API Documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#product).

Below, call the product API to list all of the products available in AppEEARS.
```{r}
prods_req <- GET(paste0(API_URL, "product"))             # Request the info of all products from product service
prods_content <- content(prods_req)                      # Retrieve the content of request 
all_Prods <- toJSON(prods_content, auto_unbox = TRUE)    # Convert the info to JSON object
remove(prods_req, prods_content)                         # Remove the variables that are not needed anymore
# prettify(all_Prods)                                      # Print the prettified product response
```
***

## Search and Explore Available Products

Create a list indexed by product name to make it easier to query a specific product.

```{r}
# Divides information from each product.
divided_products <- split(fromJSON(all_Prods), seq(nrow(fromJSON(all_Prods))))
# Create a list indexed by the product name and version
products <- setNames(divided_products,fromJSON(all_Prods)$ProductAndVersion)
# Print no. products available in AppEEARS
sprintf("AppEEARS currently supports %i products." ,length(products))   
```

Next, look at the product's names and descriptions. Below, the 'ProductAndVersion' and 'Description' are printed for all products.

```{r}
# Loop through the products in the list and print the product name and description
for (p in products){                                   
  print(paste0(p$ProductAndVersion," is ",p$Description," from ",p$Source))  
}
```

The product service provides many useful details, including if a product is currently available in AppEEARS, a description, and information on the spatial and temporal resolution. Below, the product details are retrieved using 'ProductAndVersion'. 

```{r}
# Convert the MCD15A3H.006 info to JSON object and print the prettified info
prettify(toJSON(products$"MCD15A3H.006")) 
```

Also, the products can be searched using their description. Below, search for products containing Leaf Area Index in their description and make a list of their productAndVersion.

```{r}
LAI_Products <- list()                                        # Create an empty list 
for (p in products){                                          # Loop through the product list
  if (grepl('Leaf Area Index', p$Description )){              # Look through the product description for a keyword 
    LAI_Products <- append(LAI_Products, p$ProductAndVersion) # Append the LAI products to the list
  }
}
LAI_Products
```

Using the info above, Create a list of desired products.  

```{r}
desired_products <- c('MCD15A3H.006','MOD11A2.006')   # Create a vector of desired products 
desired_products
```

***

## Search and Explore Available Layers 

This API call will list all of the layers available for a given product. Each product is referenced by its `ProductAndVersion` property which is also referred to as the product_id. First, request the layers for the `MCD15A3H.006` product.

```{r}
# Request layers for the 1st product in the list: MCD15A3H.006
MCD15A3H_req <- GET(paste0(API_URL,"product/", desired_products[1]))  # Request the info of a product from product URL
MCD15A3H_content <- content(MCD15A3H_req)                             # Retrieve content of the request 
MCD15A3H_response <- toJSON(MCD15A3H_content, auto_unbox = TRUE)      # Convert the content to JSON object
remove(MCD15A3H_req, MCD15A3H_content)                                # Remove the variables that are not needed anymore
#prettify(MCD15A3H_response)                                          # Print the prettified response
names(fromJSON(MCD15A3H_response))                                    # print the layer's names    
```
Next, request the layers for the `MOD11A2.006` product.
```{r}
MOD11_req <- GET(paste0(API_URL,"product/", desired_products[2]))  # Request the info of a product from product URL
MOD11_content <- content(MOD11_req)                                # Retrieve content of the request
MOD11_response <- toJSON(MOD11_content, auto_unbox = TRUE)         # Convert the content to JSON object
remove(MOD11_req, MOD11_content)                                   # Remove the variables that are not needed anymore
names(fromJSON(MOD11_response))                                    # print the layer names
```

Lastly, select the desired layers and pertinent products and make a data frame using this information. This data frame will be inserted into the nested data frame that will be used to create a JSON object to submit a request in [Section 3](#section3).

```{r}
desired_layers <- c("LST_Day_1km","LST_Night_1km","Lai_500m")   # Create a vector of desired layers
desired_prods <- c("MOD11A2.006","MOD11A2.006","MCD15A3H.006")  # Create a vector of products including the desired layers
# Create a data frame including the desired data products and layers
layers <- data.frame(product = desired_prods, layer = desired_layers) 
```
***
## Submit a Point Request
The Submit task API call provides a way to submit a new request to be processed. It can accept data via JSON or query string. In the example below, create a JSON object and submit a request. Tasks in AppEEARS correspond to each request associated with your user account. Therefore, each of the calls to this service requires an authentication token (see [Section 1c.](#login)).

***
### Compile a JSON Object
In this section, begin by setting up the information needed for a nested data frame that will be later converted to a JSON object for submitting an AppEEARS point request.
For detailed information on required JSON parameters, see the [API Documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#tasks).

For point requests, beside the date range and desired layers information, the coordinates property must also be inside the task object. Optionally, set `id` and `category` properties to further identify your selected coordinates.

We'll start by requesting point-based data for `NEON.D17.SOAP` and `NEON.D17.SJER`:

```{r}
startDate <- "01-01-2020"       # Start of the date range for  which to extract data: MM-DD-YYYY
endDate <- "10-01-2020"         # End of the date range for  which to extract data: MM-DD-YYYY
recurring <- FALSE              # Specify True for a recurring date range
#yearRange <- [2000,2016]       # If recurring = True, set yearRange, change start/end date to MM-DD

lat <- c(37.0334, 37.1088)        # Latitude of the point sites 
lon <- c(-119.2622, -119.7323)    # Longitude of the point sites
id <- c("0","1")                      # ID for the point sites
category <- c("SOAP", "SJER") # Category for point sites

taskName <- 'NEON SOAP SJER Vegetation'           # Enter name of the task here
taskType <- 'point'                    # Specify the task type, it can be either "area" or "point"
```

To be able to successfully submit a task, the JSON object should be structured in a certain way. The code chunk below uses the information from the previous chunk to create a nested data frame. This nested data frame will be converted to JSON object that can be used to complete the request. 

```{r, cache=TRUE}
# Create a data frame including the date range for the request
date <- data.frame(startDate = startDate, endDate = endDate)
# Create a data frame including lat and long coordinates. ID and category name is optional.
coordinates <- data.frame(id = id, longitude = lon, latitude = lat, category = category)

task_info <- list(date,layers, coordinates)               # Create a list of data frames 
names(task_info) <- c("dates", "layers", "coordinates")   # Assign names

task <- list(task_info, taskName, taskType)               # Create a nested list 
names(task) <- c("params", "task_name", "task_type")      # Assign names 
remove(date, layers, coordinates, task_info)              # Remove the variables that are not needed anymore
```
`toJSON` function from `jsonlite` package converts the type of data frame to a string that can be recognized as a JSON object to be submitted as a point request.  
```{r, cache=TRUE}
task_json <- toJSON(task,auto_unbox = TRUE)   # Convert to JSON object
```
***
### Submit a Task Request
Token information is needed to submit a request. Below the login token is assigned to a variable.
```{r, cache=TRUE}
token <- paste("Bearer", fromJSON(token_response)$token)     # Save login token to a variable
```

Below, post a call to the API **task service**, using the `task_json` created above.
```{r, cache=TRUE}
# Post the point request to the API task service
response <- POST(paste0(API_URL, "task"), 
                 body = task_json , 
                 encode = "json", 
                 add_headers(Authorization = token, "Content-Type" = "application/json"))

task_content <- content(response)                                 # Retrieve content of the request 
task_response <- prettify(toJSON(task_content, auto_unbox = TRUE))# Convert the content to JSON object
remove(response, task_content)                                    # Remove the variables that are not needed anymore
task_response                                                     # Print the prettified task response
```
***
### Retrieve Task Status
This API call will list all of the requests associated with your user account, automatically sorted by date descending with the most recent requests listed first.
The AppEEARS API contains some helpful formatting resources. Below, limit the API response to 2 entries for the last 2 requests and set pretty to True to format the response as an organized JSON object to make it easier to read. Additional information on AppEEARS API
[retrieve task](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#retrieve-task), [pagination](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#pagination), and [formatting](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#formatting) can be found in the API documentation.

```{r, cache=TRUE}
params <- list(limit = 2, pretty = TRUE)                            # Set up query parameters
# Request the task status of last 2 requests from task URL
response_req <- GET(paste0(API_URL,"task"), query = params, add_headers(Authorization = token))
response_content <- content(response_req)                           # Retrieve content of the request
status_response <- toJSON(response_content, auto_unbox = TRUE)      # Convert the content to JSON object
remove(response_req, response_content)                              # Remove the variables that are not needed anymore                         
prettify(status_response)                                           # Print the prettified response
```

The task_id that was generated when submitting your request can also be used to retrieve a task status. 
```{r, cache=TRUE}
task_id <- fromJSON(task_response)$task_id                 # Extract the task_id of submitted point request
# Request the task status of a task with the provided task_id from task URL
status_req <- GET(paste0(API_URL,"task/", task_id), add_headers(Authorization = token)) 
status_content <- content(status_req)                       # Retrieve content of the request       
statusResponse <-toJSON(status_content, auto_unbox = TRUE)  # Convert the content to JSON object
stat <- fromJSON(statusResponse)$status                     # Assign the task status to a variable  
remove(status_req, status_content)                          # Remove the variables that are not needed anymore
prettify(statusResponse)                                    # Print the prettified response
```

Retrieve the task status every 5 seconds. The task status should be `done` to be able to download the output.
```{r}
while (stat != 'done') {
  Sys.sleep(5)
  # Request the task status and retrieve content of request from task URL
  stat_content <- content(GET(paste0(API_URL,"task/", task_id), add_headers(Authorization = token)))
  stat <-fromJSON(toJSON(stat_content, auto_unbox = TRUE))$status    # Get the status
  remove(stat_content) 
  print(stat)
}
```

***
## Download a Request

### Explore Files in Request Output 

Before downloading the request output, examine the files contained in the request output.
```{r, cache=TRUE}
# Request the task bundle info from API bundle URL
response <- GET(paste0(API_URL, "bundle/", task_id), add_headers(Authorization = token))
response_content <- content(response)                          # Retrieve content of the request
bundle_response <- toJSON(response_content, auto_unbox = TRUE)  # Convert the content to JSON object
prettify(bundle_response)                                       # Print the prettified response
```
***
## Download Files in a Request (Automation) 
The bundle API provides information about completed tasks. For any completed task, a bundle can be queried to return the files contained as a part of the task request. Below, call the bundle API and return all of the output files. Next, read the contents of the bundle in JSON format and loop through file_id to automate downloading all of the output files into the output directory. For more information, please see [AppEEARS API Documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#bundle).
```{r, warning= FALSE, results= 'hide', , cache=TRUE}
bundle <- fromJSON(bundle_response)$files
for (id in bundle$file_id){
  # retrieve the filename from the file_id
  filename <- bundle[bundle$file_id == id,]$file_name     
  # create a destination directory to store the file in
  filepath <- paste(outDir,filename, sep = "/")
  suppressWarnings(dir.create(dirname(filepath)))
  # write the file to disk using the destination directory and file name 
  response <- GET(paste0(API_URL, "bundle/", task_id, "/", id),
                  write_disk(filepath, overwrite = TRUE),
                  progress(),
                  add_headers(Authorization = token))
  }
```

***
## Explore AppEEARS Quality Service
The quality API provides quality details about all of the data products available in AppEEARS. Below are examples of how to query the quality API for listing quality products, layers, and values. The final example ([Section 5c.](#decode)) demonstrates how AppEEARS quality services can be leveraged to decode pertinent quality values for your data. For more information visit [AppEEARS API documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#quality).

First, reset pagination to include offset which allows you to set the number of results to skip before starting to return entries. Next, make a call to list all of the data product layers and the associated quality product and layer information.

```{r, cache=TRUE}
params <- list(limit = 6, offset = 20, pretty = TRUE)     # Set up the query parameters
q_req <- GET(paste0(API_URL, "quality"), query = params)  # Request the quality info from quality API_URL
q_content <- content(q_req)                               # Retrieve the content of request
q_response <- toJSON(q_content, auto_unbox = TRUE)        # Convert the info to JSON object
remove(params, q_req, q_content)                          # Remove the variables that are not needed 
prettify(q_response)                                      # Print the prettified quality information
```
***
### List Quality Layers 
This API call will list all of the quality layer information for a product. For more information visit [AppEEARS API documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=R#quality)

```{r, cache=TRUE}
productAndVersion <- 'MCD15A3H.006'                            # Assign ProductAndVersion to a variable 
# Request the quality info from quality API for a specific product
MCD15A3H_req <- GET(paste0(API_URL, "quality/", productAndVersion))
MCD15A3H_content <- content(MCD15A3H_req)                      # Retrieve the content of request
MCD15A3H_quality <- toJSON(MCD15A3H_content, auto_unbox = TRUE)# Convert the info to JSON object
remove(MCD15A3H_req, MCD15A3H_content)                         # Remove the variables that are not needed anymore
prettify(MCD15A3H_quality)                                     # Print the prettified quality information
```
***
### Inspect Quality Values 
This API call will list all of the values for a given quality layer. 
```{r, cache=TRUE}
quality_layer <- 'FparLai_QC'                                 # assign a quality layer to a variable
# Request the specified quality layer info from quality API
quality_req <- GET(paste0(API_URL, "quality/",  productAndVersion, "/", quality_layer, sep = ""))
quality_content <- content(quality_req)                        # Retrieve the content of request
quality_response <- toJSON(quality_content, auto_unbox = TRUE) # Convert the info to JSON object
remove(quality_req, quality_content)                           # Remove the variables that are not needed 
prettify(quality_response)                                     # Print the quality response as a data frame
```
***
## Decode Quality Values
This API call will decode the bits for a given quality value. 
```{r, cache=TRUE}
quality_value <- 1                        # Assign a quality value to a variable 
# Request and retrieve information for provided quality value from quality API URL 
response <- content(GET(paste0(API_URL, "quality/", productAndVersion, "/", quality_layer, "/", quality_value)))
q_response <- toJSON(response, auto_unbox = TRUE)     # Convert the info to JSON object
remove(response)                                      # Remove the variables that are not needed anymore
prettify(q_response)                                  # Print the prettified response
```
***
## Load Request Output and Visualize

Here, load the CSV file containing the results from your request using `readr` package, and create some basic visualizations using the `ggplot2` package.

## Load a CSV
Use the `readr` package to load the CSV file containing the results from the AppEEARS request.
```{r, message= FALSE, warning= FALSE}
# Make a list of csv files in the output directory
files <- list.files(outDir, pattern = "\\MOD11A2-006-results.csv$") 
# Read the MOD11A2 results
df <- read_csv(paste0(outDir,"/", files))
```
Select the MOD11A2.006 LST Day column for the data from Grand Canyon National Park using `dplyr` package.
```{r}
lst_GC <- df %>%
  # Filter df for the point from GC
  filter(Category == "SOAP") %>%  
  # Select desired columns
  select(Latitude, Longitude, Date ,MOD11A2_006_LST_Day_1km, MOD11A2_006_LST_Night_1km) 
```
Extract information for LST_DAY_1KM from MOD11_response of product service call from earlier in the tutorial.
```{r}
#fromJSON(MOD11_response)$LST_Day_1km                        # Extract all the info for LST_Day_1km layer

fillValue <- fromJSON(MOD11_response)$LST_Day_1km$FillValue  # Assign fill value to a variable
unit <- fromJSON(MOD11_response)$LST_Day_1km$Units           # Assign unit to a variable
sprintf("Fill value for LST_DAY_1KM is: %i" ,fillValue)      # Print LST_DAY_1KM fill value
sprintf("Unit for LST_DAY_1KM is: %s" ,unit)                 # Print LST_DAY_1KM unit 

```
***
## Plot Results (Line/Scatter Plots)
Next, plot a time series of daytime LST for the selected point in Grand Canyon National Park for 2018. Below, filter the LST data to exclude fill values. 
```{r}
lst_GC <- lst_GC %>%
  # exclude NoData
  filter(MOD11A2_006_LST_Day_1km != fillValue)%>%
  filter(MOD11A2_006_LST_Night_1km != fillValue)

```
Next, plot LST Day as a time series with some additional formatting using `ggplot2`.
```{r, fig.width = 12, fig.height=5}
ggplot(lst_GC)+
  geom_line(aes(x= Date, y = MOD11A2_006_LST_Day_1km), size=1, color="blue")+
  geom_point(aes(x= Date, y = MOD11A2_006_LST_Day_1km), shape=18 , size = 3, color="blue")+
  labs(title = "Time Series",
       x = "Date",
       y = sprintf( "LST_Day_1km (%s)", unit))+
  scale_x_date(date_breaks = "16 day")+ 
  scale_y_continuous(limits = c(250, 325), breaks = seq(250, 325, 10))+
  theme(plot.title = element_text(face = "bold",size = rel(2.5),hjust = 0.5),
        axis.title = element_text(face = "bold",size = rel(1)),
        panel.background = element_rect(fill = "lightgray", colour = "black"),
        axis.text.x = element_text(face ="bold",color="black", angle= 315 , size = 10),
        axis.text.y = element_text(face ="bold",color="black", angle= 0, size = 10)
        )
```

Using the `tidyr` package, the LST Day and Night values for Grand Canyon NP are being gathered in a single column to be used to make a plot including both  `LST_Day_1km` and `LST_Night_1km`.
```{r}
lst_GC_DN <- tidyr::gather(lst_GC, key = Tstat , value = LST, MOD11A2_006_LST_Day_1km, MOD11A2_006_LST_Night_1km)
lst_GC_DN[1:5,]                     # print the five first observations 
```

Next, plot LST Day and Night as a time series with some additional formatting.

```{r,fig.width = 12, fig.height=5}
ggplot(lst_GC_DN)+
  geom_line(aes(x= Date, y = LST, color = Tstat), size=1)+
  geom_point(aes(x= Date, y = LST, color = Tstat), shape=18 , size = 3)+
  scale_fill_manual(values=c("red", "blue"))+
  scale_color_manual(values=c('red','blue'))+
  labs(title = "Time Series",
       x = "Date",
       y = sprintf( "LST_Day_1km (%s)",unit))+
  scale_x_date(date_breaks = "16 day")+  
  scale_y_continuous(limits = c(250, 325), breaks = seq(250, 325, 10))+
  theme(plot.title = element_text(face = "bold",size = rel(2.5), hjust = 0.5),
        axis.title = element_text(face = "bold",size = rel(1)),
        panel.background = element_rect(fill = "lightgray", colour = "black"),
        axis.text.x = element_text(face ="bold",color="black", angle= 315 , size = 10),
        axis.text.y = element_text(face ="bold",color="black", angle= 0, size = 10),
        legend.position = "bottom",
        legend.title = element_blank()
        )
```

Finally, bring in the daytime LST data from `SJER` , and compare with daytime LST at `SOAP` , shown below in a scatterplot using `ggplot2` package. 
Here, the `dplyr` is used to extract the LST_DAY_1km for Zion National Park.  
```{r}
lst_Z <- df %>%
  filter(MOD11A2_006_LST_Day_1km != fillValue) %>%        # Filter fill value 
  filter(Category == "SJER")%>%                           # Filter Zion national park
  select(Date, MOD11A2_006_LST_Day_1km)                   # Select desired columns
```
Make a scatterplot.
```{r, eval=FALSE}
ggplot()+
  geom_point(aes(x=lst_Z$MOD11A2_006_LST_Day_1km, y=lst_GC$MOD11A2_006_LST_Day_1km), shape=18 , size = 3, color="blue")+
  labs(title = "MODIS LST: SOAP vs. SJER, 2020",
       x = sprintf("SOAP: LST_Day_1km (%s)",unit),
       y = sprintf( "SJER: LST_Day_1km (%s)",unit))+
  theme(plot.title = element_text(face = "bold",size = rel(1.5), hjust = 0.5),
        axis.title = element_text(face = "bold",size = rel(1)),
        panel.background = element_rect(fill = "lightgray", colour = "black"),
        axis.text.x = element_text(face ="bold",color="black", size = 10),
        axis.text.y = element_text(face ="bold",color="black", size = 10)
        )
```

This example can provide a template to use for your own research workflows. Leveraging the AppEEARS API for searching, extracting, and formatting analysis ready data, and loading it directly into R means that you can keep your entire research workflow in a single software program, from start to finish.


## Submit an Area Request

## NASA EOS Written Questions
1) How does the scientific method of Earth Science differ from thta of traditional/classical labratory sciences?

## NASA EOS Culmination Write Up

Write up a 1-page **derived data product or research pipeline proposal** summary of a project that you might want to explore using NASA EOS data. Include the types of data that you would need to implement this project and how you would retrieve them. Save this summary as you will be refining and adding to your ideas over the course of the semester.  

Suggestions: 

1. Tables or bullet lists of **specific** data products

2. An **overarching high-quality figure** to show how these data align

3. One paragraph summarizing *how* this data or analysis is useful to **you and/or the infrastructure**.
